{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Local-Interpretable-Model-agnostic-Explanations-(LIME)\" data-toc-modified-id=\"Local-Interpretable-Model-agnostic-Explanations-(LIME)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Local Interpretable Model-agnostic Explanations (LIME)</a></div><div class=\"lev1 toc-item\"><a href=\"#Reference\" data-toc-modified-id=\"Reference-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Reference</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Interpretable Model-agnostic Explanations (LIME)\n",
    "\n",
    "Nowadays, algorithms decide which emails reach our inboxes, whether we are approved for credit, and whom we get the opportunity to date. But, as algorithms give answers, they raise questions. If an algorithm denies your loan application, wouldn’t you like to know why or what you could change for a more positive outcome? Or perhaps you’d like to know if your bank is right to trust the algorithm in the first place. This is the reason why model interpretation is important when we wish to intrepret our model to non-technical audiences and ensure that a model is right for the right reasons and wrong for the right reasons. Insights into the model can help improve it and can help build trust that the model, once deployed, will continue to do a good job. \n",
    "\n",
    "Fundamentally, machine learning algorithms learn relationships between inputs and outputs. During model training, we supply examples of these inputs and outputs as \"learning material\" and the algorithm would then try to learn the relationship between them.\n",
    "\n",
    "Some relationships are simple and can be captured by, linear models, which are easy to inspect and understand. For example, the probability of loan default increases with loan amount and decreases with income. These models are interpretable, meaning, they allow a qualitative understanding between inputs and outputs.\n",
    "\n",
    "<img src=\"img/linear_model.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "But some relationships are complex and may depend on multiple factors in non-linear ways. To accurately model these relationships we need models with the flexibility to capture that complexity. Models such as random forests, gradient boosted trees, and neural networks can do just that. But, these complex models can be intrinsically harder to inspect, understand and interpret.\n",
    "\n",
    "<img src=\"img/complex_model.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "What if we really need the accuracy of complex models, but also wish to have the interpretability. In these situation, \"model-agnostic\" interpretability tools such as **LIME** may be the right approach. LIME is mainly based on two ideas: perturbation and locally linear approximation.\n",
    "\n",
    "**Perturbation: ** LIME takes a prediction we want to explain and systematically perturbs its inputs. These perturbed inputs become new, labelled training data for a simpler approximate model. By repeatedly perturbing the input and observing its effect on the output you can develop an understanding of how different inputs relate to the original output.\n",
    "\n",
    "**Locally Linear Approximation: ** LIME fits a linear model to describe the relationships between the (perturbed) inputs and outputs. In doing so, it weights perturbed samples that are closer to the targeted example more heavily to nudge the algorithm to focus on the most relevant part of the \"decision function\". This linear model approximates the more complex, non-linear function learned by the high-accuracy model locally, in the vicinity of the to-be-explained prediction.\n",
    "\n",
    "<img src=\"img/lime.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "Based on these two ideas, LIME is an exciting breakthrough. It allows us to train a model in any way we like and still have an answer to the local question, “Why has this particular decision been made?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lime.lime_tabular import LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size = 0.2)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 100)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTabularExplainer(\n",
    "    X_train, feature_names = iris.feature_names,\n",
    "    class_names = iris.target_names, discretize_continuous = True)\n",
    "\n",
    "i = np.random.randint(X_test.shape[0])\n",
    "exp = explainer.explain_instance(\n",
    "    X_test[i], rf.predict_proba,\n",
    "    num_features = X_test.shape[1], top_labels = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table = True, show_all = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent call\n",
    "# exp.as_map()\n",
    "exp.local_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_pyplot_figure(label = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Blog: Why your relationship is likely to last (or not): using Local Interpretable Model-Agnostic Explanations (LIME)](http://blog.fastforwardlabs.com/2017/09/01/LIME-for-couples.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "63px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
