{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2017-07-09 15:51:03 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 6.1.0\n",
      "\n",
      "numpy 1.13.1\n",
      "pandas 0.19.2\n",
      "matplotlib 2.0.0\n",
      "sklearn 0.18.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "from graphviz import Source\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eli5 experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96666666666666667"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "feature_names = boston.feature_names\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 14)\n",
    "\n",
    "model_xgb = XGBClassifier(n_estimators = 1)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "accuracy_score(y_test, model_xgb.predict(X_test))\n",
    "\n",
    "# model_xgb = XGBRegressor(n_estimators = 30)\n",
    "# model_xgb.fit(X_train, y_train)\n",
    "# r2_score(y_test, model_xgb.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_feature_imp(model_xgb, feature_names = None, importance_type = 'gain'):\n",
    "    # XGBClassifier is a scikit-learn like\n",
    "    # wrapper around the actual underlying model\n",
    "    booster = model_xgb.get_booster()\n",
    "    \n",
    "    # get the normalized feature importance (sum up to 1)\n",
    "    score = booster.get_score(importance_type = importance_type)\n",
    "    all_features_score = np.array(\n",
    "        [score.get(f, 0.) for f in booster.feature_names], dtype = np.float32)\n",
    "    \n",
    "    normed_score = all_features_score / np.sum(all_features_score)\n",
    "    \n",
    "    # construct a dataframe with the feature name mapping with the score\n",
    "    if feature_names is None:\n",
    "        feature_names = booster.feature_names\n",
    "\n",
    "    feature_imp = {'weight': normed_score, 'feature': feature_names}\n",
    "    df_feature_imp = pd.DataFrame(feature_imp)[['feature', 'weight']]\n",
    "    df_feature_imp = (df_feature_imp\n",
    "                      .sort_values('feature')\n",
    "                      .reset_index(drop = True))\n",
    "    return df_feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>petal length (cm)</td>\n",
       "      <td>0.442693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>petal width (cm)</td>\n",
       "      <td>0.557307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sepal width (cm)</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature    weight\n",
       "0  petal length (cm)  0.442693\n",
       "1   petal width (cm)  0.557307\n",
       "2  sepal length (cm)  0.000000\n",
       "3   sepal width (cm)  0.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_imp = xgb_feature_imp(model_xgb, feature_names = feature_names)\n",
    "df_feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import Booster, XGBRegressor, XGBClassifier\n",
    "\n",
    "def _check_booster_args(xgb):\n",
    "    if isinstance(xgb, Booster):\n",
    "        booster = xgb\n",
    "    else:\n",
    "        booster = xgb.get_booster()\n",
    "        regression = isinstance(xgb, XGBRegressor)\n",
    "\n",
    "    return booster, regression\n",
    "\n",
    "\n",
    "booster, regression = _check_booster_args(model_xgb)\n",
    "regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgb regression n_targets\n",
    "n_targets = 1\n",
    "if isinstance(model_xgb, XGBClassifier):\n",
    "    n_targets = 1 if model_xgb.n_classes_ == 2 else model_xgb.n_classes_\n",
    "\n",
    "n_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['y']\n",
    "if not regression:\n",
    "    names = model_xgb.classes_\n",
    "\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.atleast_2d(X_train[0])\n",
    "\n",
    "# if regression:\n",
    "# proba = model_xgb.predict_proba(X)[0]\n",
    "# proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0:[f2<2.3] yes=1,no=2,missing=1,gain=54.04,cover=53.3333\\n\\t1:leaf=0.141176,cover=16\\n\\t2:leaf=-0.0730435,cover=37.3333\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_dumps = booster.get_dump(with_stats = True)\n",
    "tree_dumps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'children': [{'cover': 16.0, 'leaf': 0.141176, 'node_id': 1},\n",
       "  {'cover': 37.3333, 'leaf': -0.0730435, 'node_id': 2}],\n",
       " 'cover': 53.3333,\n",
       " 'depth': 0,\n",
       " 'gain': 54.04,\n",
       " 'missing': 1,\n",
       " 'no': 2,\n",
       " 'node_id': 0,\n",
       " 'split': 'f2',\n",
       " 'split_condition': 2.3,\n",
       " 'yes': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def _parse_dump_line(line):\n",
    "    # match the branch pattern, e.g.\n",
    "    # \\t1:[f2<2.3] yes=3,no=4,missing=3,gain=34.829,cover=35.5556'\n",
    "    # may have 0 or more than 1 '\\t' at the beginning, which is \n",
    "    # used for indicating depth of branch when printed\n",
    "    branch_match = re.match(\n",
    "        '^(\\t*)(\\d+):\\[(.+)<(.+)\\] '\n",
    "        'yes=(\\d+),no=(\\d+),missing=(\\d+),'\n",
    "        'gain=(.+),cover=(.+)$', line)\n",
    "    \n",
    "    if branch_match is not None:\n",
    "        matched = branch_match.groups()\n",
    "        n_tabs = matched[0]\n",
    "        depth = len(n_tabs)\n",
    "        branch_info = {'depth': depth,\n",
    "                       'node_id': int(matched[1]),\n",
    "                       'split': matched[2],\n",
    "                       'split_condition': float(matched[3]),\n",
    "                       'yes': int(matched[4]),\n",
    "                       'no': int(matched[5]),\n",
    "                       'missing': int(matched[6]),\n",
    "                       'gain': float(matched[7]),\n",
    "                       'cover': float(matched[8])}\n",
    "        return depth, branch_info\n",
    "\n",
    "    # if it's not a branch, then it has to be a leaf node\n",
    "    # match the leaf pattern, e.g.\n",
    "    # \\t1:leaf=0.141176,cover=16\n",
    "    leaf_match = re.match('^(\\t*)(\\d+):leaf=(.+),cover=(.+)$', line)\n",
    "    n_tabs, node_id, value, cover = leaf_match.groups()\n",
    "    depth = len(n_tabs)\n",
    "    leaf_info = {'node_id': int(node_id),\n",
    "                 'leaf': float(value),\n",
    "                 'cover': float(cover)}\n",
    "    return depth, leaf_info\n",
    "\n",
    "\n",
    "def _parse_tree_dump(text_dump):\n",
    "    \"\"\" Parse text tree dump (one item of a list returned by Booster.get_dump())\n",
    "    into json format that will be used by next XGBoost release.\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    stack = []  # type: List[Dict]\n",
    "    for line in text_dump.split('\\n'):\n",
    "        if line:\n",
    "            depth, node = _parse_dump_line(line)\n",
    "            if depth == 0:\n",
    "                assert not stack\n",
    "                result = node\n",
    "                stack.append(node)\n",
    "            elif depth > len(stack):\n",
    "                raise ValueError('Unexpected dump structure')\n",
    "            else:\n",
    "                if depth < len(stack):\n",
    "                    stack = stack[:depth]\n",
    "                stack[-1].setdefault('children', []).append(node)\n",
    "                stack.append(node)\n",
    "    return result\n",
    "\n",
    "text_dump = tree_dumps[0]\n",
    "result = _parse_tree_dump(text_dump)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful\n",
    "from xgboost import DMatrix\n",
    "\n",
    "# XGBClassifier does not have pred_leaf argument as of now, so use booster\n",
    "dmatrix = DMatrix(X, missing = model_xgb.missing)\n",
    "leaf_ids = booster.predict(dmatrix, pred_leaf = True)[0]\n",
    "xgb_feature_names = {f: i for i, f in enumerate(booster.feature_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = booster.get_dump(with_stats = True, dump_format = 'json')\n",
    "len(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'children': [{'children': [{'cover': 16, 'leaf': -0.0705882, 'nodeid': 3},\n",
       "    {'children': [{'cover': 16.8889, 'leaf': 0.141615, 'nodeid': 7},\n",
       "      {'cover': 2.66667, 'leaf': -3.25116e-09, 'nodeid': 8}],\n",
       "     'cover': 19.5556,\n",
       "     'depth': 2,\n",
       "     'gain': 4.65416,\n",
       "     'missing': 7,\n",
       "     'no': 8,\n",
       "     'nodeid': 4,\n",
       "     'split': 'f2',\n",
       "     'split_condition': 4.95,\n",
       "     'yes': 7}],\n",
       "   'cover': 35.5556,\n",
       "   'depth': 1,\n",
       "   'gain': 34.829,\n",
       "   'missing': 3,\n",
       "   'no': 4,\n",
       "   'nodeid': 1,\n",
       "   'split': 'f2',\n",
       "   'split_condition': 2.3,\n",
       "   'yes': 3},\n",
       "  {'children': [{'cover': 1.33333, 'leaf': -2.55448e-09, 'nodeid': 5},\n",
       "    {'cover': 16.4444, 'leaf': -0.0707006, 'nodeid': 6}],\n",
       "   'cover': 17.7778,\n",
       "   'depth': 1,\n",
       "   'gain': 0.619154,\n",
       "   'missing': 5,\n",
       "   'no': 6,\n",
       "   'nodeid': 2,\n",
       "   'split': 'f2',\n",
       "   'split_condition': 4.85,\n",
       "   'yes': 5}],\n",
       " 'cover': 53.3333,\n",
       " 'depth': 0,\n",
       " 'gain': 12.9454,\n",
       " 'missing': 1,\n",
       " 'no': 2,\n",
       " 'nodeid': 0,\n",
       " 'split': 'f3',\n",
       " 'split_condition': 1.75,\n",
       " 'yes': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "tree_id = 1\n",
    "t = booster.get_dump(with_stats = True, dump_format = 'json')[tree_id]\n",
    "result2 = json.loads(t)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'children': [{'cover': 16, 'leaf': 0.141176, 'nodeid': 1},\n",
       "  {'cover': 37.3333, 'leaf': -0.0730435, 'nodeid': 2}],\n",
       " 'cover': 53.3333,\n",
       " 'depth': 0,\n",
       " 'gain': 54.04,\n",
       " 'missing': 1,\n",
       " 'no': 2,\n",
       " 'nodeid': 0,\n",
       " 'split': 'f2',\n",
       " 'split_condition': 2.3,\n",
       " 'yes': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_id = 0\n",
    "\n",
    "# leaf_ids : the leaf id of that example for each tree\n",
    "leaf_id = leaf_ids[tree_id]\n",
    "print(leaf_id)\n",
    "\n",
    "# parse the tree dump into json format\n",
    "t = booster.get_dump(with_stats = True, dump_format = 'json')[tree_id]\n",
    "result1 = json.loads(t)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f0': 0, 'f1': 1, 'f2': 2, 'f3': 3}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# node undefined = -2\n",
    "xgb_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_tree_paths(result):\n",
    "    if 'leaf' not in result:\n",
    "        node_id = result['nodeid']\n",
    "        left_child, right_child = result['children']\n",
    "        left_paths = _get_tree_paths(left_child)\n",
    "        right_paths = _get_tree_paths(right_child)\n",
    "\n",
    "        for path in left_paths:\n",
    "            path.append(node_id)\n",
    "\n",
    "        for path in right_paths:\n",
    "            path.append(node_id)\n",
    "\n",
    "        paths = left_paths + right_paths\n",
    "    else:\n",
    "        node_id = result['nodeid']\n",
    "        paths = [[node_id]]\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0], [2, 0]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = _get_tree_paths(result1)\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: [0, 1], 2: [0, 2]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_count = -1\n",
    "\n",
    "leaf_to_path = {}\n",
    "for path in paths:\n",
    "    path.reverse()\n",
    "    node = path[-1]\n",
    "    leaf_to_path[node] = path\n",
    "    \n",
    "    if node > node_count:\n",
    "        node_count = node\n",
    "        \n",
    "node_count += 1\n",
    "        \n",
    "print(node_count)   \n",
    "leaf_to_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain the feature, value, where index i holds the feature\n",
    "# and value for node i \n",
    "\n",
    "def _get_feature(result, feature, xgb_feature_names):\n",
    "    node_id = result['nodeid']\n",
    "    if 'leaf' not in result:\n",
    "        feature[node_id] = xgb_feature_names[result['split']]\n",
    "        left_child, right_child = result['children']\n",
    "        _get_feature(left_child, feature, xgb_feature_names)\n",
    "        _get_feature(right_child, feature, xgb_feature_names)\n",
    "    else:\n",
    "        feature[node_id] = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, -2, -2], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = np.zeros(node_count, dtype = np.int32)\n",
    "_get_feature(result1, feature, xgb_feature_names)\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.4,  1.5,  0.2]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38218513,  0.30924702,  0.30856785]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'children': [{'cover': 16, 'leaf': 0.141176, 'nodeid': 1},\n",
       "  {'cover': 37.3333, 'leaf': -0.0730435, 'nodeid': 2}],\n",
       " 'cover': 53.3333,\n",
       " 'depth': 0,\n",
       " 'gain': 54.04,\n",
       " 'missing': 1,\n",
       " 'no': 2,\n",
       " 'nodeid': 0,\n",
       " 'split': 'f2',\n",
       " 'split_condition': 2.3,\n",
       " 'yes': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum1(node, prefix = 0):\n",
    "    if node is None:\n",
    "        return 0\n",
    "    \n",
    "    p = prefix * 10 + node.value\n",
    "    if node.left is not None or node.right is not None:\n",
    "        return sum1(node.left, p) + sum1(node.right, p)\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum1(node, prefix = 0):\n",
    "    if node is None:\n",
    "        return 0\n",
    "    \n",
    "    # p = prefix * 10 + node.value\n",
    "    \n",
    "    if node.left is not None or node.right is not None:\n",
    "        return sum1(node.left, p) + sum1(node.right, p)\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_parent(result):\n",
    "    if 'leaf' not in result:\n",
    "        left_child, right_child = result['children']\n",
    "        _get_parent(left_child)\n",
    "        _get_parent(right_child)\n",
    "    else:\n",
    "        result['leaf'] = _parent_value(result['children'])\n",
    "    #return \n",
    "    \n",
    "def _parent_value(children):\n",
    "    \"\"\"\n",
    "    Value of the parent node: a weighted sum of child values.\n",
    "    \"\"\"\n",
    "    covers = np.array([child['cover'] for child in children])\n",
    "    covers /= np.sum(covers)\n",
    "    leafs = np.array([child['leaf'] for child in children])\n",
    "    return np.sum(leafs * covers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0087776098338186448"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_parent_value(result1['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-764efa883dda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hi' is not defined"
     ]
    }
   ],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_parent(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_parent_value(result1['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_indexed_leafs(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _indexed_leafs(parent):\n",
    "    \"\"\" Return a leaf nodeid -> node dictionary with\n",
    "    \"parent\" and \"leaf\" (average child \"leaf\" value) added to all nodes.\n",
    "    \"\"\"\n",
    "    if not parent.get('children'):\n",
    "        return {parent['nodeid']: parent}\n",
    "    indexed = {}\n",
    "    for child in parent['children']:\n",
    "        child['parent'] = parent\n",
    "        if 'leaf' in child:\n",
    "            indexed[child['nodeid']] = child\n",
    "        else:\n",
    "            indexed.update(_indexed_leafs(child))\n",
    "    parent['leaf'] = _parent_value(parent['children'])\n",
    "    return indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _indexed_leafs(parent):\n",
    "    \"\"\" Return a leaf nodeid -> node dictionary with\n",
    "    \"parent\" and \"leaf\" (average child \"leaf\" value) added to all nodes.\n",
    "    \"\"\"\n",
    "    if not parent.get('children'):\n",
    "        return {parent['nodeid']: parent}\n",
    "    indexed = {}\n",
    "    for child in parent['children']:\n",
    "        child['parent'] = parent\n",
    "        if 'leaf' in child:\n",
    "            indexed[child['nodeid']] = child\n",
    "        else:\n",
    "            indexed.update(_indexed_leafs(child))\n",
    "    parent['leaf'] = _parent_value(parent['children'])\n",
    "    return indexed\n",
    "\n",
    "def _parent_value(children):\n",
    "    \"\"\" Value of the parent node: a weighted sum of child values.\n",
    "    \"\"\"\n",
    "    covers = np.array([child['cover'] for child in children])\n",
    "    covers /= np.sum(covers)\n",
    "    leafs = np.array([child['leaf'] for child in children])\n",
    "    return np.sum(leafs * covers)\n",
    "\n",
    "tree_id = 0\n",
    "\n",
    "# leaf_ids : the leaf id of that example for each tree\n",
    "leaf_id = leaf_ids[tree_id]\n",
    "print(leaf_id)\n",
    "\n",
    "# parse the tree dump into json format\n",
    "t = booster.get_dump(with_stats = True, dump_format = 'json')[tree_id]\n",
    "result = json.loads(t)\n",
    "result\n",
    "\n",
    "\n",
    "indexed = _indexed_leafs(result)\n",
    "indexed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf = indexed[leaf_id]\n",
    "leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0.0\n",
    "score += leaf['leaf']\n",
    "path = [leaf]\n",
    "while 'parent' in path[-1]:\n",
    "    path.append(path[-1]['parent'])\n",
    "path.reverse()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leaf_id = leaf_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match the branch pattern, e.g.\n",
    "# \\t1:[f2<2.3] yes=3,no=4,missing=3,gain=34.829,cover=35.5556'\n",
    "# may have 0 or more than 1 '\\t' at the beginning, which is \n",
    "# used for indicating depth of branch when printed\n",
    "tree_dump = tree_dumps[1].split('\\n')[0]\n",
    "\n",
    "\n",
    "import re\n",
    "line = tree_dump.split('\\n')[0]\n",
    "branch_match = re.match(\n",
    "    '^(\\t*)(\\d+):\\[(.+)<(.+)\\] '\n",
    "    'yes=(\\d+),no=(\\d+),missing=(\\d+),'\n",
    "    'gain=(.+),cover=(.+)$', line)\n",
    "matched = branch_match.groups()\n",
    "matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _target_feature_weights(leaf_ids, tree_dumps, feature_names, xgb_feature_names):\n",
    "    feature_weights = np.zeros(len(feature_names))\n",
    "    # All trees in XGBoost give equal contribution to the prediction:\n",
    "    # it is equal to sum of \"leaf\" values in leafs\n",
    "    # before applying loss-specific function\n",
    "    # (e.g. logistic for \"binary:logistic\" loss).\n",
    "    score = 0\n",
    "    for text_dump, leaf_id in zip(tree_dumps, leaf_ids):\n",
    "        leaf = _indexed_leafs(_parse_tree_dump(text_dump))[leaf_id]\n",
    "        score += leaf['leaf']\n",
    "        path = [leaf]\n",
    "        while 'parent' in path[-1]:\n",
    "            path.append(path[-1]['parent'])\n",
    "        path.reverse()\n",
    "        # Check how each split changes \"leaf\" value\n",
    "        for node, child in zip(path, path[1:]):\n",
    "            idx = xgb_feature_names[node['split']]\n",
    "            feature_weights[idx] += child['leaf'] - node['leaf']\n",
    "        # Root \"leaf\" value is interpreted as bias\n",
    "        feature_weights[feature_names.bias_idx] += path[0]['leaf']\n",
    "    return score, feature_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import DMatrix\n",
    "\n",
    "output_margin=False\n",
    "ntree_limit=0\n",
    "\n",
    "# each record indicating the predicted leaf index of each sample in each tree\n",
    "leaf_preds = booster.predict(dmatrix,\n",
    "    output_margin=output_margin,\n",
    "    ntree_limit=ntree_limit,\n",
    "    pred_leaf=True)[0]\n",
    "\n",
    "leaf_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrib_preds = booster.predict(dmatrix,\n",
    "    output_margin=output_margin,\n",
    "    ntree_limit=ntree_limit,\n",
    "    pred_contribs=True)[0]\n",
    "\n",
    "contrib_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dumps = booster.get_dump(with_stats=True)\n",
    "tree_dumps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = contrib_preds[0]\n",
    "temp[0::n_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[0::n_targets].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[1::n_targets].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[2::n_targets].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5 import explain_weights\n",
    "explain_weights(model_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.dump_model('temp.txt')\n",
    "booster = model_xgb.get_booster()\n",
    "original_feature_names = booster.feature_names\n",
    "\n",
    "features_names = iris.feature_names\n",
    "booster.feature_names = features_names\n",
    "features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgdump = booster.get_dump()\n",
    "xgdump[0]\n",
    "print(booster.get_dump()[0])\n",
    "# recover original feature names\n",
    "booster.feature_names = original_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Feature Contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instances = boston.data[[300, 309]]\n",
    "print(\"Instance 0 prediction:\", model_tree_reg.predict([instances[0]]))\n",
    "print(\"Instance 1 prediction:\", model_tree_reg.predict([instances[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leaf_to_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# contributions = []\n",
    "# for row, leaf in enumerate(leaves):\n",
    "#     path = leaf_to_path[leaf]\n",
    "\n",
    "# leaf = leaves[0]\n",
    "path = leaf_to_path[leaf]\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature = list(model_tree.tree_.feature)\n",
    "\n",
    "path_features = set()\n",
    "path_features_dict = {}\n",
    "\n",
    "for depth in range(len(path) - 1):\n",
    "    path_feature = feature[path[depth]]\n",
    "    path_features.add(path_feature)\n",
    "    contrib = values[path[depth + 1]] - values[path[depth]]\n",
    "    \n",
    "    joint_features = tuple(sorted(path_features))\n",
    "    contrib += path_features_dict.get(joint_features, 0)\n",
    "    path_features_dict[joint_features] = contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias - 2.8953765912305585 - 4.9119953416149027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contributions = []\n",
    "for leaf in leaves:\n",
    "    path = leaf_to_path[leaf]\n",
    "    path_features = set()\n",
    "    path_features_dict = {}\n",
    "\n",
    "    for depth in range(len(path) - 1):\n",
    "        path_feature = feature[path[depth]]\n",
    "        path_features.add(path_feature)\n",
    "        contrib = values[path[depth + 1]] - values[path[depth]]\n",
    "\n",
    "        joint_features = tuple(sorted(path_features))\n",
    "        contrib += path_features_dict.get(joint_features, 0)\n",
    "        path_features_dict[joint_features] = contrib\n",
    "    \n",
    "    contributions.append(path_features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contributions = []\n",
    "\n",
    "for leaf in leaves:\n",
    "    # for each leaf, check which is the path\n",
    "    # that it took to get to the leaf\n",
    "    for path in paths:\n",
    "        if leaf == path[-1]:\n",
    "            break\n",
    "    \n",
    "    # compute the contribution of each feature \n",
    "    # for a given observation\n",
    "    contribs = np.zeros(line_shape)\n",
    "    for depth in range(len(path) - 1):\n",
    "        contrib = values[path[depth + 1]] - values[path[depth]]\n",
    "        feature_idx = feature[path[depth]]\n",
    "        contribs[feature_idx] += contrib\n",
    "    \n",
    "    contributions.append(contribs)\n",
    "    \n",
    "contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_predict_tree(model, X, joint_contribution=joint_contribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tree_explainer import TreeExplainer\n",
    "\n",
    "tree_explain = TreeExplainer(model_tree, iris.feature_names)\n",
    "best_idx, prediction, df_explained = tree_explain.explain(X_train[0])\n",
    "\n",
    "# style the contribution weight\n",
    "# https://pandas.pydata.org/pandas-docs/stable/style.html#Builtin-Styles\n",
    "# http://seaborn.pydata.org/tutorial/color_palettes.html#custom-diverging-palettes-with-diverging-palette\n",
    "cmap = sns.diverging_palette(10, 133, s = 85, l = 60, n = 4, as_cmap = True)\n",
    "df_explained = df_explained.style.background_gradient(cmap = cmap, subset = 'contrib')\n",
    "\n",
    "print('predicted class: ', best_idx)\n",
    "print('prediction: ', prediction)\n",
    "df_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 500)\n",
    "rf.fit(X_train, y_train)\n",
    "accuracy_score(y_test, rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explainer = LimeTabularExplainer(X_train, feature_names = iris.feature_names, \n",
    "                                 class_names = iris.target_names, discretize_continuous = True)\n",
    "\n",
    "i = np.random.randint(0, X_test.shape[0])\n",
    "exp = explainer.explain_instance(X_test[i], rf.predict_proba, num_features=2, top_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp.available_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table = True, show_all = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Blog: Interpreting random forests](http://blog.datadive.net/interpreting-random-forests/)\n",
    "- [Blog: Random forest interpretation with scikit-learn](http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/)\n",
    "- [Blog: Random forest interpretation – conditional feature contributions](http://blog.datadive.net/random-forest-interpretation-conditional-feature-contributions/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
