{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "    \n",
       "    div.cell {\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 12pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "    }\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir(os.path.join('..', 'notebook_format'))\n",
    "from formats import load_style\n",
    "load_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2017-07-25 16:31:25 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 5.4.1\n",
      "\n",
      "numpy 1.13.1\n",
      "tensorflow 1.1.0\n",
      "sklearn 0.18.1\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from subprocess import call\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,tensorflow,sklearn,gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec (Skipgram)\n",
    "\n",
    "At a high level `Word2Vec` is a unsupervised learning algorithm that uses a shallow neural network (with one hidden layer) to learn the vectorial representations of all the unique words/phrases for a given corpus. The advantage that word2vec offers is it tries to preserve the semantic meaning behind those terms. For example, a document may employ the words \"dog\" and \"canine\" to mean the same thing, but never use them together in a sentence. Ideally, the word2vec algorithm would be able to learn the context and place them together in similar vector semantic space.\n",
    "\n",
    "We'll start off by using the Gensim's implementation of the algorithm to provide a high-level intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data:\n",
      "\n",
      " From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "example input:\n",
      " ['From:', 'lerxst@wam.umd.edu', \"(where's\", 'my', 'thing)', 'Subject:', 'WHAT', 'car', 'is', 'this!?', 'Nntp-Posting-Host:', 'rac3.wam.umd.edu', 'Organization:', 'University', 'of', 'Maryland,', 'College', 'Park', 'Lines:', '15', 'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day.', 'It', 'was', 'a', '2-door', 'sports', 'car,', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s.', 'It', 'was', 'called', 'a', 'Bricklin.', 'The', 'doors', 'were', 'really', 'small.', 'In', 'addition,', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body.', 'This', 'is', 'all', 'I', 'know.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name,', 'engine', 'specs,', 'years', 'of', 'production,', 'where', 'this', 'car', 'is', 'made,', 'history,', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car,', 'please', 'e-mail.', 'Thanks,', '-', 'IL', '----', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst', '----']\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset = 'train')\n",
    "\n",
    "# gensim’s Word2vec expects a sequence of sentences as its input,\n",
    "# where each sentence a list of words. We'll be lazy and not perform\n",
    "# any sort of text preprocessing\n",
    "documents = [doc.strip().split() for doc in newsgroups_train.data]\n",
    "\n",
    "# exmaple output of the data\n",
    "print('raw data:\\n\\n', newsgroups_train.data[0])\n",
    "print('example input:\\n', documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44593, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.023563</td>\n",
       "      <td>-2.233488</td>\n",
       "      <td>1.669341</td>\n",
       "      <td>0.415943</td>\n",
       "      <td>0.650172</td>\n",
       "      <td>-2.350407</td>\n",
       "      <td>0.366107</td>\n",
       "      <td>-1.342206</td>\n",
       "      <td>1.275405</td>\n",
       "      <td>-2.084954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.638189</td>\n",
       "      <td>-0.339148</td>\n",
       "      <td>-0.424759</td>\n",
       "      <td>-1.191694</td>\n",
       "      <td>-1.537094</td>\n",
       "      <td>0.685526</td>\n",
       "      <td>-1.552187</td>\n",
       "      <td>1.137069</td>\n",
       "      <td>1.698536</td>\n",
       "      <td>-0.203164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-1.275725</td>\n",
       "      <td>-0.323098</td>\n",
       "      <td>1.490118</td>\n",
       "      <td>-1.649044</td>\n",
       "      <td>0.943990</td>\n",
       "      <td>-2.243789</td>\n",
       "      <td>-0.352389</td>\n",
       "      <td>0.913918</td>\n",
       "      <td>-0.007678</td>\n",
       "      <td>-2.101020</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047411</td>\n",
       "      <td>1.412227</td>\n",
       "      <td>1.244074</td>\n",
       "      <td>0.526244</td>\n",
       "      <td>2.408385</td>\n",
       "      <td>1.202359</td>\n",
       "      <td>-1.039252</td>\n",
       "      <td>0.489790</td>\n",
       "      <td>2.798970</td>\n",
       "      <td>-1.336727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-1.659646</td>\n",
       "      <td>-1.866756</td>\n",
       "      <td>3.310670</td>\n",
       "      <td>-0.185935</td>\n",
       "      <td>-1.897423</td>\n",
       "      <td>-0.170021</td>\n",
       "      <td>1.136050</td>\n",
       "      <td>-2.220443</td>\n",
       "      <td>0.585102</td>\n",
       "      <td>0.266670</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.246955</td>\n",
       "      <td>0.713795</td>\n",
       "      <td>0.782606</td>\n",
       "      <td>1.718141</td>\n",
       "      <td>-2.802217</td>\n",
       "      <td>-1.187473</td>\n",
       "      <td>0.222166</td>\n",
       "      <td>-1.900159</td>\n",
       "      <td>-1.893901</td>\n",
       "      <td>-0.989673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1.071501</td>\n",
       "      <td>-3.333133</td>\n",
       "      <td>0.297913</td>\n",
       "      <td>-0.999243</td>\n",
       "      <td>-0.786862</td>\n",
       "      <td>-1.774176</td>\n",
       "      <td>-1.010605</td>\n",
       "      <td>-0.971707</td>\n",
       "      <td>1.416180</td>\n",
       "      <td>-1.936583</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.365023</td>\n",
       "      <td>-1.177814</td>\n",
       "      <td>1.376837</td>\n",
       "      <td>1.049419</td>\n",
       "      <td>-0.825544</td>\n",
       "      <td>-0.192187</td>\n",
       "      <td>0.275484</td>\n",
       "      <td>1.075993</td>\n",
       "      <td>2.368169</td>\n",
       "      <td>-0.549991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.202602</td>\n",
       "      <td>-0.059123</td>\n",
       "      <td>2.400447</td>\n",
       "      <td>1.077665</td>\n",
       "      <td>1.350307</td>\n",
       "      <td>-0.936709</td>\n",
       "      <td>0.329720</td>\n",
       "      <td>-0.947536</td>\n",
       "      <td>1.280276</td>\n",
       "      <td>-1.987152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512694</td>\n",
       "      <td>-1.145365</td>\n",
       "      <td>0.811848</td>\n",
       "      <td>-1.594738</td>\n",
       "      <td>0.934255</td>\n",
       "      <td>-0.710259</td>\n",
       "      <td>-0.924183</td>\n",
       "      <td>-1.264377</td>\n",
       "      <td>-0.637149</td>\n",
       "      <td>0.633192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "the  0.023563 -2.233488  1.669341  0.415943  0.650172 -2.350407  0.366107   \n",
       "to  -1.275725 -0.323098  1.490118 -1.649044  0.943990 -2.243789 -0.352389   \n",
       "of  -1.659646 -1.866756  3.310670 -0.185935 -1.897423 -0.170021  1.136050   \n",
       "a    1.071501 -3.333133  0.297913 -0.999243 -0.786862 -1.774176 -1.010605   \n",
       "and  0.202602 -0.059123  2.400447  1.077665  1.350307 -0.936709  0.329720   \n",
       "\n",
       "           7         8         9     ...           90        91        92  \\\n",
       "the -1.342206  1.275405 -2.084954    ...    -0.638189 -0.339148 -0.424759   \n",
       "to   0.913918 -0.007678 -2.101020    ...     1.047411  1.412227  1.244074   \n",
       "of  -2.220443  0.585102  0.266670    ...    -1.246955  0.713795  0.782606   \n",
       "a   -0.971707  1.416180 -1.936583    ...    -3.365023 -1.177814  1.376837   \n",
       "and -0.947536  1.280276 -1.987152    ...     0.512694 -1.145365  0.811848   \n",
       "\n",
       "           93        94        95        96        97        98        99  \n",
       "the -1.191694 -1.537094  0.685526 -1.552187  1.137069  1.698536 -0.203164  \n",
       "to   0.526244  2.408385  1.202359 -1.039252  0.489790  2.798970 -1.336727  \n",
       "of   1.718141 -2.802217 -1.187473  0.222166 -1.900159 -1.893901 -0.989673  \n",
       "a    1.049419 -0.825544 -0.192187  0.275484  1.075993  2.368169 -0.549991  \n",
       "and -1.594738  0.934255 -0.710259 -0.924183 -1.264377 -0.637149  0.633192  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apart from the input sentence, the only additional paramter\n",
    "# we'll set is to specify use all possible cpu to train the model\n",
    "workers = cpu_count()\n",
    "word2vec_checkpoint = 'word2vec'\n",
    "\n",
    "word2vec = Word2Vec(documents, workers = workers)\n",
    "word2vec.save(word2vec_checkpoint)\n",
    "word2vec = Word2Vec.load(word2vec_checkpoint)\n",
    "\n",
    "# obtain the learned word vectors (.wv.syn0)\n",
    "# and the vocabulary/word that corresponds to each word vector\n",
    "word_vectors = pd.DataFrame(word2vec.wv.syn0,\n",
    "                            index = word2vec.wv.index2word)\n",
    "\n",
    "print(word_vectors.shape)\n",
    "word_vectors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has learned the word vectors, we can use them to look up related words and phrases (words that have similar semantic meaning) for a given term of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('network', 0.8267158269882202),\n",
       " ('keyboard', 0.8047323822975159),\n",
       " ('application', 0.8029298782348633),\n",
       " ('board', 0.8025233745574951),\n",
       " ('machine', 0.7981600761413574)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar(positive = ['computer'], topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Details\n",
    "\n",
    "The way the model works underneath the hood is that trains a neural network by do the following: Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. To be explicit, there is actually a **window size** hyperparameter to the algorithm that quantifies the word \"nearby\". A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).\n",
    "\n",
    "The diagram below shows some of the training samples (word pairs) we would take from the sentence \"The quick brown fox jumps over the lazy dog.\" We'll used a small window size of 2 just for the example. The word highlighted in blue is the input word.\n",
    "\n",
    "<img src=\"img/skipgram.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "After feeding a bunch of word pairs to the network, it is going to tell us the probability for every word in our vocabulary being \"nearby\" word that we chose. The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if we gave the trained network the input word \"Soviet\", the output probabilities should be much higher for words like \"Union\" and \"Russia\" than for unrelated words like \"watermelon\" and \"kangaroo\".\n",
    "\n",
    "\n",
    "So how is this all represented? First of all, we know we can't feed a word just as a text string to a neural network (or probably any machine learning model), i.e. we need a way to represent the words to the network. To do this, we first build a vocabulary of words from our training documents. We'll assume that our corpus has a vocabulary size of 10,000.\n",
    "\n",
    "We’re going to represent an input word like \"ants\" as a one-hot vector. This vector will have 10,000 components (one for every unqiue word in our vocabulary) and we'll place a \"1\" in the position corresponding to the word \"ants\", and 0s in all of the other positions. The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word. Here’s the architecture of our single-layer neural network.\n",
    "\n",
    "<img src=\"img/word2vec_architecture.png\" width=\"60%\" height=\"60%\">\n",
    "\n",
    "There is no activation function on the hidden layer neurons, but the output neurons use softmax. We’ll come back to this later.\n",
    "\n",
    "When training this network on word pairs, the input is a one-hot vector representing the input word and the training output is also a one-hot vector representing the output word. But when we evaluate the trained network on an input word, the output vector will actually be a probability distribution (i.e., a bunch of floating point values, not a one-hot vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hidden Layer\n",
    "\n",
    "Let's say that we wish to learn word vectors with 300 features. The number of features is a hyperparameter that we would have to tune to our application to see which one yields the best result. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).\n",
    "\n",
    "Now if we look at what would happen when we multiply the 1 x 10,000 one-hot vector representation of the word with a 10,000 x 300 matrix that represents the hidden layer's weight, it will effectively just select the matrix row corresponding to the \"1\". The following figure is a small example that does a matrix multiplication of a 1 x 5 one hot vector with a 5 x 2 hidden layer's weight to give you a visual. \n",
    "\n",
    "<img src=\"img/hidden_layer.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "This means that the hidden layer of this model is really just operating as a lookup table. The output of the hidden layer is just the \"word vector\" for the input word.\n",
    "\n",
    "## The Output Layer\n",
    "\n",
    "The 1 x 300 word vector for \"ants\" then gets fed to the output layer. The output layer is a softmax regression classifier. There's another documentation on Softmax Regression [here](http://nbviewer.jupyter.org/github/ethen8181/machine-learning/blob/master/deep_learning/softmax.ipynb), but the gist of it is that each output neuron, one per word in our vocabulary will produce an output probability between 0 and 1 and the sum of all these output values will add up to 1.\n",
    "\n",
    "Specifically, each output neuron has a weight vector which it multiplies against the word vector from the hidden layer, then it applies the function `exp(x)` to the result. Finally, in order to get the outputs to sum up to 1, we divide this result by the sum of the results from all 10,000 output nodes. Here’s an illustration of calculating the output probability for the word \"car\".\n",
    "\n",
    "<img src=\"img/output_layer.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "Note that neural network does not know anything about the offset of the output word relative to the input word. In other words, it does not learn a different set of probabilities for the word before the input versus the word after.\n",
    "\n",
    "Recall that in the beginning of the documentation, we mentioned that the goal for word2vec is to represent each word in the corpus as the vector representation while trying to reserve semantic meaning. This means that if two different words have very similar \"contexts\" (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if the word vectors are similar. So to hit the notion home, if two words have similar contexts, then word2vec is motivated to learn similar word vectors for these two words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Word2vec\n",
    "\n",
    "You may have noticed that the skip-gram neural network contains a huge number of weights ... For our example with 300 features and a vocab of 10,000 words, that's 3M weights in the hidden layer and output layer each! Training this on a large dataset would be slow and prone to overfitting, so the word2vec authors introduced a number of tweaks to make training feasible.\n",
    "\n",
    "- Treating common word pairs or phrases as single \"words\" in their model\n",
    "- Subsampling frequent words to decrease the number of training examples\n",
    "- Modifying the optimization objective with a technique they called \"Negative Sampling\", which causes each training sample to update only a small percentage of the model’s weights\n",
    "\n",
    "It’s worth noting that subsampling frequent words and applying Negative Sampling not only reduced the compute burden of the training process, but also improved the quality of their resulting word vectors as well.\n",
    "\n",
    "\n",
    "## Negative Sampling\n",
    "\n",
    "Training a neural network means taking a training example and adjusting all of the neuron weights slightly so that it predicts that training sample more accurately. In other words, each training sample will tweak all of the weights in the neural network.\n",
    "\n",
    "As we discussed above, the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples! Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Here’s how it works.\n",
    "\n",
    "When training the network on the word pair (\"fox\", \"quick\"), we want the \"correct output\" of the network, that is the output neuron corresponding to \"quick\" to output a 1, and for all of the other thousands of output neurons to output a 0.\n",
    "\n",
    "With negative sampling, we are instead going to randomly select just a small number of \"negative\" words (let’s say 5) to update the weights for. (In this context, a \"negative\" word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).\n",
    "\n",
    "> The paper says that selecting 5-20 words works well for smaller datasets, and we can get away with only 2-5 words for large datasets.\n",
    "\n",
    "Recall that the output layer of our model has a weight matrix that's 300 x 10,000. So we will just be updating the weights for our positive word (\"quick\"), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!\n",
    "\n",
    "In the hidden layer, only the weights for the input word are updated (this is true whether you’re using Negative Sampling or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Notation\n",
    "\n",
    "This section goes back and re-visit the mathematical notation for the Word2vec skipgram, hopefully, the math won't look so daunting after having an understanding of the model from a non-mathematical standpoint.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "J_{\\theta} \n",
    "&= arg\\underset{\\theta}{max} \n",
    "\\prod_{(w, c) \\in D} p(D = 1 \\big| c, w; \\theta) \\prod_{(w, c) \\in D'} p(D = 0 \\big| c, w; \\theta) \\\\\n",
    "&= arg\\underset{\\theta}{max} \n",
    "\\prod_{(w, c) \\in D} p(D = 1 \\big| c, w; \\theta) \\prod_{(w, c) \\in D'} \\big(1 - p(D = 1 \\big| c, w; \\theta)\\big) \\\\\n",
    "&= arg\\underset{\\theta}{max} \n",
    "\\sum_{(w, c) \\in D} log \\big(p(D = 1 \\big| c, w; \\theta)\\big) \\sum_{(w, c) \\in D'} log \\big(1 - p(D = 1 \\big| c, w; \\theta)\\big) \\\\\n",
    "&= arg\\underset{\\theta}{max} \n",
    "\\sum_{(w, c) \\in D} log \\big( \\dfrac{1}{1 + \\text{exp}(-v_c \\cdot v_w)} \\big) \\sum_{(w, c) \\in D'} log \\big (1 - \\dfrac{1}{1 + \\text{exp}(-v_c \\cdot v_w)} \\big) \\\\\n",
    "&= arg\\underset{\\theta}{max}\n",
    "\\sum_{(w, c) \\in D} log \\big( \\dfrac{1}{1 + \\text{exp}(-v_c \\cdot v_w)} \\big) \\sum_{(w, c) \\in D'} log \\big (\\dfrac{1}{1 + \\text{exp}(v_c \\cdot v_w)} \\big) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \"\"\"Read data into a list of tokens/words\"\"\"\n",
    "    filename = 'text8.zip'\n",
    "    base_url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        call('wget ' + base_url + filename, shell = True)\n",
    "        \n",
    "    with ZipFile(filename) as f:\n",
    "        file = f.namelist()[0]\n",
    "        \n",
    "        # ensure compatibility each python2 and python3's str type\n",
    "        # https://stackoverflow.com/questions/37689802/what-is-tensorflow-compat-as-str\n",
    "        data = tf.compat.as_str(f.read(file)).split()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['anarchism', 'originated', 'as', 'a']\n",
      "data size 17005207\n"
     ]
    }
   ],
   "source": [
    "words = read_data()\n",
    "print('data:', words[:4])\n",
    "print('data size {}'.format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_dataset(words, vocabulary_size):\n",
    "    count = Counter(words).most_common(vocabulary_size - 1)\n",
    "\n",
    "    # build up word index and replaced the words by its assigned indices\n",
    "    data = []\n",
    "    unknown_count = 0\n",
    "    word_index = {word: idx for idx, (word, _) in enumerate(count)}\n",
    "\n",
    "    for word in words:\n",
    "        if word in word_index:\n",
    "            idx = word_index[word]\n",
    "        else:\n",
    "            idx = 0\n",
    "            unknown_count += 1\n",
    "\n",
    "        data.append(idx)\n",
    "\n",
    "    # 'UNK' flag for out of vocabulary word\n",
    "    unknown = 'UNK', unknown_count\n",
    "    count.append(unknown)\n",
    "    word_index_rev = {idx: word for word, idx in word_index.items()}\n",
    "    return data, count, word_index, word_index_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201)]\n",
      "Sample data [5235, 3082, 11, 5, 194, 1, 3134, 45, 58, 155]\n"
     ]
    }
   ],
   "source": [
    "# TODO : ??? do we need to return a 4 element tuple\n",
    "vocabulary_size = 50000\n",
    "data, count, word_index, word_index_rev = build_dataset(words, vocabulary_size)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(indexed_words, window):\n",
    "    \"\"\"\n",
    "    Form training pairs according to the skip-gram model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    indexed_words : list\n",
    "        list of index that represents the words, e.g. [5243, 3083, 11],\n",
    "        and 5243 might represent the word \"Today\"\n",
    "        \n",
    "    window : int\n",
    "        window size of the skip-gram model, where word is sampled before\n",
    "        and after the center word according to this window size\n",
    "    \"\"\"\n",
    "    for index, center in enumerate(indexed_words):\n",
    "        # random integers from `low` (inclusive) to `high` (exclusive)\n",
    "        context = np.random.randint(1, window + 1)\n",
    "\n",
    "        # get a random target before the center word\n",
    "        for target in indexed_words[max(0, index - context):index]:\n",
    "            yield center, target\n",
    "\n",
    "        # get a random target after the center word\n",
    "        for target in indexed_words[(index + 1):(index + 1 + context)]:\n",
    "            yield center, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data: [5235, 3082, 11, 5, 194, 1]\n",
      "skip gram sample:\n",
      "(5235, 3082)\n",
      "(5235, 11)\n",
      "(5235, 5)\n",
      "(3082, 5235)\n",
      "(3082, 11)\n",
      "(3082, 5)\n",
      "(3082, 194)\n",
      "(11, 3082)\n"
     ]
    }
   ],
   "source": [
    "iterator = generate_sample(indexed_words = data, window = 3)\n",
    "\n",
    "print('original data:', data[:6])\n",
    "print('skip gram sample:')\n",
    "\n",
    "# we start off by using the first word as the center word,\n",
    "# and since there's no word before it, we will not have any\n",
    "# sampled word before it; after that we keep sliding the center\n",
    "# word and generate word pairs\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(iterator, batch_size):\n",
    "    \"\"\"Group a numerical stream into batches and yield them as Numpy arrays\"\"\"\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype = np.int32)\n",
    "        target_batch = np.zeros((batch_size, 1), dtype = np.int32)\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(iterator)\n",
    "\n",
    "        yield center_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5235 3082 3082 3082 3082]\n",
      "[[3082]\n",
      " [5235]\n",
      " [  11]\n",
      " [   5]\n",
      " [ 194]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "iterator = generate_sample(indexed_words = data, window = 3)\n",
    "batches = get_batch(iterator, batch_size)\n",
    "\n",
    "# e.g. generate a batch\n",
    "center_batch, target_batch = next(batches)\n",
    "print(center_batch)\n",
    "print(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 128 # dimension of the word embedding vectors\n",
    "NUM_SAMPLED = 64 # number of negative examples to sample\n",
    "LEARNING_RATE = 1.0\n",
    "SKIP_WINDOW = 1 \n",
    "# batch_gen = process_data(VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Define placeholders for input and output. Input is the center word and output is the target (context) word. Instead of using one-hot vectors, we input the index of those words directly.\n",
    "\n",
    "```python\n",
    "# explicitly naming our operations will make it easier later to track them\n",
    "center_words = tf.placeholder(tf.int32, shape = [BATCH_SIZE], name = 'center_words')\n",
    "\n",
    "# for target_words:\n",
    "# we will use this with tensorflow's NCE loss later, and the function requires rank 2\n",
    "# input, that's why there's an extra dimension in the shape\n",
    "target_words = tf.placeholder(tf.int32, shape = [BATCH_SIZE, 1], name = 'target_words')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Define the weight/variable. In this case, the embedding matrix. Each row corresponds to the representation vector of one word. If one word is represented with a vector of size EMBED_SIZE, then the embedding matrix will have shape [VOCAB_SIZE, EMBED_SIZE]. We initialize the embedding matrix to value from a random distribution. In this case, let’s choose uniform distribution.\n",
    "\n",
    "```python\n",
    "# word vectors\n",
    "embed_matrix = tf.Variable(\n",
    "    tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0), name = 'embed_matrix')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Inference (compute the forward path of the graph). Recall that the hidden layer serves as a lookup table and its purpose is to get the vector representations of words in our dictionary.\n",
    "\n",
    "<img src=\"img/hidden_layer.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "i.e. The output of the hidden layer is just the \"word vector\" for the input word. Our embed_matrix has dimension [VOCAB_SIZE x EMBED_SIZE], with each row of the embedding matrix corresponds to the vector representation of the word at that index. So to get the representation of all the center words in the batch, we get the slice of all corresponding rows in the embedding matrix. TensorFlow provides a convenient method to do so called `tf.nn.embedding_lookup()`. This method is really useful when it comes to matrix multiplication with one-hot vectors because it saves us from doing a bunch of unnecessary computation that will return 0 anyway.\n",
    "\n",
    "```python\n",
    "# input -> hidden layer\n",
    "embed = tf.nn.embedding_lookup(embed_matrix, center_words)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Define the loss function and optimizer** For nce_loss, we need weights and biases for the hidden layer to calculate NCE loss.\n",
    "\n",
    "```python\n",
    "# hidden layer -> output layer's weights\n",
    "nce_weight = tf.Variable(\n",
    "    tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE], stddev=1.0 / EMBED_SIZE ** 0.5))\n",
    "\n",
    "nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "\n",
    "# hidden layer -> output layer + NCE loss\n",
    "loss = tf.nn.nce_loss(weights = nce_weight, biases = nce_bias,\n",
    "                      labels = target_words, inputs = embed,\n",
    "                      num_sampled = NUM_SAMPLED, num_classes = VOCAB_SIZE)\n",
    "avg_loss = tf.reduce_mean(loss, name = 'loss')\n",
    "\n",
    "# choose an optimizer to perform the heavy lifting\n",
    "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "optimize = optimizer.minimize(avg_loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the operations we will create the session to execute the computation, including feeding in the inputs, running the optimizer to minimize the objective function we just defined and fetch the loss value so we can check convergence. The following code chunk pretty much dumped everything into one giant function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??tf_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tf_word2vec_no_frill import tf_word2vec\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 256\n",
    "EMBED_SIZE = 128  # dimension of the word embedding vectors\n",
    "WINDOW_SIZE = 5  # the context window \n",
    "NUM_SAMPLED = 64  # Number of negative examples to sample.\n",
    "LEARNING_RATE = 0.5\n",
    "EPOCHS = 1000\n",
    "TENSORBOARD = './graphs/no_frills/'\n",
    "word2vec_param = {'vocab_size': VOCAB_SIZE, 'batch_size': BATCH_SIZE,\n",
    "                  'embed_size': EMBED_SIZE, 'num_sampled': NUM_SAMPLED,\n",
    "                  'learning_rate': LEARNING_RATE, 'epochs': EPOCHS,\n",
    "                  'tensorboard': TENSORBOARD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = read_data()\n",
    "# words = [word for doc in documents for word in doc]\n",
    "data, count, word_index, word_index_rev = build_dataset(words, VOCAB_SIZE)\n",
    "iterator = generate_sample(indexed_words = data, window = WINDOW_SIZE)\n",
    "batch_gen = get_batch(iterator, BATCH_SIZE)\n",
    "word_vectors, history = tf_word2vec(batch_gen, **word2vec_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAGKCAYAAAAL9ei2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnWeYHMW1hr8zs6tVFiggJEAIEIgcRY4GLgbjgPG1jQk2\ntq8TxjkJgzG2MehiLjbY5GiisQ0GY4mMUEIBCZCQhHLOeaXNOzN1f3RXT3VNdXWYsLOr8z6PtDMd\nqqp7Zvqrc+rUKRJCgGEYhmGYrkuqoxvAMAzDMEx5YbFnGIZhmC4Oiz3DMAzDdHFY7BmGYRimi8Ni\nzzAMwzBdHBZ7hmEYhunisNgzDLNbQkRXE1Gmo9vBMJWAxZ5hSggRDSCi24hoIRG1ENEmIppIRF8m\nopqObt/uBBEJ5V8jEc0moq8XWeZDRPR2iZrIMBWDHz4MUyKIaD8AkwFkANwI4H0A7QBOA/BTAHMA\nfNBhDYwAEaUAkBAi29FtKRHXAngOQB8AXwPwEBHVCyH+2bHNYpjKwpY9w5SOewDUATheCPGUEGK+\nEGKxEOKvAE4AsBgAiKiWiMYQ0VoiaiOi+UR0uVqQa41eQ0RPENEuIlpDRNcp+39PRAv1BhDRvUQ0\nWXl/AhG9RkQNRLSZiJ4nov2V/TcR0RIi+iIRLQDQBuAQIkoR0S3uObuI6Cki+oHu9iai/yKiKUTU\n7F7Po0Q0QNn/GBG9QUTfJKKVRLSTiP5NRIO1cs4noklE1ERE9UQ0gYgOUvZfRkQfuN6SFUR0BxH1\nivCZ1AshNrifw3XuZ3Bp0MFE9AkimkVEra5X5h5ZDxHdBODrAM5WPAZXR2gDw3Q4LPYMUwKIqD+A\nTwD4ixCiXt8vhGgXQjS6b28B8A0APwRwJIAnATxJROdpp/0awEQAxwK4FcAtyjF/hSPKJyttqAPw\nRQCPu+8PBzABwFQAowCcCyAL4HUi6q7UMxTANQC+AuBwAGvctn0fwI8BHA9gFhxvhXrN5wJ4EcDf\nABwN4BIAwwE8T0SkHHoigI8BuBjAxwEcBeB2pZzzAbzq1nEqgJMAPAbX8+gK6r0A/s9t35cBnA/g\nPsSnBUA30w4iOhrAv+Hc82Pg3I9PKvXcDuBpOPdziPvv2QRtYJjKI4Tgf/yP/xX5D45ACQCXhhzX\nE0ArgGu07f8C8JbyXgC4SzvmIwC3Ku+nAbhbef/fAJoB7OG+fwzA37Qy6gA0AbjEfX8TgByAYdpx\nawH8Ttv2NwAZ5f3bAMZoxwxz236s0oZNAOqUY34BYL3yfhKA/1ju2QoA39a2neXWs6flPAHgSvd1\nDYD/cbd92912tXY9TwCYoZXxGff+7O++fwjA2x39feN//C/uP7bsGaY0UPghAIARcCzLidr2CQCO\n0Lbp4/vrAKju778C+CIR1brvvwzg30KIHe77EwF81nXhNxBRA4CtALoDOFgpZ6MQYpV3IUT94Fj7\n07T6p2rvTwTwQ638+e4+tfwFQohWy3WcAOA1GCCiQQD2B3CHVs/L7iEjTOcpPOQe3wLgjwDGALg/\n4NgjYP5cCI5HgWE6LRygxzClYTEcC/BwAM+XqMw27b2Af+jtbwD+BOBiIpoC4EI4rnRJCo61OsZQ\n9lbldaNhv6zPRgrA/7p16GxQXpuuI2rnSF7vDwCMN+xfE3L+9XCGGhrgdGp4mU9mt4TFnmFKgBBi\nGxG9DOBaIvqz0MbtXeu7G4AlcNz4ZwGYqxxytvY+Sp3bieglAFfBcZ9vgzP2LZkJZyx9aRyRE0LU\nE9E6OOPn45Rdp2iHzgRwhBBiSZx2G5gF4AIAdxnaspGIVgMYKYR4MEHZG2O0bx6cz0XlbDidk3nu\n+zYA6QTtYJgOhd34DFM6roEz1W4WEV1ORIcT0QgiuhKOMB4shGiCI2q/I6LPE9EhRPRLOGPDtySo\n83E4QWTfBvCU8E+ZuwXAYXCC/04iogOI6GNEdCcRHRhS7v/BcdFfQUQHE9EP4Qiy2mm4EcBn3Mj4\nY4noICK6kIgeJqIeMa7hdwAuIqI/EdHRRDSSnIQ3I9391wP4PhFdT0RHuvsvIaIgd3xS/gDgeCL6\nIxEdSkQXAvgznPsqhzmWAziUiI4gooFuUCTDVD0s9gxTIlxBOB7AC3AC394D8A6Ab8GJ6JaW+/UA\nHoTjgp8L4Eo4gWRvJqj2ZQD1cET9ca09H8GZ498bjsU/3623B4AdsPMnAH8BcCecfAGnwOkAtCjl\nj4cT4X80nCC7OXDGxXfB6fREQgjxGpyZDCcDmA5gBpxI+HZ3/xMAvgCnUzMDwLtw7u/aqHVEbMcc\nAJ+GY93PhjM8MRZOR0rysFv/OwA2A/hSKdvAMOWCeAiLYZgoENEjAI4RQpzQ0W1hGCYePGbPMEwB\nRDQUwGfhBMVlAXwKTrT/tR3ZLoZhksGWPcMwBbgZ7p6F46LvDiew8M8Jg+QYhulgWOwZhmEYpovD\nAXoMwzAM08VhsWcYhmGYLk6XCdAbOHCgGD58eEc3g2EYhmEqxqxZs7YIIQaFHddlxH748OGYOXNm\nRzeDYRiGYSoGEa2Mchy78RmGYRimi8NizzAMwzBdHBZ7hmEYhunisNgzDMMwTBeHxZ5hGIZhujgs\n9gzDMAzTxWGxZxiGYZguDos9wzAMw3RxWOwZhmEYpovDYs8wDMMwXZyKiT0RXUtEM4molYgei3jO\nm0QkiKjLpPVlGIZhmEpTSct+HYCbATwS5WAiugJAbVlbFIElm3Z1dBMYhmEYpigqJvZCiOeFEC8A\n2Bp2LBH1A/BrAD8ve8MsjF+wCeffMREvvL+2I5vBMAzDMEVRrWP2twC4F8AG20FE9E13aGDm5s2b\nS96IRRsdq37++p0lL5thGIZhKkXViT0RjQJwOoA/hx0rhHhACDFKCDFq0KDQ5XwTI4QoW9kMwzAM\nU26qSuyJKAXgHgA/EEJkOr49zl/WeoZhGKYzU1ViD6AvgFEAniWiDQDedbevIaIzO65ZDMMwDNN5\nqdiUNnf6XA2ANIA0EXUHkNEs+HoAQ5X3+wGYAeAEAKUflGcYhmGY3YBKWvY3AGgGMBrAle7rG4ho\nGBE1ENEw4bBB/kNe4DcKIdoq2FYAAMHx47MXn2EYhunMVMyyF0LcBOCmgN29A85ZAbiKyzAMwzBM\nIqptzJ5hGIZhmBLDYm+Bo/EZhmGYrgCLPcMwDMN0cVjsGYZhGKaLw2LPMAzDMF0cFvsICJ58xzAM\nw3RiWOwtEPGsP4ZhGKbzw2LPMAzDMF0cFvsI8NQ7hmEYpjPDYm+BnfgMwzBMV4DFnmEYhmG6OCz2\nDMMwDNPFYbG3kE+Xy4P2DMMwTOeFxZ5hGIZhujgs9gzDMAzTxWGxt8DR+AzDMExXgMU+AjxizzAM\nw3RmWOwtcLpchmEYpivAYs8wDMMwXRwW+wjwzDuGYRimM8Nib8GbZ8+j9gzDMEwnhsWeYRiGYbo4\nLPYMwzAM08VhsbcgY/F5zJ5hGIbpzLDYMwzDMEwXh8WeYRiGYbo4LPYMwzAM08VhsbcgtL8MwzAM\n0xlhsbfAgXkMwzBMV4DF3oJgtWcYhmG6ACz2FnKu1rPmMwzDMJ0ZFnsLrPEMwzBMV4DF3gK78RmG\nYZiuAIu9hbzWs+gzDMMwnRcWewtytTs28BmGYZjODIu9BRZ5hmEYpivAYm8hx2LPMAzDdAFY7C2w\nG59hGIbpCrDYW2CRZxiGYboCLPYWeOodwzAM0xVgsbfAWs8wDMN0BVjsLXjpcnmePcMwDNOJqZjY\nE9G1RDSTiFqJ6DHLcV8hollEtJOI1hDRbURUU6l2qrDIMwzDMF2BSlr26wDcDOCRkON6AvghgIEA\nTgZwHoCflrdpZgQvhMMwDMN0ASpmMQshngcAIhoFYF/Lcfcqb9cS0VMAPlbm5gW1xfnbEZUzDMMw\nTInoDGP2ZwGY1xEVS5HPsWnPMAzDdGKqWuyJ6GsARgG4PWD/N904gJmbN28uef2exrPWMwzDMJ2Y\nqhV7IroEwK0ALhJCbDEdI4R4QAgxSggxatCgQSVvg7To2bJnGIZhOjMdEuUeBhFdCOBBABcLIT7s\nqHbk3fgd1QKGYRiGKZ6Kib07fa4GQBpAmoi6A8gIITLacecCeArAZ4UQMyrVPhNs2TMMwzBdgUq6\n8W8A0AxgNIAr3dc3ENEwImogomHucb8C0A/AOHd7AxG9XMF25hG+PwzDMAzTKank1LubANwUsLu3\nclyHTLMz4cXnsWXPMAzDdGKqNkCvGsi5g/VZHrRnGIZhOjEs9hbyln2HNoNhGIZhioLF3oIUeQ7Q\nYxiGYTozLPYWpMjHcePPWbMDw0ePxbRlW8vVLIZhGIaJBYt9BOIM2U9Z4oj8+IWbytQahmEYhokH\ni70FwfPsGYZhmC4Ai30AQgi0tOcAcDQ+wzAM07lhsQ/gj28sxrMzVwNIKPbcP2AYhmGqBBb7AJ6b\ntcZ7zV58hmEYpjPDYh9At5r8rcmy2jMMwzCdGBb7AGrT5L2O48YnCj+GYRiGYSoJi72BXE74pttx\nND7DMAzTmWGxNzB12VYs2dTgvWexZxiGYTozLPYGdFd8Nhe/DO4eMAzDMNUCi72BtKb2uRLOs99Q\n34I3P9pYsvIYhmEYJgwWewOplF/sSxmNf+k9U/D1v84sWXkMwzAMEwaLvYGUbtnHEPuwYPx19S0J\nWsQwDMMwyWGxN6AZ9iV145ezTIZhGIYxwWJvIF1GN76EI/wZhmGYSsFib6DAjZ8kGj9EzNmwZxiG\nYSoFi72BYsbso8KWPcMwDFMpWOwNqG78/z5h30Sr3lFI3lwWe4ZhGKZSsNgbUIfsu9WkIgvz5MVb\nsHDDLgDsxmcYhmGqh5qObkA1os6zTxNFFuYrH54euQ627BmGYZhKwZa9AXXMPkXxVr2LCk+9YxiG\nYSoFi70BNV1uKkXlmWfPWs8wDMNUCBZ7AynlrqSJfPPsW9qzmLBoc9F1sBufYRiGqRQs9gZUN346\nRT5hfnzqCnzlkRl4bd4G3zmNrZlYdbDYMwzDMJWCxd6AOvWOiHxJddKu2f/qPP/KdWfeNj5WHUkS\n9TAMwzBMEljsDahT5NMpf7rcPnXOBIbtTW2+c7Y1+t+b2KAsgsOWPcMwDFMpWOwNqAF6aSJfNL4U\n/pb2rLUMk5bfPHa+95rFnmEYhqkULPYGdDc+kE+SI4U/TOxNqPLObnyGYRimUrDYGyAtQA/Ii7y0\nyLc0tIVmyStAOZwte4ZhGKZSsNgbUC17T+w1y37VtiY8M2N1rHJVgWexZxiGYSoFi70BNTe+nIYn\n3e7q+P1bC/wR+SqtmRzqm9p924TPsi++nQzDMAwTBRZ7A3q6XCBviasWeffadGAZT0xbiWN++5pv\nbJ8te4ZhGKYjYLE3oCfVAVQ3fv64Hhaxlxz6q1ewbkczAC1Aj8WeYRiGqRAs9gbUMfu8G7/Qsq9J\nR7t989btBKC58SNE46/a2oSGmJn5GIZhGEaHxd5AypdUxxV7V6jVMfuoQry1odV9Fc+Nf9YfxuNL\nD0yLVAfDMAzDBMFib0CdeleTdl63u/57KfZH7dMPu1raC082sNXNricSTL37cG19pOMYhmEYJggW\n+xC61zjj8jLQLicEiIA+3WvQ0BLNspepdP0BevHb8pe3FltnADAMwzCMiZqObkC1IyPuW9rzln2a\nCDXpFJojZtFranM6BXEC9HKG3sDtry0CAKwYc3GkejOuNyJqbAHDMAzTNWEVCKGuxrlF0rLPCoFU\nipAisyCbaM04oqvqe1j2vUwJJuKfd8cEjLj+5aLLYRiGYTo3FRN7IrqWiGYSUSsRPRZy7I+IaAMR\n7SSiR4iorkLNLEBa9pc/6ATK5VzLPk0U2RXf5oq9as1nQ6LxsyUQ+5Vbm4oug2EYhun8VNKyXwfg\nZgCP2A4ioo8DGA3gPAD7AzgQwG/K3roAutc6t6ixzbXsc06EPimr4bVm7O58KfYqYW78dl4pJzav\nzF2PX70wt6ObwTAMU3VUTOyFEM8LIV4AsDXk0K8AeFgIMU8IsR3AbwFcXe72BaFnycsJgRQ569xL\nwf7ZP+ZYyzC58cPEPpvlpDtx+faT7+GJaSs7uhkMwzBVRzWO2R8BYLbyfjaAwUQ0QD+QiL7pDg3M\n3Lx5c1kaIy17ALjq4elOgF6KkCLCgg27MHz0WExYZK9bWvZCnWcfYrizZc8wDMOUimoU+94A1Mnl\nO92/ffQDhRAPCCFGCSFGDRo0qCyNqavJW/aTFm9BVrhir2Te6VZjv40trps/lmXPK+UwDMMwJaIa\nxb4BQF/lfT/3764OaAvqav23KJcTSBH58ud3C5na1tyWn6PvlRMWjc9ufIZhGKZEVKPYzwNwjPL+\nGAAbhRBhY/1lQR+zl278tLoMbshdlNP2/FPv7OeUYuodwzAMwwCVnXpXQ0TdAaQBpImoOxGZkvo8\nDuDrRHQ4Ee0J4FcAHqtUO3W61xSKvW7Zq69NyOQ7qnyHuemzPGbPMAzDlIhKWvY3AGiGM63uSvf1\nDUQ0jIgaiGgYAAghXgFwG4DxAFYCWA7g1xVsp49uNSnss0cP731WCNSk/WP2YcjseyKCGz+XE9hQ\n34J2duMzDMMwJaKSU+9uEkKQ9u8mIcQqIURvIcQq5dg7hBCDhRB9hRBfFUK02souN5efPMx7LdPl\nxtB6L22tP0DPfOydby7GKbe+yQlxGIZhmJJRjWP2VcFlJ+6HGz95OACgRlH2uWvrkUqRb837MKSV\nrup7ULrctxduAgBsqG/2tn35kRmR62IYhmEYHRb7AMZ87mh87YwDAMAn7Cu2NiFN5FsGt2c3+3pC\ncs68KvD1ze1e4F4mm8OjU5ajLZPzLH7V8p8YMo+fYRiGYWyw2EdAt+JTKSc3vmRAr27W84Vw3P+q\nLT/6+Q/x3/e9AwB4duZq/Oal+Xhg4lIvcE8f0w9bOIdhGIZhgmCxj0CNJvbpFHxj9qbc9zqtmWzB\nKnlz1zr5ghpbnSVwdzS1BwfusdYzDMMwCeH17COQ1ibSp8kfjf/equ2hZRx+46uB++TUvZzIW/S6\n6Icl4SkVm3a2oC2bw7579qxIfQzDMEz5YbGPgG7Zp1L+efbFJsDJi73w3Pj61LtKif1Jt7wJAFgx\n5uKK1McwDMOUH3bjR0DPmZOmeNH4YciyckJ40/P0oYGOGrJvy+Rw9/glXjBhOcp/ZPJyb3oiwzAM\nU3rYsk9AKkUFHYCk/PJfH2LG8m0AXMveVfU2TfwqZdnrPD19Jf7w6kJkcwLfPOtAvLdqO047aGDR\n5ba0ZyEE8MiU5fjDqwtRW5PCVafsn6isrQ2tuOyBaUW3iWEYpqvCln0EdJlNkz8avxienr4KSzY1\nAACyuXwaXd2yL0eAXkt7YdCgTqO7iE9TWxY3/XseLn9wOpZsKn5NotPGvIXDbnwFO5vbnXrcIMUk\njJu7AYvde8gwDMMUwmIfBU0P09qYfanI5YQnvs2a27wclv2hv3oFv3huTqRj566t9wIR65szaM1k\n8b1n3vc6KnHZ1tgGoLAjxTAMw5QeFvsEpFLxcuNHJSuEZ8HLZXElokxD2v+Ytca6X3Y+Ji/ZgkUb\npbALrN7WhJdmr8On/zK5qPpl/oDS383Ksb2xDcNHj8XYOes7uikMwzBGWOwTkCbEyo0flZwQngWv\nu7VVy37ch+sjTfcrBSbLWwh4GQSb2ooL3JOXVYyjpKM7Cks3O52gR6Ys7+CWMAzDmGGxj4DQJC+d\nKt2YvUoulxd7XURVsb/mqfdw6T1O9r0lmxq8fPpxiJqRL2j4oFQZ/WQp1OGSzTAM03XhaPwEpKg8\nbnwnqY7zuqnNb9n/M8Ddfv4dEwDEnxcfNeDPpOkixvlRKUPfiWEYhnFhyz4BZQvQU5Lq6Jb9rS8v\niFVWfVN7aF1RCDoqq6n9VQ9Px/0TlkYq01d+GSL0eB0BhmEYPyz2EdC1w1nitvT15EQ+Gr/YsfAv\nPWifdx45ut9wnBCF509avCV2hwQoHCIpBbuD1je0ZrCjqa2jm8EwTCeBxT4Cpnn25Zl6By+pju7G\nj8v89Tt973VrN6ogmtz1aqY/AKFz9W3kA/Si389sTvgy7umn7gZaj9NufRPH/vb1jm4GwzCdBBb7\nCPSoTfvepwhlEfusLxo/vmW/elsTPnfvOxj3oX8K2LRlW3HAdeN8EfzRxb7wwKwSSAj4cwI8OHFZ\nzFY7qHdTCGG1Wi++axJGXP9y4P7dwY2/s6W4ziDDMLsXLPYR+NQxQzH6okPxzbMO9LaVZepdTpln\nnyAX/fUvzMWsldtxzVPv+bZPWLQZADB16dZ8XUWM2WeUdgL+IYenZ6yK3mAFte90z9tLcexvX8eG\n+hbjsQs22DP4dX2pZxiGiQeLfQTSKcK3zz4I/XrUAnDEJGghnC+dtB/27Fnr2/bGj89Cn7rwiQ9Z\nZcw+LkKIQIv23redwDlVPCOLveGwbC7nC9BTEwDFzfRnavMrczcAADbuNIu9jj5tbzcw7BmGYWLB\nYh8DdanboDHm/fr3xAn79wcA9OqWxr1XHI8Re/XBwYN7h5afE8mXy83mRKjIPTFtJXa1tHt1RcEU\nQNee9XcsVC+EHqUfXr6DejdlGUlXFixH0B/DMExnhsU+Bqr46EJ0iCLmtWln3xdO3A8XHTXEeLyJ\nYsaaMzkRSeSkFR65LqNl73fjq9a87ploac9i+rKtCMIUoCfLSxoXwZY9wzCMHxb7GKjio2v3uYcO\n9l7XuPPy/MeHC1dcq1glkxPIxcifX0yAXkYL0FPbndWO/81L8/DFB6YFLpgjOyjq7SnWsmcYhmH8\nsNjHoCYdTbxrXZFStSqKcOnCGkfsstloln1QXcHHGerK5fzWvE/4/ccudIPp1Oh61avgWfZq+Z5l\nH6mJDMMwTAgs9jGIYqkLke8UqMdEEntNKOPk38/kcrHc18Wky81k/fEBapyB3omoSTlfMZ/1b6pc\ndePH9HAUzLNnNz7DMIwPFvsYyAA9Z9U3/77DhvQBAIzYq7fnxqe4bnxNpVIxPh1nzD6cL9w/FUD0\nMXtTqZmc8Al2Jhss9vIa1GtTOwemVsjd+v2ISqUD9LhvwTBMtcML4cRAXfxmu5Z7/tPHDMUhg/vg\nsCF98c6SLc7xAW78ffbogbU7mgvKb8v4Tft4ln3w1DuVFVubsK2xrTjLXhuzb1d892onIJcTmLZs\nm/vaf75efmNrBtmcQDpFXhlxYhDC2lxO2JPAMEy1E9l2JKIfE9Gx7utTiGgVES0nolPL17zqQhXf\nLQ2tvn1EhMOG9AWQD9BTBV6+/NQxQ3HcsD0AoGA+fqOWIjfOynrZbPjUO8mUJVt8Yh13FkA26x8y\nCBL7f89e573OKMqd8Q3sO8ePeXkBxrz8EYC8dyDunH1/iZUjaTsZhmEqRRw3/o8ALHdf3wrgDgA3\nA/hTqRtVrUjxFgBa24PNTjlmb3Ljpwjo2c1Jv6tnyWvRFr+JE6A3Z+0O1DfbV7qTvLN0S2A0vU6U\naPwgN/52JSjP7wkotOwB4KXZ633tiSqi+l2qdLpcFnuGYaqdOGLfTwhRT0R9ABwD4M9CiIcBjCxP\n06oP1av+g/MOxjXnHGQ8rsYSjU8ArjlnBA7duw/+6/C9fec1aeIfx41/7dPvY3HA9DadTTtb/QvZ\nWLTKJGRbGtpw15uLvfeqZa+63v1BefntmQD/vLxHss4oUxEfnbIcL7sZ9ySVlt5q1vpbx32E4aPH\ndnQzGIbpYOKI/WoiOg3AZQAmCiGyRNQXQHFrsXZS+vWsxc8vPNS4T6ZvVdO4prwOAGH4wF545Ydn\nYUCvbr7z9GVtwyz7SYs3x243ALRmcprYB6uVadd9E5Zi9pp67317wDz7oAj8TIBlL683b9lbLsLl\nNy/N93L/29pcTqrZsr8/4cJEDMN0LeIE6P0MwD8BtAH4nLvtkwBmlLpR1U6Ym3hHs+O+7tcjf3s9\nK92i3wUBeiFif93zH1r3B9HSng2cJ6/SmsniqenhC9v8TVn8xifqOdV1nzO+ViPnazzL3t6uUEJO\na2nPYsmmBhy5T79k5WsUkQuJYRimIkS27IUQ44QQQ4UQw4UQs9zN/wDw6fI0rfow5cM/ZHBvbwxe\nsmWXI/YD+9R529KKZR+VsGPlwjxxac3k8P7q/HK3Qe7yFktcgso7S/3pcBdt3IXho8f6VqdTOzKm\naHwg7/3IedH4pZt6t2Z7E86/YwI27mzBL5//EJ/882RsirjQThhROiU7mtqwdHO0YRaGYZhSEyca\n/3AiGuy+7k1EvwHwSwDJFKeL8MoPzsLcmz7u2yYj9Qf2zou9FO44SeHC5tn36Z5s5mRLexY/ena2\n9z5oilvC1PR49t3VAIA35m/0tgVb9nmkZS+HAhLPszec9uS0VViyqQH/nLUG761yOjoNrcnWhG9q\ny+Cbj8/0pk9GCQj8xJ2TcN7/TUhUH8MwTLHEGbN/BsAe7uvbAZwF4BQA95e6UdWO+mhPpahgitzF\nRzuL3xwyuI+3zZ2N57PWw8T09IMGWpfG7V1Xm0iQ9UC+IMs0qRd9g2sxD1I8G21ZdepdtDH7Unrx\n5UekCnPQyoVhvDpvA16bvxG3vbIAQLR8AOvqS+NFSEqlZygwDFNdxBH74UKIheQ8IS8F8HkA/w3g\n4/bTug5RpeHq04Zj8e8vQn8lAM+Lxo+hL91r07jvqhMC9/fpXhMrYj8IaUHrgpBUIDa6wrZBcZP7\n3filicbP6In4XUztznckzMvqxkHP51/NAXoSjitgmN2bOGLf4k67OwnAKiHEFgCtALqXpWWdGCJC\nbdp/az03vmrZh8hNOkXWIL29+3UPTLwjvQtRmLduJ4BCQUgqEHK+vyrw0rJvz+bw+vxN3nZ1fL0w\nGt/egLYgsTdsk/c9J/I5ApL2k/RleTuDkLJlzzC7N3HE/mkAbwH4K4DH3G3HI59oZ/chwXMzbZh7\nH+Ucm9jf+/bSggh+yXmH7hW5nq884kyo0C3pUgpEe8Yp64+vL8J9E5YqleRfxo3GDwogNJ0mPSBC\nCMUyT6b2umegMwhp9beQYZhyEjnCSwjxIyK6AEC7EGK8uzkHJ7PebkExHvO8ZR/vnKRruseJ+pfo\n4prUYjVzA6QnAAAgAElEQVSdJoPyVmxtDDxWb3PYWHhrJnqKB3kbs6rYB9yi91ZtR3NbFqePGGjc\nLzQ/fmew7DvDUAPDMOUj1qp3QojXACwlolOJaJgQYqYQ4q0yta1qSbKqWj5dbvQAvXQqXhY9lSSn\n6YKQ1GI1nSfjAnQBV4/VOzZh0fiBlr3h80mlorvcL73nHVzx0PTA/bKIlDI0UO10RBOfmbEKv/vP\n/MpXzDBl4s43FmP46LG+GUWdhThT74YQ0QQAiwE8D2AJEU0goqFla10XQg7hq3IWpMdSqNNFWPZJ\nIs0L3PiJagaWbm4s2CYF0dZR0q81rLMRaNkbTvOEWVkdMLGnpogAvc7g8i8V1z3/IR6evPuN8jFd\nl/snOkOQrQHDp9VMHMv+XgCzAfQXQgwBsCeADwDcV46GVSNJx3iBvGUZRYR7davxzknuxo9/TmGA\nXumEKReQAld9W2DZh7nxAy37QmRnKyeKX+1eLyHObeool39n8D4wDFM+4oj9GQB+IoRoBAD3788B\nnBblZCLqT0T/IqJGIlpJRJcHHEdEdDMRrSWieiJ6m4iOiNHOsiGn0g3r3yv2uekYY/Zy1bw0kRe0\nFpckHRM9Y10phUmWVTi9L/9av9YwgQrqXZtOk5Z9Npffn3gevzbmH0dIoyzuUw5Y6xlm9yaO2G8H\ncLi2bSSAHRHPvxtOXv3BAK4AcG+AiH8ewNcAnAmgP4CpAJ6I0c6ycepBA/DI1aPwkwsOiX2uKV1u\nkPDXuKnzTAl7opLMstfEvoTC9M9Za9DSni0QHZtlHyaigfPsDba7OvVO7i82aY/sUMnbFOWWd5SF\nzVrPMMWj59joTMQR+9sAvEFEY4joO0Q0BsDr7nYrRNQLzuI5vxJCNAghJgN4EcBVhsMPADBZCLFM\nCJEF8CQKOxkdxrmHDi6YQx8FU3R8kEu/Np3vGESx7M8ZOQj77tkjUtk2kqanjUJ9czv+8OpCaxCg\nLvbqQj/vrtiGW1/+yLc/E9AZMU+9y9fnWfYhEnjNU7OM2zujZc9ufIYpHZ3x1xRnIZwHAXwRwEAA\nn3L/Xg5g3winHwIgI4RYpGybDcBk2f8NwEFEdAgR1QL4CoBXTIUS0TeJaCYRzdy8Odlyr5VCClnY\nM/feK473jk2nok2hq0mlCjoFiSx7zVAutUBs3tVa8CPxp8v1fx3VJX8/f99U3D9hma9zsHJbk7Ee\nW1KdrDJmH3Z54z7cEFC+P8AvKOjuxQ/W4p0lW3zbytmhssFazzClozMG2sZaScWdZudNtSOiOgCv\nAbgx5NTeAHZq23YC6GM4dj2AyQAWAsgCWA3g3ID2PADgAQAYNWpUVd99T+wVKTLp8cA+dZ5wR51n\nT1TYKUhi2W/a5c/fXmojlKiwzLEfrvdeR/FiLNuSj/T/1QtzjceYfoj5FLzKmH1obWbyxdun8/3g\nbx8AAFaMuTjfjpCbOm9dPfrU1WLYgJ4JWxdAVf86qpP6pnb07VGTeA0Fpushn9+dIbeGTnx/dCFR\nfgkNAPpq2/oB2GU49kY4KXn3g5OK9zcA3iKiEj/9KosU47AOYU2KUOMOE6RT0dz4KSoc/09i2X+0\nPt8fm7R4Mxpakq0KZ8PWI04RFezP5gSaFQv/krunRKjD+furF+bilblOZ8LLypcTgPeDTZhHwP1b\nDjf+xXdNxll/GG89JgmmNs5ZswMvzV5X8rq6Amt3NOOY376GByYu6+imMNXIbir2US57EYAaIjpY\n2XYMgHmGY48F8DchxBohREYI8RicaX5VM26fBNOqayZUl3w6YoAegQos+yQZ9NZsb/ZeX/XwDPzw\n2fdjl2GDYO/spFOF4/C/eG4ODrsxP4qzK0YH5IlpK/HtJ98DoE79E0VH48sT8/Pso5/aYW58w7ZP\n/2UKvvdMaT/jrsIad4jojY82hhxZeoQQmLp0a6d0Fe8uFD+Bt/KEij0RnRv0D8DHolTiTtN7HsBv\niagXEZ0B4NMwR9m/C+DzRDSYiFJEdBWAWgBLIl9VFZJ34ysY9DiVyk+9ixqglxPC4MaP30ZdaE3J\ncYqBiKxWcIrIt/wt4ETxx8VUhRTZrLIQTtTueVsmh7vHL0FLuz+JT9iYvYkoy+EmYeXWxoL2qbBw\nJKOY3BpJeeGDtfjSg9Pw3HtrK143Y0f+jDqjGz/KmP3DIftXRazrGgCPANgEYCuA7wgh5hHRMADz\nARwuhFgF4H8B7AUnYU8vOCL/OSFE1Cl+VYnJjW96kDjj9Hk3fhTLPpMTBeKeZJyxlFPtTBDsLm8i\noF1Tw4G9u2FLQ1useky9buk+V6uPqn9PTV+JP7y6EEIIHDdsT68sb+pdjPuW1LIXQmBrYxsG9q4r\n2NeWyeHsP7yNjx8xGPdfNcp4fk4Ac9fWo765PTDnP5OnI5/lK7Y4XoVVAQGoTMfTGTvPoWIvhDig\nFBUJIbYBuMSwfRWcAD75vgXAd91/XQZ9rfYgUkSolW58g2U/pF93rK/3B9K1Z3MGN378NlZiepa9\nCkJ7Rhf7uvhib6hDCnI2l+8KRNHoTDaH1duc4Y3/zFmP219b5E1zzI/ZR29b0g7V0zNW4fp/zcWr\nPzwLI/f2x7XKzsfbC4NnpAgIfPLPkwEAdTUp3PbfRydqx25HB8bmcVhg9dL5pL40Y/ZMBFImN76B\nnt3S+QQ8Kf9YfI/aNE45cEDBOVmDZR93zD6XE6HpaYuG7GJPVDiUsGCDKYbTjqkKbyEeITzBjTLu\n9puX5uORKcsBOFMHgXxsw+NTV+IPry4oKkCvLZPDZQ9MxayV263njV+wCYDjrldRLQxrM5R9rZkc\nbh23IFqDd1OqwXCrZBOem7UGw0ePRX1TewVr7XzkDYUq+ILEhMW+QpgC9D573D6+Y+654njs17+n\nl7QnnfJPR9uzZ62xt5/NiQIRiWsVZHKi7F9ggn3MnoCyrSaVU9z4UefZA8BLc/LR6qZpkHePXxpL\nGHQ3/oqtjZi2bBtGPzfHep5MDfxvLXo+J/IPHtu91R0K6rUEuSSffXdVaCekq+LlUujgdlQKuWDR\n6u08dBCJzqf1LPaVwjRmP3LvPnjp2jO89584aggAYK++zrhsY2vW91A+eHAf49NHiOBsclHJ5HJl\nEfu/XH6c95rI/hsRANqzxbfBtsRuRokJiHK5WaU9QcGSce5bUjd+myv2/5mz3rddKAGHtpLvfHOx\n772av0je81Vbm9DYmp/t8IvnPsTn7n0nUXu7Ch05xX536Wh0KorM0dGRsNhXiKDlUE0Pk8P2dlIS\nLN/S6Au0u+tLxxnd8zlRaNnHJWPwDpQCvb02YRQiON99HJraslijWSiy2Gwub9pHEWm1E1UTkCa5\nElPvgjweAvkIf9v1PDPDH0ebVj4X2QE66w/jccVD0xO1r8vRgU/zjqi6M4pXR9IZ3fixMugxyQlK\nqmNyDX/p5GGYs7YeXzvDHxvZr0eAG18InxhccuzQ2GZBJlseN756eU40vu1oURLL/ov3T0Vjm38a\nWs6z7ONdp+oJiGrZZ7I5fPGBacZjgzpUAsAVD5nPAYC2rOqR8I/Te5Z9jFundsLaMwJwFnTEB6s7\n9aSXktMRU++8ui1Vt7RnkRMCPbuV9hHOyQKj0Qm1ni37ihEQuW2y1HvX1eDPXzoO++zRo2Cf6ceY\nE/Dmp4+59Cjc8YVjYzcvk8uVZQ646pmgkKw6uVxpxux1of/92PneHHQ1Gj/KD1a17INSF+vDBlsa\n2gLHuvV7rJY4ZclW7ViBByYuRWNrxnPjA8C8dflMhzmRrJOmTulsK3tkZp765nY8PnWF7549PHk5\nho8eW1XTmaqnJWZOvPkNHH7jqx3djN0Ob8XMDm5HEtiyrxDy0apHgMedImeyNIQQnij16JZOtCxu\nJivKkt3Nt6QvyGrZCwg0tpU+Re+Dk5ajZ7c0AOc6o65616euBruUMexyuvG3NrQWbBs3dz1uGbcA\n6+tbfB6Pz2gpg5OMvizZ1OC9zmg9kF0t7Vi3o0U/pSTc8MJcvDR7HQ4f0hejhvcH4HTGAKcjJhNK\nVQsdYulG+B2q30um8lRTxzQqbNlXCM/CFQHbI5dTuC0nhPfArnEjr+K6H7NlisbX+x1hY/ZNrcFZ\n4IpBWsaOZS+j183HHjSoFwDgxAP6+7aHufGj3L1sTuCVuesLst1tN0x52t7Y5rVdtezVoQAhin/w\ntGf851/50HR8/E8TiyoziC3u9MVW5Xrkb6CaHp/V8CzvyCEExk41fD/iwmJfIVJmrY+0qh0AHLNv\nPwDmzkEul4/yrnUto7gWSUt7tmQZ9H5x4aHe65Tmxrf9SHICZbHsAX80fj43fvD4udMezQsTKPb2\n8lRmLN+Gbz/5Hs7437fw5UdmWI+VwxE9u6V94uivu3iPjJ61cPaa+sBjd7a04+uPvYtNO4uz/E13\nspqCnvRljDuyDRWpq4rufTVT9LoaHQiLfYUIWiEtitbPuuF8PPutU33lqDjjts5r6QaN+oz62cdH\nAnDGUuPGxo0cbFqhWFvGVw3QoxDLHsJbw/775x0ceFwSZLXqmP0VD03HT/8xO/DYqD9o+aCM0leS\niXm2NLRh4qLNVjGR96JHtxq0ZcweDxGxXhtx4iSen7UGby7YhLvHJ1uqwiRg3hBXFT5A2bpmTHTJ\nhXCY0hAUjR8l092A3nXoXpsO3K9OvatJxftI+3Z3wjbqm9tjWVa3XnoU/vmdU437Thi2p/faf30h\n1yrgzfPev395VjSevabec4k3tWWNC+1ESVKjIl3yUc5ry/pF21ZFk3sv/jNnHXYGrPYnlIyASXHi\nGOKVUY5HXVVZ9lXQlEp2NJKspbE70xkXwmGxrzBRXcNBmA5Xx9vjBjj17VELANjR1B5LNEbs1Rt9\nutcWbH/zJ2fjZCWlrz5MYc/ylrfsbZ2bciObOGnxFt/22QHT0p53VydT4wKCaNPc8baphk1uJ2KZ\nZfVBgeKFqS2bi1xGqURBHXpIssZApZBtW7GlEfdNWFqROqvwNjAu+Zk8ne9T4mj8CpEKCEIqTTR+\nXkS9AL2ID2VP7JvbA0XqhP33RCab843lBsUa9NU6AIVu/OC2CDiWfV1NqkPHSuNamFtlIJ3rDred\nrou9Hgmv0twWHqwocsVbxOXKsWBCVmMaOqgqy157f9kD07BhZwu+dNIw9OtR2MktB5X8DXRG8epI\nOuPdYsu+QgStfR53wZqgMXvPjR8SoKdr9NB+PUAEbGloxWvzNxrPGX3Rofjnd07D984d4W2Tkel3\nfOEYa/n+qXf2h4pwA/R61dV06IM/adWeZa8VoF6zPqfdNl6udwxMCJQgQC+bw7i5G2KdU7Q3QZkB\nIDuwIkLowIV/moiHJi0rrvIENLhDKklWk+xMcIxCNDpj54jFvkJ4D7SCqXdxyykkqwTo1YaM2atj\n+k98/SSM3LsPBvSqw71vB7soUwTUplP4yQUjvW3Ssr/0+H21Y8k7R30PRInGF2hpz6F7TQpDDQmF\nKkXSH3KQG18tLo4bP8p3oxRT79qyOXz/mfcjHVustSlbakrkE6WDt2DDLtw89qPiGgFg8cZdmLZs\na+B+/Z7KzzRu5zwJnVBHdhvk96IzfkYs9hUiv+qdf3s6tmXvHH/8sD1w/mGDAfizskkRDipVan23\nmhTOPHgQAGB7k329eNOQQFAgIGkir1pCC9bvwrIt9vFnJ7FKCscP2xOnjyhczrcSJB07zrvxNbFX\nXutT6GyWfZShGHUmRlKSBPgVux5Du3ofAmaq6JTSmvqvP07EZQEpjYH8ZyY/gzi5FEoF29jVh9D+\ndiZY7CtE8NS7ZG78i48eip9f6Fja6kOwNiRAb48e3Qq29QgJhjOVGDRmT57Ik+89AMwMWy5VOOIn\nhwiO3ncP+/FlIukQQpAbXy1PF/tMkWsBCBQ/1p3k9Kemr8JBvxyXuE6zZW8/pxTrJsRFfnvz6w+U\nrw0frN6BSYs3l6383ZEPVu8oWf4QFbbsGQtBAXoxxd4bDhCe4KrfZZnSNajYp75xMgBgDyXI6O/f\nMk+hs7UxKJuct5m09xHICYFMNp8yNaiOclDf3I5RN7+BWSu3J+61twa48X1ir2XOs+Wlj3L5QhTm\n249LRzy3VI9Gfp69vSWtAbkGyoLWFC9xUhmrvOTuKbjqYXuiJSY67yzZgkvunoKHJy8vednVFEwa\nFRb7CpEP0NO2F/EJ1KT8LkZ1m4kzRgzEQYN64+ZLjsQ/vp0X+MOGmJPjSExiH2TZ28bswxDCWXgm\n7Q4RRM0uWAreW7UdWxpaceebixNbb9mcQFNbBi9/6A92U4traS+1ZV98JH2c6y36E3GravOly3X+\nhhlg+r2rBLpHLkoQYSU56tevltTb0JWm26/Z0QwAWLhxV8nK7MwZ9HjqXYXIi57/WyLF+bAhfSOW\n45YioFj2itinNdPa5V/XnIbj3GQ3V56yv29f2NiwaXfQfP6U5saPJfZwcvzXdoBlr9aUxOuXcqcV\nXv3ou5ixfJtvn/r5NGuWvW3MPsoDRZ12mZSOmN+eJEBPWvaV+FroGdK8h3wF/CBx6tjVmoEQXUOk\nN+9qRa+6dMmX7S2HMHMGPSaQ/Niff3vPbjW478rj8dT/nBytHMXSkEFyaplB1nAxVrLpQRI8Zu/8\nzY/ZR68nJ9wAPRlkGHByOToB6seSxFKScQ+60AP+z0d3RZvEfmi/7gCCV8hTccQ+TkuNpRRbQIya\nnLraDVPv1HHx7z71HqZr0fLSsu9WU97H1n/mrMPXHpvpts1PJS26qL+dzic7Zk78/Ru49J53OroZ\nkeiMlj2LfYWQwesmIbnwyCHo36swcM5YjpKcRwquOkZcmzKP2RczZcg8Zm//6uiiHwXhBeg5Zevj\n217dmlfhYyMHRa4jCknEs0c3ezpjycad/qVs9aCzXt3SyAknhmDsnPWh9ZbGjR/j4CK+R1OWbMG7\nK5wgzcWbduGqh6ejSVn4SLajoTWDsR+ux9f/OtN3vuwodQtYarhUvDR7XeC+anzGl9KN39EitmBD\n6Vzu5aSj71MSWOwrRN56KbogAM6XzThmH+BeL8ayjzNmLw+Ve+Ol6vcH6Okub0mt9rB/5OoTcZK2\nHG1c/G78+B9SXU2w2NvGefUMen171CIrhHGBHhO5Esyzr5Qb/4qHpnuv/zNnPSYt3oIJCzf7vFUf\nrqnHmx9tAlBoVect+/KmUlZvp+5dKldglhoxHreKUn5+nTHwLIj8wGkZovGrsttnh8W+QgQsZ58Y\nAYG0K4rqg0JaxQcM6OU7Pkzsb/rU4YH7jGP2AeVJz4LM+R/Hss95AXrOOUEBWXrdpVzEQwiR6EOy\nWfa2B4Nu2ffrUQshBDbUR1tCVpRgnn21PLhyAvjUXybjh89+AKDweyct+7qaFHY0tWH46LF4JGak\n9ZrtTbj4rkmRjycAv/jnHO99sVo46uY38PT0VQXbGwxLO0f9XlfL57c7UY3rOITBYl8hjnHnjH/z\nzAOLKueofZx17UcO7uMl5PFPvXO27dmrG1aMuRh79akDEC665x46OHCfSddNnYcVYy72RN6z7GO5\n8WWAnvO1DLLs04q7IJXvvheF+mBNYt10rw3+KdkeDPqYfd/utbEeJEL4h3H+9Mai6CcrZUQl6NMU\nQuDBicuwtaHVuC9KmWGLRLUqY/br3c7Qs++uDm+0wiOTV2Deup2xznl2Zr6OuMI6afFmPPuuI+5C\nCGxpaMUv//VhwXEtEdZBCKKUxngXMuzLCqfLZQKR4nvGwQOLKueTRw/FWz85G+cdNtgTUtvUO6kD\nYZa9bbU8cwY9e3mmDHphCDhT0Wo8yz5gzF4pVF5XsdbNDS/kH8CJxuxDliAOol1LstOrLo2cELGu\nRy3/T28sjnyepBSPrfdW7cDvx32EE25+o6ADE/W5GLZuhLTsa9PklRnXqZONkJRAbUVB+TFv1lUP\nz8AvnnO+W7bvVTHrGwSdurOlHU9NX+nd15b2bODiSpWcbZAUIYSxMxlEOZftrd67FAyLfSfkwEG9\nAeQF+mMj9/L2FX7Bna9lWFpem3ib9oR1HtRMelEFX7rxvTH7gAeTWrcXsBjh13d4wPTGZ2aswupt\nzd77JA+8XnXB04WsYq8JY8+6mlgZv3JCFG2NhVkpf3x9kXVRnmxOYOrS/HLAL37gD3ALa578rugT\nE/SvTZs75JFOpRKLUtQZDoH7EtXqEPQ92LizBWu2Nxv3RSHoXvz8H3Nw/b/mYu5ax5Nx4u/fwGE3\nvmIvq4pV7KFJy3HCzW9g1damjm4KW/ZMZalNpzDp5x/Dny47NvAYqRthgXI1lghnkys+rNesJtWZ\n8LOP2St3EUIgo0TjX3Xq/sbjVLGPMw2vd/ca7N23e8H2657PW/WTFm9JlLzFJvZLNjUE7mtXhP2K\nk4dhcJ/ubtBdtHpLMc8+7PQ731zsuaJ1ho8ei5vHzsftr+WHDzIFln0yN35BcJy3GA0Uyz6e9WZJ\naxDSOoef/XMOHpzorLr3/qrtxvH3IJ6attK4/eRb3sTn75vqvY/7aQb1DVduc0RR3qJdLf64gIUb\nduHwG1/B6m158axmCZuwyEklvGJr8PoalaITaj2LfWdnv/490d3iQpYP2mLc+Emm7anz7Pfr39Nc\np6FNGWWe/Tkj98KKMRdb25ry3PhR2lS+hCy9LYlALn9weuA+1Y3/jTMPRDoVz1oXCH7Yb2+0L3CU\nLyO8spwnroX7XpptnyIYWroSja+if1amleeifJyZbA7XPf8hVm9riuTGt7V44qLN+P04Z9W9z97z\njnH8PYibXpof+dg4BHWmGt1leYOCR+8evwRNbVlMXZrPZ1DNFqvMrxBl6WcfZbik6r1LwbDYdxEG\nuYF4Ot6YfYhg25bGTTL0JR/UQeeO2Ks3Hv3qib5tem78IExj9lGsWwIhU6YwWptlb0NtT02akCKK\nZannhAh0+1//QjQhiqJ/ts+kW8jnFcdLoaJ3MuV9IZ9lH17uzJXb8cyMVfjJ32dHsuz9U+/Cj+9o\ngm6vzGEQdAnz1zvu/SF75L1d1SxiMr+CbT0JlXJ+dFXcJwqE0+V2EV7+wZnYuLNwupbsqeuRzTr2\nAD173d848wBs3uUPnCHLWPphQ/ri7986BXPW1Pu2N7Vl0dSWsQ4pAH4RSMcYs0+lguMAiqV3XbK5\n34+9s8J7XZNKgYgcN37E84Pc+JlsDh+tj5agJEpdaSKM+3C9sWNRq2W0048QEFaLMdiN7z9O3S+9\nEVHE2HP/p8z3atHGXehRmzZ6oAhOh7KY5XzjEldIgvI4NLY63/WgpkvL35lt6hzU2p7DxEWbcdYh\npU1UVQoSW/ZloDPmI2Cx7yIM7F2Hgb0LrXv5nSwmQE8V188cO7QgAOv6iwvn6OczBhaWt88e3dGn\ne21Bz/v9VTtC2wL4OyajLzrUeqwKgdAUEOFfLD0TWvYq6RS549HRHyTn3zEBx+xXuBTwtU+/j+Vb\noo1tRqnvgUnLsGxzI842iICe5EgvTi5wZKwbwE53LFk/RLfspUFHpETjR7DfZFBekGhf8MeJAGAc\nMgKc72OxYq/HMUQherpcc9vk1NWwz1cVrt/+Zz4+Wr8T//neGTjSneZbCaJ8B3WxX7m1ET989gM8\ndvVJ6Nez1naqj1krt2FIvx4YukePZI1F57Ts2Y3fxZHfyTDL3hbopO6687LjAh+KvnO0fOe+NoUE\nV4Wl4pX7v3POQfj8qP0ABMcF+NpEhcvPlopelqQ6UalJOW78bM5uCevMXr2jYNsr8zYYjjQTpapl\nm4M7DlHS1wbdd9WzEWbZS9Em5L/X6jEbd7YYrT51rF+PxjfdZ33qXZz0vNsa23Dji3MLPEh68qRS\nEvb5Be32ptspByzd7AST1je3F9+wGET5XUqxb3U7TnePX4L3V+3Ay3PD00qrfO7eqTjrtvHxG6lQ\nzVMUg2Cx7+LkFKsmKckC9Jy/pp+E6UGtEjZmL69FPerWS4/CV08fHtKm8o3i1aRTqCtygZaaNCGV\noopn54rz4DIlOtIXpjGtGBe0ut9H63cqx+kBeuZo/A9W7/CCyuQRLe1ZnHzLm/j1v+cW1CGFpCZF\nyGYL26ajtyPs+6jy+NQVeHzqSjz6znLf9lK6nvX2hbmUk+yvtJs6ynfeG7PP+BdEag25t6aii43d\nYcueqTry7s7kJNHIkw8YAMBJElPYpsKoapUwN74cklBP711Xg48fsbf1vHIGW6XInjI3CjWplG8J\n40oRpy5TzEOY5SvcNQ8Cdnroz1/bmP3/vrLAt0/GjLw2b2NBFWpiKd2yDxM1AhUMUwDwLeCjIl3D\n87Usfa3Z0g0f6VZw2McXFoBp0r2k378rHpqGZ2ZEn46Yb0N4hXWaG79bOu17X0lY7JmqQ41gTkqU\ncVGd311yJF770VnYq0/hvHZJkKaHtTXISxH2AyynZZ8iQv+e0VYuDCLtuvEB/8NvcF/zTIsg4rpg\n4xg5jQaRaw0Zj7aN2auoc80B05h9YRkrtjZhS0OrF5yqT0OdvXqH51UgooIAQ6PQae9NYj/6ucKZ\nDhMWbcbWBme6o762QRw3fpinZfYa/7BNuBs/vmWfVMumLNnqy1sRlSjiqY/Ze+8TxEMUC7vxmarj\nW2c5ufijjDu+dO0ZuOqU/Qu2JxkB6FaTwiGD+xj3hbnxwwhyq0ZN3lIOUkQ4bKg5Q5+KKZhOUpPK\nZxuUFmi3dAr7a4sahXHBHyfEOj7OXHGTZR+W112gcHU/dV8Q+udlEqX65nacPuYtbHIte3WNgjlr\nduAzd0/BHa87CX/SVDj1MtSyJ/P3TU+U1J7N4SuPzPA8DnqpSazPoE725+71d4pM3/uRN7ys7LfX\no54fNDOi3ARlNrzzjcX40gPTAOSfYbLzFubG9xYfK8O18EI4TNXx4wtGYsWYi0OnswHAUfv2w56G\nqNZSW8RnjHDWBzAF6F192nB85bTh1vPzY/b+88N+f8Xkyg4aj5dFplLAIXuZOzeSe644Hi9+9/TA\n/S1PovEAACAASURBVKkUeW2UFqiAiN3Z2rgzev7wuMjpWir6OL4QwCtzNyjvLW58CwUBegF62ZrJ\neW58danhdTsc61oKczpdmMPAFkAq2xAlS6M+80EXmHK6mvUryOWETwCDhFtap0bhqrCYBbXxj28s\nwtRlTnxGrWbJ6259G0II43c3KdWcfCgIFnvGh8ndWiqx71aTwozrz8PXzzgAgNnS/syxQ62ufyD4\n4RtmjRSTPS8oaY4sMkVkjE+IW7/syKiWTjmHH+JiCtAzbfvFc8qysAgO0LM9NIOS6piQD3zVpauX\nnabCKXRRLDRjEJ/2ftnm4JTIQPD1myuMfijgBCeuUDob+m84TJfMbvzSidmjU5Zjfb09939QrgAV\n+bvXxT2K2P9j5hoc8etXQz+nqHQ+qWexZzTkw9BnyZZAaz767YWYfeMF2KtPd98iOTqm8VGdIPEL\ne6gVo5n/c+YB1rYQEXpaUubKY8KQHQLp9RaiusTeNPasu/YF/EIrRPDUKttHFhSNb2yXe8PUtuiu\n4XQqmhtf3UIoDOozobuS9TPCIsZNBH3s+vaf/WMOzrn9bS9oMIr3wr+/MCNhpKzCEVi3oxm/eWk+\nvv7YTOtxYfd43rp6vDTbye8hO3SyAyVXQ9RR79Nr853AzcWWdSpi0QnVnsWe8SEfhj/7+EhvWyny\nyffoli6IVjeJmD6Ny0TQmH24ZZ/sQmbdcD6uOWeEcZ8skgD0DInGj1K/HqAnUFwnpRI0t2fRR/F8\nrN/R7CXKAQCIZPPMg+bZm5DDBGqUvN7BICrsMJg6ELpHIMoc8IIIee2U599bE1qGd27Ifj3+ZsaK\nbQDyGfMKOzQh9ZUwQE9l8uIt+Lcr0GFBo2G/3YvvmozZbsZNOX1Sin0U70ypf0OdMYMeiz3jQ2b6\nSqcInzx6CLrXpnzjoKXE9AOMYtnLpDr6+eWy7G05CtS4AduCREC0TlN+udfSuPGD1kwoJdmcQFrp\ngN3z9lLffgERGKBnI2jVOxNS4KTgAYXfhxrj1LuwRtjrdeoRgdPhpIdse1O0RYlkeUEs2bQr0Esg\nLdzCjkeCaPyQc1Zva8J7q7Zbj7ny4ekY8/IC6zG2NgQhP+u8+95+7gsfrPM6G2o1f5+5OnKdOp1Q\n6ysn9kTUn4j+RUSNRLSSiC63HHsgEf2HiHYR0RYiuq1S7dzdkdHihw3pi79cfjxm//qCSNZ2Esxi\nHy5sQWP2YeOMSQP0bNkH1SLDkupEs+ydv55lL+IH6KlUyilgC2JrzeRwi7tSXBz0IpssUf+yk9qW\nzaGl3Sx66ZRp6l34UzusQzBlydbCcrQpr3EyN9oOPf+OiYH75HXr1xhWs9oPs2W+VDnztvG49J53\nQkouZN2OZmxpKAwgjSOesm3SWxSlH7lwQ+E6EY9OWRG9Uo1OqPUVtezvBtAGYDCAKwDcS0RH6AcR\nUTcArwN4C8DeAPYF8GQF27lb89nj9sGkn38MpxzoJMUpl1UPmKcWRZkiGCS+YT/6pBayTOLzwndP\nx22fO9q3Ty0ytFMUoXopmtJ6ESjOsq/UeL/N+/HUtJWYtmybcZ/tIa82feXWRjw0eXngsarremeL\nY8XpVrwpXW54Uh2nnAG9gnMoXPnwdMzTkuh47XIFKY7YJ03pLDtDBW78oHgJd3NO5LvJ+c5JcD3F\nzCw4bcxbGHXzGwXb41yzZ9m7jQzq5IflBykmop7d+AEQUS8AnwPwKyFEgxBiMoAXAVxlOPxqAOuE\nEHcIIRqFEC1CiDmG45gyQESR8syXApkCf5hSXzQ3fmG6XCDC1LsYbVORQnbsfnvg86P21crMZ/PT\nxX5IP/+sgkiWvYzGl2Jf5DOlUuP9tvUM1u4oXI0xCur9WrwxerT7zmY3UM1g2etTAE/6/Zve6w/c\nNQb8U+8cb4DamTH1a7Y1+t30MkhRClMcMYsrQrI9MjixMEAvrL7CbbZhl6CAuGKII55Zt23tbqcj\nyqn5r1JpRLoTan3FLPtDAGSEEIuUbbMBFFj2AE4BsIKIXnZd+G8T0VEVaSVTUYzR+GUI0JO5A5K6\nw9V2EhFe/O7pnpD7LHutozL1uvO0csLrkl6EUs0Jrpgb3zL88pwlOM029EK++26vPxvRsreJyhfu\nn2psU1YI3zCFaThIL1efgaDHJ/5j5mr8z1/fNbZDnhZVUGRHS67oWDD1LkIGPf2KbHkRkuRMCCOO\neGYSBOilvFiYZHUW0vnUvlJi3xuA7ufaCcCUhWRfAJcBuAvAUABjAbzouvd9ENE3iWgmEc3cvHlz\niZvMlBvT8zvKmH3Qcr1BFtEAd+nfpGP2uov6mP32QL8esgMRJ0AvwrWl5Jhp3FaaKSaRkM5elmC/\npAst2R64PZRseGGXoUb772oxW/aPvbMCiyweAlPcAaEwAJFgWIxGM4SdoESh7Pcf/7N/zsEbH20y\ntkN2UqLOdZdOFc+yjz3P3tAGZWN9c7tvZcV25WLvm7DUuxdxPRKvzN2A1dua3DYUnvv6/MJ1DtS2\nyc88Tk4AtZ5iXPGcQS+YBgB6LtF+AAqjJoBmAJOFEC8LIdoA3A5gAIDD9AOFEA8IIUYJIUYNGlS4\nzjZT3UghUh/ktSHL2wJAWh5TkHTFfHx/d7w1SDB+d8mR1vpMOua1Xdl28F698a2znfTEprS4UXS3\nmNUJTUS4nZHRhyVUonxucYkTb6CuFy/du3EfyEFDSLmcCF122bTSnyr2cWYj5MUz2vGy8yvFvjBx\nULhlrx+hCvpXHpmBz9w9xWuXWv6Ylxd4GQrjaue3n5yFi++aVFDmtGVb8a0nZuJXLxSuYAjk76s3\nKyGgXvWq5DfJdC9WbAlevjkIduMHswhADREdrGw7BsA8w7Fz0Bl9JExsTM9yW+S7JDg3vvl4GVwV\nJB5D+toz9pmsY/L25belUoTrLjoMC2++EM99+9SCc2zi9f6v/gtA6cU+ySJGQQy23KdyWPbqgzns\nOrI+YY0/Tg7kYy70dLk54b8+08doCmhrV1PWxohpiztLUf5mAt34gWIo9xceoN47Gcsgt+lufC8R\nTwIFlPkY1CZ//bF38eq8jYFDWflo/JzvvY7+OerHCgCvztuAc25/G6/N24A48EI4AQghGgE8D+C3\nRNSLiM4A8GkATxgOfxLAKUR0PhGlAfwQwBYA8efuMFUNaX+jEiQsQT/6PdzV6IKikpMIlZdMx/Dk\nr6tJG9cisIm9TDhUcsu+hMXJ4RATcdZ8V7E9NH2iF+bGV13m7vcgrvjImAv9tIIxe0NjdMEUwp+6\n15YQ6MBB/oWOvPZH7KzI1rS2BwXohVn2hWWZxuWzmshKpKctzt0uvF9qfIPwN0ZDn+EQVK//9hWO\n2eeE8GZRzF9vnk0RBLvx7VwDoAeATQCeBvAdIcQ8IhpGRA1ENAwAhBALAVwJ4D4A2wF8BsCnXZc+\n04VIOp4cNxq/hzuWHvTQ05tx66VHeecEkRd762E+bMIrOwJRFl2JQynH7G15BJK22/bQ9Fv2djLZ\nnHd/pWUbW+wN10dAQTS+CdOUPnWKWmC6YCEKOhfr3eVxo7ZeFh1keSfJjW8advDK165Fzw1hoiDG\nQTtUfS/rCersyLbJv4GeC6PHIn9dyzY34q43F1vLCEIvuxwzFEqNPZl3CRFCbANwiWH7KjgBfOq2\n5+F4ApguTNJ5rkECdvQ+/Yzb62rtlodeXpooenBUDDG1HSofmKWeF1/K0mx5BMLGtIOwudqzQmDa\nsq34cE09RuzVO/A4wBGgupo0mtuz3gM97jLnhw1x4oV9Y73u3HxfZ8ZwU00CqwpA0HVmc6LgdzB5\nyRavjCjkxc/cyQmPxjeVqd4D/+wC3bKPO3sAKLwfvpkLchgmoMA2zbIPXtUvj/z4gtI2F2Ooj52z\nHt99+j3865rTcNywPYsoqbxwulymwymV9Tl8YC+cefDAgu3da6RlH1C/9j5K3IA3vz5G+2zXKS3H\nUrvxzzqkdIGrtraVut2A83ld9sA0/H7cR8bVGFUy2ZzXqZPCG9eyFwbRkpa3b8zecG6bJoANrRls\nb8rngw8UeyECv5c2kR7QqxsOG9LXV7b+V6Ib6S9+sNZfj8h3OLx0zYooyusN8hxEiY3I5ASGjx4b\neI4/St7cbq+srOzM2d34aieKPLEvzQo/6nfkgUnLAOSXVK5WWOyZDiNxb1pLRRpGd1cEckLg7Z+e\ng0e/eqJvv15OOhU9z35+IZzwxtisdvmQ1UXzP987I7RcG18+df+izlexuerDxuwf/soo/PYzTlqN\nqOmXVTduJuQh3ZrJKZ06s+iFYRICWYLquTB9jPq5a3c04/P3OfP2u9WkAjse2ZwIFHXbd7B39xr8\n4Dwn3lkPSAxb2e8Hf/sAmWzOl0EvX6drvfsse+eCj/3t61i1takgcC5KjIS+MqJutcdJ7JPJCuRy\nynoEgW78wm2BYh+zY5jz9wgBOM8NE8NHj8VXH50Rq/xywGLPdDgE4M2fnI2/f6swgt1+XjS198aa\nhWP9H7+f39Wmi3CKKHI2PvkgjOL2j7OePQBcfPQQHBkwNBGVlvYSrVUKu/UeNmZ/3mGD0ctdAlim\nYg7jw7X13uswy/6dpVs9y/4Xz32I52atSSD2hcfLIsI8F7YUsj1q01Y3fpAFa2t9isjrdEjdCXJr\nm6pWPRG+8XKvo5Tfr175mws24vKHphdcg9oOEwUrGOb8HYwnp60sOCfo41u4cRcO/OU4LHenzAV1\nMnKFehx59cVVW5tw8V2TMHdtPX789w8Kpuf5qvRWqgwub/zCjs8Dw2LPVAUHDeqNkw7oH+nYuB4B\nGRkvHwqkfev1x3g6RZGj2ON4r+Mk1TG1KwmlzOFtS2WcjjFmnyRwP4r7tbuyjsNP/jE7dkxIezaH\nTTtbfKu5Se9CjS+pTuEFyKx9Jnp2Sweuo27tkFjaT1T4fcrkBFozWcOiQ4XlvKpMNfO50N3bHJQl\nz/QdDhs7N7VA9dq8t2o7nk2wAp2cthc8tbBwaCCoU6YXccu4jzBv3U48++5qPP/eWnxdy3ZoqrLa\n8+Wz2DMdRiV+Gzd96nAlK50r9vpBuhufCM995zR7wV5SHf/fCKdY171XswOWIljviKF98ctPHFp0\nOUBxln2UcmyX++O/zw4tt67W/zizTXcz0Z7N4RN3TfZZfws2OFOy/OlyC89dva05sFzb55jNCWuA\n2aKNu/C9Z94vGMYgFH6Pc0LghffX4v1VO3zb5TrwKj96Nn8/1erbc/7xcEDLJWG4lL+9uwrDR4/F\nxp2WMWvtEmVOACCf8TApQV41fz/KHFzo7dWKaGxz2rSnm6Nj6Wa/ZW+aKVLt0/FY7JmOJ0Qngp6V\nUbRwr77dPQGVP0Y9UE5/GBMRjhhqd58nkWFZT9/utYHHhC24EhciwjfPOqj4glDcmL1KUKDiPZcf\nH7tNKt21FRrfXW5fb12nPSsKll+VD/lyBCACMhrfvC8nBH7wtw/w0ux1WKAt0aq68SWZrDDe23vf\nXmptQ87nUnfbFZTQyFD+P2Y6ax/oguirQ7vI08e85b1OusqfJLBPZwj6ixugFxgrotSZH06pbrVn\nsWc6kPL/OITIR9cH5eogAL0UazvKg13+wE850Bl6GD4w+kqBfboHz3j1Z2qLLjCmJn//3BGRz4/C\nIEtu/FiWfcB1XXTUEC/oLAm6ZT9jhXlZ3SBsQqB+Lk1t2QLxtbbLEpDoROMHW6YfucleiIA/vp5f\nRyxFVNBJ3dHUhnfcaXtRkIF2Jq31zXEPsexl+21iZ9PzqOPoccsWvmOkZR+vrqBYEZM3gd34DBNA\nTzdga+Rg03pIefTnS5TflCreNZ7YOyeaLPlXfniW9z7CKrtem648ZX9Mu+68UE+AU7/zt2+PaJZ9\nvIQ9/oMvOnJv/PiCkZHPt3VAJPZ0udEfJbZDhxWxvHJdjT0RUhg2sU+aRwCwez2caHwz/3ovP0Vu\n+ZZG3OkmgAHc74ZW7PPvr8ULH6yL3K7mgIx7gDbPXtluGpLwpspZfpc2IYyzboCZgM6SwWMR6MbX\nypAdbXWMv745H5dhutYPtOGTaoPFnukwhu7RA0//z8m4/fPHJDrfpoVqlHxKH7Mn/Vhgv/49cbg7\nbznKWHm+fGBvywIxKvKB0tcV1p/81yEFx/jd+HEs++LczFG8GXtbxD7KaoUSm8eimPV0dMs+Lral\nW6NOFzRh6wiZkupItjXmk4Ze+/T7vn1E8VY9sFndpn2+MemIlr3NHW/roOvT8uISHKCXR8+nH7UM\ntSNy/O9etx7/16krsTCGx6fSsNgzHcppIwaiV13pEzmqzyR9zF5HPsCkBRbJjR+xHX0NFvNPLhiJ\nffbogS+fOrxgnyka3+YJkOh6EkX7/33t6d7rKG74gUUucSvbFOTGB4rrtNRFcclY0BPjqOzVN/ja\nw7DdW9uYvS3AMGWIxrdhE+KcKLSNg443ddRM8/UL6wje11Sk2Jdj6p28yvZM/njfdEHDinoAsG6H\nP1Bz2rKtkeqrBCz2TNWjP2DirDilZj8Lsuzlz9XLYuce8NevnYT/C/A66HOcgxj/03O8pD7y2CP3\n6Ycpo89Fv56FIl5jsOyvPXcEfnGhPaI+6oP/tIPyc9zVc6KItc16j9JZ6Od2WmxL5RYz7Jl0MR6J\nzY2/zx49Epdru7e2aHybQBPFG+ax5SlYt6MZy7TgOvVWqD4E2/fM9tnZPlYZ+Z6U8Qs34++GqXuq\nx0J+trYOnYn2gCGGoGvVOy6XPTDNe33ji3PRVOS1FgOLPVP1BD1ebA+77547Avvs0QNnjBhY8LDV\nH1iDXatNirx0+599yCB87oR9A9rkjwMIYkDvOhw40J7XPaht0lo/aFBvfOccJ6J++ADzmHZBHELA\nXXv6G6cYz4kyJm2zyKOM2Z976F740xePxY8vKBy+kBQTmW1aadDG0fv64ywaLFPAhvZLLvZWy144\nXdfLTx6G33/2yMhlpohiLV9sE7lxc9cXtktNqhPixpfEWQhHpam1+EVkfv7POYY686+l2LdHnGcv\nCRra8afizd+U5vbga3l86ko8OmVF4P5yw2LPVD1SrMdcehRe+9FZIUc7HDakL6aMPhd79urmBdzJ\n36f68J163bnYd8+evnriRNVGOTKOBebvmPhPHP/Tc/Diteb0uUm832pdUSx7m1UXZcyeiHDJcftY\nA+nizo1XibvyHgH40flOx6NPXQ0aLe5kOd86LrVpsn6fnAx6At3SKQyyLCGsQxQ98RMQLHKA2UpV\nNc73jbSKfbJ9ca3tqJiS6sQds5+7tjBHgVN2HvWWNIdY7h05PY/Fnql65EN8/wG9cMjgPrFdvdLq\nzLvx8z/PIYrFJt3AkazLGA/aMJf/QOUhb4vGP2BgL88VDjiuZZmgJ8k8cNUQjiKUtgWCSjUP3baG\ne1iQXNzxfoH8PR4W4DEBgO+fd3DinAfd0inYhollNL4pI54N8v6LRtwpZ+rnoP5ebG1MOvXMFhhZ\nDObc+PHqWrbFnDsgihvfJOylXHI6Liz2TNUj3bP6AzfqDycfoGf/oUtXdlgediD/nI3yfPNc/gY/\nwISfnYM3fqxO+4v+MBjQuxs+c+xQAIaHcIRi/EMGxT2Ekq5nr2O79aWqQyJE/jbZZhqkiRI/pOtq\n09YOjAzQI1CsmQgpir42BGDP3W8iE5AbXybOMcVdJB2CKX7qnRlTc4LaWDj1Lqxs82wF1Ts05uUF\n4Y2sICz2TNUjH/JJH7jyIRr2LJJCm43Q+/es9ZjBgjr7D+iFPXrmXcTxLWQ3ziC+1mtj9kWKveIm\nGPf9MxOXY3Pjh7Ux7tdDfWDb8/4nX6egWzplFUE59c4JuIteSyqmG9/mKm81dASCDr/LnetvCoZM\nunxslM51Eky/zWKz9UmCZhC0uGP2Exdtxv0TlxXs70DDnsWeqX7Sntg77+P+XNOGE5/5xin43SX+\ngChZT6yHT4RDr7voUAzuW4eDB4cH6tmC4Ezkl9rVAvRiLrpj6mQcvFfv0FkApvMPH9o30jkmbGOa\nYQF48Waeu5a9e4qtk5VKFaamjYpteVsg78ZPkT3Tnk5Y5+CsQwb53qtC/KWT9gst39fmiNce13sg\nidK5TkKcZXP133HYJcvMhjpS7L/8iHlJ27jf0VLCYs9UPdLqSvoz0ZPqAMCpBw3AVafs7zuuxnBc\nYJlKUp0wThsxENN/eb6XMdBGHMtedUPH7SQAfte9yWruXpvGV08fHqks/fzPHrdP7PYAdsur1Ja9\nM1bunGS772lDatqo1KQp1LLPCSenfZzpfWTIja9Sq12PKvbda8MzDQZl0JOYNDOphV42y97wOw52\n49vf6yzeaF7FMOzZUaYlFiLBYs9UPdJlmNS6kqeF/RC/dsYBAIBR++9pPU5tS6mDa6MscTv6ory1\nLdsR5sb/2cdH4tGrT/TXpdxQkxv7ezFy6+tWd9ROgk4xY/Zxvx6qGNjm6BcTfFiTIuvQRFYIz8Mw\nNIbYO0l1LPWmixP7nJLZz5xIp/Ca1Ij/OFZ+NpdDbZoKpkLGRY+NMI7ZR/zBhvU/1PupWuth57Eb\nn2EspIscs5fnhf0QTxzeHyvGXIy9LMFaOnHG7KOgCktQyTIxjoDwLM6we/Pdj43Axw7dy7dNFYvb\nP38MrjxlmPd+xZiLccERe0e2aHUhTmoJ24LZ4s6jj4OtI2FaYU6ll23J4lSUMXtHMGxxAzpkXOQ2\nj543oU3JBBdluGDyki044LpxWF9vXrrXdEnvrsyvMmhd7lajPSewX/+e2M+yLkKUDpcel2B040cc\nMrB9D4FgwyFsah278RnGgheg576Pa03L50Qp57je+MkjcPIB/TFq//4lKxOI9jBQj5GvkuSUV934\n++7ZAzdfchTeGX0upow+N19+xGeTLXFRHN23WV5hD/zYbvyIY/bplD2BjU2kalKE/Qf0CtzvjNkL\n7zs684bz7Y12SaXs11usZS9Zt6PFWI9J7CYu2uy93hBD7LNZgZqU/ZsfxbmiBxrGCdAr9AqEib3y\nRmlb2MQCtuwZxoKcJ68ubuO8j3a+umhNqRi5dx88+61T0cNi1SWhR7c0BvZ2ovPjPBekuF54xN5I\npyiSG11140uxG7pHD9/YcWLLXnmyxLHybQ/ZsFLien4ExP+3d+ZhclVl/v++VdXpTi/ppNNL1s7W\nCZ19TwhJgIQA2QBNQFlkEwioGVR0YBhwQEZZlJlRYWbQYRFBZ4Rxm1EfxscZ+Cmj+LCMqAjysIMI\nhiWQhKST7j6/P+49VbfuPecuVdVd1dXfz/PU0913PfVW9X3Pu5z3zSrxYhL0oiYKN526OPv3GF+J\nZCdmn/sutzbW4pvnr4wcu6nFbdiYvMo+SSJgQ23aHLOP+Gf60zvxlX1vv0ImlQp9P3E+W/9qgCRL\n77yTzDf39uAXz4bXtM9bemfYHraUs1xQ2ZOKx1adLa5LTD/3Kr3ftOaio2bEPlY/BI+b04FMSvDx\nDTPx7LWbsbgzOu8g3/q2yTgefhd7fj31mBdBtPtU8/hVxwW2JY/Z55RsWLngqAQ9/0Rnyth6LJo8\nGoDz3W2ur8m27vW76nfc9Sj6+lXe9Vd3tWYnfGGEvV//mLwx9BXT4nuj+vpV7Ji9l6hKcvn36Ecm\nHTGhiqHsA3kCEa1788eQ237Zd4Kld/1Ymxe512msy6A2k8KSztF5+8tZVKf07cYIKTH6waXrdZ97\nxDT86rm3cMoyc916P/pBOkC1O0pOXEvYq6w6RtXhmWs3J7tPjKl+3GdTJiX49o7Ds7XBvecliVNq\nl3dXeyOe+bM54xlAXiVBz40SoTynhFvn4XLwn1ubSWH7kon49cu7s5+lPsZWBTB4+fA3E5VH4J98\nPbvLkeUN2+dnWznHob/fPJKZHY1487m3rOclSdDr7VduqMROnAmj/56JLHvP9r090RMVW1EdPZno\n7evH8XPHYUQmhcc8fe6ZjU9ICNrq0qUuxzXX4fsfW51XZjYOQ8Ouj34geFcCFJPwE68tbbzr19Wk\nsHL6WBx9mJME6D0riTGzdcF4fOcjq7BtSfTSvckt+dnrAjFPAix4rdOU2MfpNJ2x4/cK1NWks/kQ\nOnauRT3CkoQXrJMQckNEt7j1L7278SdPZ8eWxLrscwv++Fk7sw0/+Njq4A4XU6EeG71uzD7s/cwa\n1xR5nUCCXoKY/XceewXPu6Vx40y2ozoV9vYrx1vh219GXU9lTyqfbGW7AtfjZtfEDxE3fpKHcTFL\nAIvpHe9npK+GQH6ntPD7fOmDi/DzS9e55wmWTmmxnnPz6YuxpqsVAPDTS47CjiOn5+3X5YPj4PWM\nKAXUWFwd6Rgx+9vPWZYt0rR1wfis23lEttSz87ct4z5pteOo74ht5UKSjH/A+Z9rqA06gFMimN5m\nTzxM0tymz7Xsw970aSs67Tv1PT0TDKUU/vH+ZwPH2BIHDxzqx7obH8B/P/l6rEmw7VH0yu79uPy7\nv8GBQ33GFR504xMSgraOCi2+oZ/hQ0TXJ3L16UMLWQJYSmVfH0hU9OYDhJ+7asZYdPgSmrynLO4c\njf9zXaFbF0zA1gWOQq/NpPPuKwJcdcJcnLVqKs649SG8/m5P6H0dN74vSdFQBdVR9iEx+7RgfXcH\nAODEBRMwamQG9z76SnaMQE7WNjd+oEVxDMs+STa+xq/sG2szoW7rfqWMCX0pCfcMJXPj96OhJmP1\nUrU2joj1XdUTjP0H+2LF3U3c/4c/x7qX13DwPlcef3k3Hn/Z+a5m0in0+iY9zMYnJAQ9Q/b/48Ql\nG7MfKto+AcVY9qXqVAcElX0Sy95kbXpP0ftNb9GbuKUVUFd7Y6yErrwHNuwKMsqN75Vjc30NRCR7\n/9oavZJEv5d4iZBR4RlJmDSoidOK2EtfvzJ61NIRbvckyv6xl3YjnRLsC5l0JInZ3/voy/iPx1+N\nfX8vBw71x7qXVyS2f71Ks+yp7EnFo12ShVr2I911xRPHxK9QNlRIUrY3eG7pxuEvBSyW302YCK7v\nCAAAHzdJREFUlKxX2emJhEnpeGsF5FnpMZSaUy7X/V3ZXdyRlr1BkLoGu3bj6wmB7R7+roOReRuI\nSho038efMxAlpX53aWBbU35+jIiUzLIHHBm+aing49wv+hr6nsV4rA4c6oucBHe21OcbDpZ/PtMK\nDyboERJCzrIvTNlPbqnHLR9agi971jtXNBEPq6yCArJP60K8FqW0MoKWfXw3vilpzXuOjhmbuqp5\nj/O6m8OW0mn8IrNZw0kse41OUNNue63kbW58P1GfjTOm+Al62e0J1tgDToJev1JYOMm3hAzhy+Hi\nxOzranJjefdArzV5Ual4CvyJV53mNLbrxKGntz/0Xmes7MT67va8JaK2/z0nQc8XnmEFPULsfOq4\nWZg3cRTWzmot+Bob541PlKldVmKW3KykhMOAsvf+HpVMFmHu6HK0hwyTPe+5F3iS9eJYUAq5NeQK\nyjqOqAQ908RCW5k6Zt9UlwmM14tfwURls6dS4e/R7ynQ6EnH2pmt+OSGWaH3ABx3dV9/UDbKvceG\n2R3G8+JY9o2exL9de3qsFnXYt9yr2G+47yns7emNPaEy1Rs4cKgvVNkv6RyDlAiUAu555GVc9+Mn\nE7rxYw1tQKCyJxVPV3sTfvgXazGqbogo60Ekt9KgvOMIuPHzYvbh55oe8t4Jgr62ybLXinbj3HF5\nZWDjWvbi+d2WwZ5OhVuWpjCE37LXyt40YQGCSuCNvU5yYVe7uS2yIHwCYtunY/Z3nbcSH98wM9KP\n39+v0O8uI/OiJ5rnuc2j/Ox+7xAA4JqT5ga6S2q8yv69g72h3imb/L3eAcCpfhd3xYFJGfcc6g+t\nP+HkKjjW/KX//ht89WfP2S17w/XL+X9KZU/IECanrAbnKXLvRatw9QlzAtv9Cjuvfn9kWMIUs8+h\nlYIpQVMrL/8DN05Yw7v0DghP0AtjWmtwCVpP1rJ3lX2tM1G1Zb7bJkS25aYi4XK1jTmpi1uX87Up\nUJvM7nviNXecYn1vjXUeZd/TF7q01v92prU24MunLgrIYNeenthJiObwS7hln04JUinJT9CzxewN\nMitnkjCVPakqdFnSUXVDf1VpHJdfMdn4hbB8agtOX5mz1NqbanHJsUF3sHfshdQJ955fXxvixncf\nqP6HaJKiLoAzWSpknf1nT5yLneuCrYB7ep01fDobX1v2NmVvi+XalENUHoFNwSZeZ68U+pQ9xBEV\ngtk0b5x1UtLg8Qa9d6gPYSk5fgU8dWw9TloULLx04V2Pxk5WteVahCXoZdzvgvdzsU20Td+bcobe\nqOxJVfGZrXPwtTOXxqoNX+nEeS5k4/cDPBYv3gf8imktuPiYmaHHb5o3LvE9junOxYK1UjAlfeVK\nKedL4MAhw4J5H0qp7H1OWTYZr+8xF1xJizkbf3XXWJx9xFSjBbfSjQcvn+r81Fbs3gMWZW/RL/6k\nVB0jL9Sy9yfoxcnG7+3rD8TB9XczrINeU10GrY211vfW5JmQ97nhAhNKKes4/crzzX0HY/dXMIV6\nomL2KXfJoXcymSRmX+CCopJAZU+qirqaNI6bm1y5DCXyyuUOsmUPOA88nZBnbaDj2fwxg+UbRefY\neiyd4kzYggV7cmhL1W8VxhHHlgXj0Tm2Hi9cvwXzJjZn48x+Upa67WFKYX13B35z9XFZZa+z2U0u\nf8AuR+8k5n2LJmD2+KbsvcMUte16cVzch09vwQ3b5zv3VwoHDvVbP4PucU24eH0X1s4MJs/qEIZt\npDp5UWNbWusskfSFiSzvLyXxl+ianBz9EZn/mVQwLNFzyOxFohufEBIL2zPHuz0bsx/kyv/Xvn9+\n3v396Bj1KUsnGTPDr9g8O/a9Gg3lWjU6Zuy35vyJWyb+alP+GLot9ddtBWSi1mN7E0rXdbfjvz5x\nJN6/2Fzz33alXk/3JvF4GETCJzT6ev718XFi9u1NdVjmTlL6+hUO9PYFLPhcm2nBJccdhq0Lxgeu\nk6seaL6P93vR2VIf+AzzvueWa/hlkBKJXVbblsQZ9rGavgv7LV6kjMGNT8ueEBIglhu/TNn4uYe9\neX9zfQ3+318ejc+7kwI/F/hq2ocR5irWD2z/A74uYz9H41fWn7AsRbN1mIuKV/s5bFyTVavbLuW3\nUiX7U0Ljv/p6uo+Axh+zt1nIeg39gUN9UCr4GfhvbbqOdv3bviNafhNHj8R3P3pEXk957xgAQznh\n7EDyr9nbr6zK149teWJ4B8RgSMeq7NMC/wf+tz/8fayuegMBlT0hQ5icG39wtb2+XZi6mzK2Ifaa\n5zBSbglc3WjGy4iMG7NXfss+Wtn7sbm4be1XC6nUZjvDr0BuPn1x4HhBvuIMsxK1IvMPMU6CnkhO\n4e3rcRRZ0LLPxzTx0W58m5z0OXMnjEJrY23AsvcuK00i6Xf3O+GYow9rC3RGjBqzt/aCibTBjX/g\noN2yrySo7AmpUKJ0iYIqW0WutTPb0DAijfPWxLfQ/bQ11eKjR8+w7tcTmJQ4He5M67W1Ze9XFCND\n4vw2rOVyRYzaptjeAv+5c022TbP/Ulvmj8cFa6fh7vNXZrcp5BSg8yNsqZq4182/sH9CY/uO6cnC\newcdK9QfFvHPLU2yyE70LPdYM7MV09sanPX+yE3YLtvYHTjPtvbdJIFbH3weAHDdtvn4+aXrrd35\nbJ9f2HwobehJYHfjW5YrlmkSQGVPyBAjV0HPVzp3EGlrqsUT12zE/EnNBV/j4Ss24FL9YA8hrCyr\njtn7Lfu/O2UhPrBsUqLx2JR9TcZcmtbmBo7D6q6xmD+pORdTNySgXbFlDuZOaMZfHn9YbrvnmDBn\njh6af4hxSiQrlZP5XteyHxnhKTFdN8qyH1M/Av/zqaMxd4LzHdKhmCljneWz+V4N8yTF5NHatccp\nSKSV+Y8vXotfXr4+cJy1YmLE981/mi0hMJM2h39K2YAqCVT2hAwxTAl61djRT7+jMAWlFbTfsp/c\nUo8vnLww0f1sbvymuhpjTL0QC23jvHE4YsZYXL9tAYCc9Rt2KV2nIJEbX3sAPEryq2cujT1OPZ6c\nZW9O0NOYFGQ2Zm+5h1/p6c9Qf6ZehV5ImVk9prqaNMY3B9359gTYMDd+KnZPCZtlH6cj40AwaMpe\nRFpE5Hsisk9EXhSR02Oc898iokRk6FdIIaRE6GVQXe2NOGnRRNSPSGP7kvhW7N9/YCE+EuI+rzTC\nFGF2nX0JJjs2y35UXcb4gC/kod1UV4NvXXA4JrvFn6KWp/nJZuMjfAWGvpr3mOMNS1JNdxXxuvF1\nzD7cjT+1tT7vfMCbjR8vEU5byCN8MqmrSRvk73q33L8uWDsNd354Rd4RUSWT9xti7VFNd0xufOux\ntj4IZbLsB1OJ/iOAgwA6ACwC8CMReVwp9YTpYBE5AwCLoRPiY9KYetx93kos7hyNhtoMfn/NxkTn\nb/NNDPzLsyoFrVDCHo45y774+9lKvzaMyOCd/cE1+KV4aNfGsOy9etWrZ7wKVyT/b62QCl3qpScy\nWcs+YnXD3AnNaG2sxRt7e1CTSuFgX392mZ9NNwYse6Ute2d788ga7FzfhY3zxuHV3fb2t4BTFGlW\nRxOuPmEOrv7P3wOwx/k1b+47GNj2ytv7cfv/Pm89x5SgZyNu2d7BYlAsexFpALAdwGeUUnuVUg8C\n+AGAMy3HNwO4CsClgzE+QoYaa2a2Zlu/FsOT12zEg5etK8GIBo7QimZZpVa8ZW9bg56ylMstRaJV\nbcTytMBYPAfaCvQAwCi3w2NLw4iCxqUnMvf9zqlxXxcj4XGaa91rJa5LBR8xw9yt0v+56pi9d5Lw\nsXVdmNHWaI3Z567l/Ex7PsMoy17H9m3csH0+Lt/UnSdDJ2Yf37LXR569ytwMaDAZLDf+LAC9Sqmn\nPdseBzDXcvy1AP4ZwGsDPTBCKo3BjL6PHJEOVDKrNMKerVqxxC2kEkbYsjTT5Uth2WulHFeBeI+q\nq0nj3NVTA9sB4Pi5Hbhu23xj34K863nue84RuWtpuer37U/QMyXGadWWVfbuRGbVjLHGBD+/Za8/\nQ5P72ybq7BJQ933UeA6MsuzDlP2Y+hp8cHknLjxqBh77zLGecYR3G/Ti/T51dZgLNg0mg6XsGwG8\n69v2LoCABERkGYDVAG6KuqiI7BCRR0TkkV27dpVkoISQyiCO+tbP01LE7G1ufMB5+O84cnpWgQGl\nsezPWzMNH149Deu626MPltzEJ/vTEusXEZy2ojNRvYGFk3MrK/z5CM0j8yOqRnFr69qVi1fZ1Roq\nGvrlpycW2iLP22tT9tDLM/MnGt7r2Dh1xWQAwFiD9yOsNkBBMfsKSKAdLGW/F8Ao37ZmAHu8G0Qk\nBeCfAHxcKRVZZkgp9TWl1DKl1LK2traSDZaQclJZkb7KJuvGt1j2p62YjJOXxkteNLnxv7DdyZoX\nEfz15tmY3pbrL79y2tikww0wd0Iz/uaEOdn19iZ07Lc2kzIqtVLg7UuvELSKA8recI1UiLI3jdav\nNHVpYLNln79t+dQxvv0I3DNKRDvXzcTz123GnAl+1WQPC4V1QPRTY1l6Vy4GS9k/DSAjIt72WAsB\n+JPzRgFYBuDbIvIagIfd7a+IyNqBHyYh5af8NkA+t3xoCf5j5+qy3T/seZl141seztdtW4AbTwku\nwbv9nGWBbabGJR9YPjnvb+2+vuu8FdhiqAc/EGyePx4XHjU9r5a/ft+b5jsZ9mtmFmbsaNmuOyzf\ns+C37MOaEeWu5ZyjFXOU5yO49M75qT0s3hCD/i0lwAOfPhoXrHWKOWWTOA2ToKglciLOMTefviSw\nz99t0DvmuEZ6OiqOMMgMymiUUvsAfBfANSLSICJrAJwI4C7foe8AmAAnW38RgM3u9qUAfjUYYyWE\n5LNx3ngscLu2VRo5yz7+OXPGj8J6TwtdTZzsaW3xDeYKhpp0Cpdvmo3mkTVZBaZd1MuntuCF67dY\nm/jExV+7wa+I/YrTpPC0btNzJm87XePSRX/MXuW75POvnds2tbUh1xPCN/4kGfD6HL/XAgAOWb5Q\nmZTEnox7JzuVMIEfzKnHRwGMBPBnAN8C8BGl1BMi0ikie0WkUzm8pl8AdCD+daVUcJ0EIVVIBXn+\nyovKdVazMcaNt8Z11YcRp268jhaUqzCKsijEQkfjzwHIbc9t0Ml1Cz3VEk1r/LMJeoZkOdP4bAl6\nWkmaikfZSPkmQXEIq21wKNSyj9lVL52rvFgBIfvBW2evlHoLwPsM21+Ck8BnOucF8NlHhhkV8Fyo\nCLJWW8gxjbUZPP25TYksOpt84yn76AnIQJJViCHvt5B4fkrs7mldUOcHO9fgd398B1tvehAbZgc9\nI7mJg6t4I+TpnzBduWU2PvejJzF+dB0mjh6Jq0/ILdbS1wwsN/W78RN8D8LEZFvdkUS2mVSqomL2\nrExHCBnSJO2sZ7PM4jzIczHiRLcsGb0hy9M0hawS8J7hn8h4J0HzJjbjheu3mK/hq3ngnZCYlJ7f\nCD9/7XSc78bi//ev8mvZ6/ObfMo+l43vjjWBZV9I18IkMXt2vSOEkCFKf0hceTDIrkUP+N1zv56+\nsjPBFd2kuhhVCuNdyVMJL0LxJnO5OzTVmYuq6olGEsu7oHr7KQktU+zFO9kZ7BbUJqjsCalQytW+\ntlIYqOdj2HWv3DLbvhMVpOwtSu3i9V34zJY5ia+bvyQ8X0BhIQPTNbLr5fPOMyXdxR/fwV4nYa6p\nzuyMTpKgd8mxszDNk+SXhGSWfaqi/oOp7AmpUOJaENVOqfWqrb85gGy7VRtfOXUxNszuwITRdaUd\nVEz8SWwaPTGsrUkXWNmv+LBANkM+68b3ZuMHj0+S5Li3xym74lf2YUvvbJy8dBLu//TRse/tJZNK\nGf8r13e346eXHJm3LZ0StLsdC8cUWLa4lFDZE0KGDXd+eIVx3b0maoK1uHMMbj17WWTy2UChY/Z+\nhV7ohEif572cVtrfcLvIJXXjawWcdJ19GPvcDnU2N76pqI6NQrwynW6XwpSYPUMTRtcF2ujWpAUX\nHjkdXz51EU5cOCHxPUsNlT0hFcpwd+MPBEfNagtvIOR5kJ9SguV8pSab/Fbi5C+TAhxd7yjWuG78\nrQudIkNd7c7iKpMT/45zlme3JVH2x83pwClLJ+EzW/NDFLl19vEt+0JEd+9Fq3DrWcsgYo7Zv7Xv\nYEBO6ZQgk07hpEUTy7Z6wwuz8QkhFclRs9rw2z++M6gFbPRj/IgZY/HFEA9AudCV3ZIoyoev2BB5\njL9FLpDzIsS17N+/eBK2LpiAK773W/zq+beM3obu8bniP0neQ11N2vh55OoOwB1rDGVfgLbvGFWH\njjl17j2D+9/cezCQkBhXboMFlT0hFcb7F0/E/U/9GTvXd5V7KGXlk8fOwpmrpqBj1ODFxysgaTqU\nPkv9+DD1FTZZ0ud5LXv9m55YJFnOVpNOGWVoatxTiiRHfatczH5g3PhRpFMSmESUun9BsVDZE1Jh\nNNXV4I5zV5R7GGUnnZJBVfRALmZfAV5XI72WBD1NKZd4LZo8GicunIBPbJgZfbB3DO5PUxjK6wIv\npTKMW4/fOba4exnb+xquyXX2hBBSoWT7o1dovkSfJUGvUHIJep414e7PEZkUvnLa4rxOfwXfx1A2\nthQlh3P97J2fcfILipWdvucJnqQ7U+OcSojTe6GyJ4QQF39jlUrDuvTO/bNQw76UDdpUzrTPYq6g\nNxCW/cC78fXbm+Jm6APAob4EnZjKBJU9IYS4VEKlszByRXXyH93FeiK855dKBZuuM1DS1fMGUxMd\n27FejpsTrPVvw+9NAHLhFc1PPpm/5r4SoLInhBAX/1KuSiMyZp/werke9MWMyj8GU0e8gcXfCCcs\nE95k2X/trGW4dONhse6VzevwbNMV/jSzOswth8fUm+sEDAZM0COEVD3/tuNw7HcLs4SSjdlXJrrn\ne7DffHHXddaPDxxaCQ+U50R8ln33OLOyBexu/Lg5BMoQ64njxn/4ig3ZDoLlgMqeEFL1HD59bKLj\nK9SwR1/fwBTVyXu/RV564minklyLp0Ts189dgXsefjm7r1Tce9EqfP///pj1xNSPyOC2s5dhcecY\n6zk20cVdHWBqvex345sYzHoRJqjsCSHExeSirSQ+um4GHnnxbevkJanhbMrGL9bEv/iYmegeNwrr\nu9uz22a0NeLyzeFNhgph+dQWLJ/akrftmNnh8XebZR87cU8Fl2easvErDSp7QghxcWvWVGzMfsGk\n0XjkymBFvGJHm5LSTXBq0ilsWTC+RFcrPbZVAG/s7Yl1vqmOALPxCSFkCKETqzbPr1xlFUahnRIF\nAxuz93LaismxytoONnU16VjHnbt6GjbMbsdZq6Zktw0FZU/LnhBCXDrH1uMPn9uI2ky8B3/FkG0v\nm/C0/NPzNw4Q121bgOu2LRjYmxTAjiOnY313O775q5eyTYBMtDSMwK1nL8/bNhTc+LTsCSHEw5BT\n9CheP4sAK6c5se9Tlk4ufkAVSFRSY11NGvMmNuO6bfNx2cbuWNe85UNLAQAHadkTQgipVLy5CZNb\n6vHC9VvKOJqB5b5PHInHXnq7pNfcMNtJQuweP6qk1x0IqOwJIaRKqHxncvnoam9EV3vxdf69ZNIp\n3HvRKsx0r/u7zx5fsSs56MYnhJAhzvYlk9BYm8H2JRMLOr/CqwRXNMuntmB0vVNToLE2g4bayrSh\nK3NUhBBCYtM5th6/++zx5R4GqWBo2RNCCCFVDpU9IYQQUuVQ2RNCyDClQgsFkgGAyp4QQoYpcZu/\nkKEPE/QIIWSYcvs5y/Hth1/GpDGl7UZHKg8qe0IIGabMaGvEXw9ANzpSedCNTwghhFQ5VPaEEEJI\nlUNlTwghhFQ5VPaEEEJIlUNlTwghhFQ5VPaEEEJIlUNlTwghhFQ5VPaEEEJIlUNlTwghhFQ5VPaE\nEEJIlUNlTwghhFQ5VPaEEEJIlUNlTwghhFQ5opQq9xhKgojsAvBiCS/ZCuCNEl5vuEI5Fg9lWDyU\nYfFQhqWh1HKcopRqizqoapR9qRGRR5RSy8o9jqEO5Vg8lGHxUIbFQxmWhnLJkW58QgghpMqhsieE\nEEKqHCp7O18r9wCqBMqxeCjD4qEMi4cyLA1lkSNj9oQQQkiVQ8ueEEIIqXKo7AkhhJAqh8reh4i0\niMj3RGSfiLwoIqeXe0yVhojUishtrnz2iMivRWSTZ/8xIvKUiLwnIveLyBTPPhGRG0TkTfd1g4hI\ned5JZSAiM0XkgIjc7dlGGSZARE4VkSfd/9tnRWStu51yjIGITBWRH4vI2yLymojcLCIZdx9laEBE\ndorIIyLSIyJf9+0rWGbuZ3G/e+5TIrKhJANWSvHleQH4VwDfBtAIYA2AdwDMLfe4KukFoAHA1QCm\nwpkwbgWwx/271ZXZKQDqAHwRwEOecy8E8AcAkwBMBPB7ABeV+z2VWZ4/AfBzAHe7f1OGyeR3LJyC\nWoe738eJ7otyjC/DHwO405XTOAC/BXAxZRgqs20A3gfgnwF83bO9KJkB+CWAvwcwEsB2ALsBtBU9\n3nILrJJerhI7CGCWZ9s3AFxf7rFV+gvAb9wv5g4Av/DJdD+AbvfvXwDY4dn/Ye8/wnB7ATgVwD1w\nJk9a2VOGyWT4CwDnGbZTjvFl+CSAzZ6/vwjgq5RhLNl9zqfsC5YZgFkAegA0efb/rBQTKLrx85kF\noFcp9bRn2+MA5pZpPEMCEemAI7sn4Mjqcb1PKbUPwDPIyTBvP4axfEVkFIBrAFzi20UZxkRE0gCW\nAWgTkWdE5BXXBT0SlGMSvgTggyJSLyITAWwCcB8ow0IoRmZzATynlNpj2V8wVPb5NAJ417ftXQBN\nZRjLkEBEagB8E8CdSqmn4MjwHd9hXhn6978LoHG4xPl8/C2A25RSr/i2U4bx6QBQA+BkAGsBLAKw\nGMCVoByT8DMA8+DI4BUAjwD4PijDQihGZlHnFgyVfT57AYzybWuGE48mPkQkBeAuOKGPne7mKBn6\n9zcD2Ktcf9VwQUQWAdgA4B8MuynD+Ox3f96klPqTUuoNOPHOzaAcY+H+H98H4LtwXM6tAMYAuAGU\nYSEUI7MB00FU9vk8DSAjIjM92xbCcU8TD+4s9DY4ltV2pdQhd9cTcGSmj2sAMAM5Gebtx/CV79Fw\nEhpfEpHXAHwawHYReQyUYWyUUm/DsUS9ykX/TjnGowVAJ4CblVI9Sqk3AdwBZ8JEGSanGJk9AWC6\niDRZ9hdOuZMbKu0F4N/gZOQ3gNn4YXK6BcBDABp929tcmW2Hk4n6BeRnol4EJxlIZ0wPm+xdn5zq\n4WQ969eNAP7dlR9lmEyW1wB4GEA7HIv053BCJJRjfBk+B+AyABkAowF8D8C3KMNQmWVcmVwHx8NZ\n524rSmbuc/VG99xtYDb+gH2ALXBiVfsAvATg9HKPqdJeAKbAsZ4OwHE76dcZ7v4NAJ6C42J9AMBU\nz7nifvnfcl9fgFu2eTi/4MnGpwwTy64GwD+5D8XXAHwFQB3lmEiGi1z5vA2n1/o9ADoow1CZXe0+\nB72vq4uVGRyP3wPuuX8AsKEU42VtfEIIIaTKYcyeEEIIqXKo7AkhhJAqh8qeEEIIqXKo7AkhhJAq\nh8qeEEIIqXKo7AkhhJAqh8qeEFJyRGSviEwv9zgIIQ5U9oRUISLygohsEJFzROTBAb7XAyJyvneb\nUqpRKfXcQN6XEBIfKntCiBURyZR7DISQ4qGyJ6R6mQ2nh8Eq162+GwBEpFZEbhSRl0TkdRG5xe3/\nDhE52u0Jf5nboOcOERkjIj8UkV0i8rb7+yT3+M/DaS17s3uPm93tSkS63N+bReQb7vkvisiVbqc1\naM+DO563ReR5Edmk34C7/zkR2ePuO2MQ5UdI1UBlT0j18iScphu/dN3qo93t1wOYBaceehecZhx/\n4zlvHJweEVMA7IDznLjD/bsTTs3umwFAKXUFnMYzO9177ESQm+C06ZwO4CgAZwE417N/JZwa4K1w\n6oTfJg4NcOrcb1JKNQE4AsCvC5YGIcMYKntChhFua+IdAD6plHpLKbUHwLUATvUc1g/gKuW0O92v\nlHpTKfUdpdR77vGfh6O049wv7V77cqXUHqXUCwD+DsCZnsNeVEr9i1KqD8CdAMbDaZ2sxzJPREYq\np1/9cG+fSkhBUNkTMrxog9Ne91ER2e269u9zt2t2KaUO6D9EpF5Evuq64N8F8DMAo11FHkUrnK50\nL3q2vQjHm6B5Tf+ilHrP/bVRKbUPwAfheCf+JCI/EpHu2O+UEJKFyp6Q6sbf1vINOG74uUqp0e6r\nWSnVGHLOpwAcBmClUmoUgCPd7WI53n+/Q3BCAJpOAH+MNXil/kspdSwca/8pAP8S5zxCSD5U9oRU\nN68DmCQiIwBAKdUPR2H+g4i0A4CITBSR40Ou0QRngrBbRFoAXGW4h3FNveuavwfA50WkSUSmALgE\nwN1RAxeRDhE5yY3d9wDYC8etTwhJCJU9IdXN/wB4AsBrIvKGu+0yAM8AeMh1y/8UjuVu40sARsKx\n0h+C4/b38mUAJ7vZ9F8xnP8XAPYBeA7AgwC+BeD2GGNPwZkYvArgLTh5Ah+JcR4hxIcoFeaBI4QQ\nQshQh5Y9IYQQUuVQ2RNCCCFVDpU9IYQQUuVQ2RNCCCFVDpU9IYQQUuVQ2RNCCCFVDpU9IYQQUuVQ\n2RNCCCFVDpU9IYQQUuX8fwBSFWhiYnbdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16e931668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the convergence or course\n",
    "# we can also do this within tensorboard\n",
    "plt.rcParams['figure.figsize'] = 8, 6\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "plt.plot(history)\n",
    "plt.title('Convergence Plot')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will mimick a command line call by putting a `!` exclamation mark in the beginning of the code cell, note that if we are using the `!` to mimick command line calls, we can't use python's variable when passing the logging directory of tensorboard to the command line call, it won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-764efa883dda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hi' is not defined"
     ]
    }
   ],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir='./graphs/no_frills/' --port=8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To terminate the tensorboard visualization in jupyter notebook we can go to the dropdown menu at the top: `Kernel -> Interrupt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "top_k = 5\n",
    "idx = word_index['computer']\n",
    "\n",
    "eval_word = word_index_rev[idx]\n",
    "print(eval_word)\n",
    "\n",
    "# remember the cdist returns a the cosine distance,\n",
    "# so when doing the argsort, which is sorting by ascending\n",
    "# order, the top k most similar word will be the first k one;\n",
    "# and since the most similar word will always be itself, we\n",
    "# exclude that from the returned result\n",
    "vector = word_vectors[idx].reshape(1, -1)\n",
    "sim = cdist(word_vectors, vector, metric = 'cosine').ravel()\n",
    "nearest_indices = np.argsort(sim)[1:(top_k + 1)]\n",
    "for nearest_idx in nearest_indices:\n",
    "    sim_word = word_index_rev[nearest_idx]\n",
    "    print(sim_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Blog: Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "- [Note: CS 20SI Lecture note 4: How to structure your model in TensorFlow](http://web.stanford.edu/class/cs20si/lectures/notes_04.pdf)\n",
    "- [Paper: Yoav Goldberg, Omer Levy (2014) word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method](https://arxiv.org/abs/1402.3722)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
