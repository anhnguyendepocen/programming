{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "    \n",
       "    div.cell {\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 12pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "    }\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir(os.path.join('..', 'notebook_format'))\n",
    "from formats import load_style\n",
    "load_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2017-08-03 22:30:18 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 6.1.0\n",
      "\n",
      "numpy 1.13.1\n",
      "scipy 0.19.1\n",
      "pandas 0.19.2\n",
      "sklearn 0.18.1\n",
      "tensorflow 1.2.1\n",
      "gensim 2.3.0\n",
      "spacy 1.9.0\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "from time import time\n",
    "from joblib import cpu_count\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,scipy,pandas,sklearn,tensorflow,gensim,spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec (Skipgram)\n",
    "\n",
    "At a high level `Word2Vec` is a unsupervised learning algorithm that uses a shallow neural network (with one hidden layer) to learn the vectorial representations of all the unique words/phrases for a given corpus. The advantage that word2vec offers is it tries to preserve the semantic meaning behind those terms. For example, a document may employ the words \"dog\" and \"canine\" to mean the same thing, but never use them together in a sentence. Ideally, the word2vec algorithm would be able to learn the context and place them together in similar vector semantic space.\n",
    "\n",
    "We'll start off by using the Gensim's implementation of the algorithm to provide a high-level intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data:\n",
      "\n",
      " From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "example input:\n",
      " ['From:', 'lerxst@wam.umd.edu', \"(where's\", 'my', 'thing)', 'Subject:', 'WHAT', 'car', 'is', 'this!?', 'Nntp-Posting-Host:', 'rac3.wam.umd.edu', 'Organization:', 'University', 'of', 'Maryland,', 'College', 'Park', 'Lines:', '15', 'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day.', 'It', 'was', 'a', '2-door', 'sports', 'car,', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s.', 'It', 'was', 'called', 'a', 'Bricklin.', 'The', 'doors', 'were', 'really', 'small.', 'In', 'addition,', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body.', 'This', 'is', 'all', 'I', 'know.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name,', 'engine', 'specs,', 'years', 'of', 'production,', 'where', 'this', 'car', 'is', 'made,', 'history,', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car,', 'please', 'e-mail.', 'Thanks,', '-', 'IL', '----', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst', '----']\n"
     ]
    }
   ],
   "source": [
    "# the .data attribute will access the raw data, where\n",
    "# each element is a document\n",
    "newsgroups_train = fetch_20newsgroups(subset = 'train')\n",
    "\n",
    "# gensim’s Word2vec expects a sequence of sentences as its input,\n",
    "# where each sentence a list of words. We'll be lazy for now\n",
    "# and not perform any sort of text preprocessing\n",
    "documents = [doc.strip().split() for doc in newsgroups_train.data]\n",
    "\n",
    "# exmaple output of the data\n",
    "print('raw data:\\n\\n', newsgroups_train.data[0])\n",
    "print('example input:\\n', documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time: 8.590173959732056\n",
      "word vector dimension:  (44593, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1.727985</td>\n",
       "      <td>0.884424</td>\n",
       "      <td>0.407958</td>\n",
       "      <td>0.411752</td>\n",
       "      <td>-1.106832</td>\n",
       "      <td>-0.894197</td>\n",
       "      <td>1.426168</td>\n",
       "      <td>-0.730258</td>\n",
       "      <td>0.946965</td>\n",
       "      <td>0.290469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.872513</td>\n",
       "      <td>-0.228314</td>\n",
       "      <td>0.125581</td>\n",
       "      <td>-0.468293</td>\n",
       "      <td>-0.751194</td>\n",
       "      <td>-0.221729</td>\n",
       "      <td>0.087633</td>\n",
       "      <td>1.344074</td>\n",
       "      <td>-0.152140</td>\n",
       "      <td>-0.037577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1.623552</td>\n",
       "      <td>1.457937</td>\n",
       "      <td>0.459143</td>\n",
       "      <td>-0.853248</td>\n",
       "      <td>-1.618912</td>\n",
       "      <td>0.569754</td>\n",
       "      <td>2.360365</td>\n",
       "      <td>1.550627</td>\n",
       "      <td>2.160586</td>\n",
       "      <td>-2.149909</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.317017</td>\n",
       "      <td>-0.027833</td>\n",
       "      <td>2.436808</td>\n",
       "      <td>-1.071193</td>\n",
       "      <td>1.647365</td>\n",
       "      <td>-0.509760</td>\n",
       "      <td>-0.307259</td>\n",
       "      <td>-0.662027</td>\n",
       "      <td>-1.538095</td>\n",
       "      <td>-2.514588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>1.026055</td>\n",
       "      <td>-0.252361</td>\n",
       "      <td>1.921669</td>\n",
       "      <td>2.261415</td>\n",
       "      <td>0.180692</td>\n",
       "      <td>1.862723</td>\n",
       "      <td>1.865289</td>\n",
       "      <td>-0.504147</td>\n",
       "      <td>1.479538</td>\n",
       "      <td>-1.101786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.343724</td>\n",
       "      <td>0.086875</td>\n",
       "      <td>-1.592488</td>\n",
       "      <td>-2.773078</td>\n",
       "      <td>0.828014</td>\n",
       "      <td>-1.083617</td>\n",
       "      <td>-1.049308</td>\n",
       "      <td>-0.260778</td>\n",
       "      <td>0.655367</td>\n",
       "      <td>0.963490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-0.804749</td>\n",
       "      <td>2.035725</td>\n",
       "      <td>0.882453</td>\n",
       "      <td>2.048077</td>\n",
       "      <td>0.790021</td>\n",
       "      <td>0.325440</td>\n",
       "      <td>-0.502577</td>\n",
       "      <td>-2.007840</td>\n",
       "      <td>3.401279</td>\n",
       "      <td>-0.871260</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.508766</td>\n",
       "      <td>1.984295</td>\n",
       "      <td>0.982883</td>\n",
       "      <td>-0.122465</td>\n",
       "      <td>1.306349</td>\n",
       "      <td>-0.506464</td>\n",
       "      <td>0.532626</td>\n",
       "      <td>0.812590</td>\n",
       "      <td>-1.888175</td>\n",
       "      <td>1.405267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.498187</td>\n",
       "      <td>-0.971075</td>\n",
       "      <td>0.061405</td>\n",
       "      <td>-0.601540</td>\n",
       "      <td>0.058572</td>\n",
       "      <td>0.646951</td>\n",
       "      <td>1.653005</td>\n",
       "      <td>0.099037</td>\n",
       "      <td>2.035611</td>\n",
       "      <td>-1.797552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.936209</td>\n",
       "      <td>0.713745</td>\n",
       "      <td>-1.389299</td>\n",
       "      <td>-1.022560</td>\n",
       "      <td>-0.541957</td>\n",
       "      <td>-1.953762</td>\n",
       "      <td>-1.340296</td>\n",
       "      <td>-0.193264</td>\n",
       "      <td>1.115122</td>\n",
       "      <td>-1.028546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "the  1.727985  0.884424  0.407958  0.411752 -1.106832 -0.894197  1.426168   \n",
       "to   1.623552  1.457937  0.459143 -0.853248 -1.618912  0.569754  2.360365   \n",
       "of   1.026055 -0.252361  1.921669  2.261415  0.180692  1.862723  1.865289   \n",
       "a   -0.804749  2.035725  0.882453  2.048077  0.790021  0.325440 -0.502577   \n",
       "and  0.498187 -0.971075  0.061405 -0.601540  0.058572  0.646951  1.653005   \n",
       "\n",
       "           7         8         9     ...           90        91        92  \\\n",
       "the -0.730258  0.946965  0.290469    ...    -0.872513 -0.228314  0.125581   \n",
       "to   1.550627  2.160586 -2.149909    ...    -2.317017 -0.027833  2.436808   \n",
       "of  -0.504147  1.479538 -1.101786    ...    -0.343724  0.086875 -1.592488   \n",
       "a   -2.007840  3.401279 -0.871260    ...    -1.508766  1.984295  0.982883   \n",
       "and  0.099037  2.035611 -1.797552    ...    -0.936209  0.713745 -1.389299   \n",
       "\n",
       "           93        94        95        96        97        98        99  \n",
       "the -0.468293 -0.751194 -0.221729  0.087633  1.344074 -0.152140 -0.037577  \n",
       "to  -1.071193  1.647365 -0.509760 -0.307259 -0.662027 -1.538095 -2.514588  \n",
       "of  -2.773078  0.828014 -1.083617 -1.049308 -0.260778  0.655367  0.963490  \n",
       "a   -0.122465  1.306349 -0.506464  0.532626  0.812590 -1.888175  1.405267  \n",
       "and -1.022560 -0.541957 -1.953762 -1.340296 -0.193264  1.115122 -1.028546  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apart from the input sentence, the only additional paramter\n",
    "# we'll set is to specify use all possible cpu to train the model\n",
    "workers = cpu_count()\n",
    "\n",
    "start = time()\n",
    "word2vec = Word2Vec(documents, workers = workers)\n",
    "elapse = time() - start\n",
    "print('elapse time:', elapse)\n",
    "\n",
    "# we can save and load the model if needed\n",
    "# word2vec_checkpoint = 'word2vec'\n",
    "# word2vec.save(word2vec_checkpoint)\n",
    "# word2vec = Word2Vec.load(word2vec_checkpoint)\n",
    "\n",
    "# obtain the learned word vectors (.wv.syn0)\n",
    "# and the vocabulary/word that corresponds to each word vector\n",
    "word_vectors = pd.DataFrame(word2vec.wv.syn0, index = word2vec.wv.index2word)\n",
    "print('word vector dimension: ', word_vectors.shape)\n",
    "word_vectors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has learned the word vectors, we can use them to look up related words and phrases (words that have similar semantic meaning) for a given term of interest by comparing distances between the vectors using distance metric such as cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('network', 0.8365573883056641),\n",
       " ('machine', 0.8307294249534607),\n",
       " ('keyboard', 0.8290441632270813),\n",
       " ('modem', 0.8137936592102051),\n",
       " ('board', 0.811998724937439)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar(positive = ['computer'], topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from finding similar words using distance metric such as cosine distance, word vectors have the remarkable property that analogies between words seem to be encoded in the difference between word vectors. For example there seems to be a constant male-female difference vector:\n",
    "\n",
    "<img src=\"img/word_vectors.png\" width=\"40%\" height=\"40%\">\n",
    "\n",
    "\\begin{align}\n",
    "W(woman) - W(man)\n",
    "&\\tilde{=} W(aunt) - W(uncle) \\\\\n",
    "&\\tilde{=} W(queen) - W(king)\n",
    "\\end{align}\n",
    "\n",
    "This property means we also perform vector manipulation (e.g. addition and subtraction) with our word vectors. For example, you might have heard or saw the famous example of: King - male + female = queen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Details\n",
    "\n",
    "The way the model works underneath the hood is that trains a neural network by do the following: Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. \n",
    "\n",
    "There is a **window size** hyperparameter to the algorithm that quantifies the word \"nearby\". A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total). There're some implementations that something even fancier: Instead of using a fix $k$ window around each word, the window is uniformly distributed from $1, 2, ..., K$, where $K$ is the max window size we specify.\n",
    "\n",
    "The diagram below shows some of the training samples (word pairs) we would take from the sentence \"The quick brown fox jumps over the lazy dog.\" We'll used a small window size of 2 just for the example. The word highlighted in blue is the input word.\n",
    "\n",
    "<img src=\"img/skipgram.png\" width=\"60%\" height=\"60%\">\n",
    "\n",
    "After feeding a bunch of word pairs to the network, it is going to tell us the probability for every word in our vocabulary being \"nearby\" word that we chose. The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if we gave the trained network the input word \"Soviet\", the output probabilities should be much higher for words like \"Union\" and \"Russia\" than for unrelated words like \"watermelon\" and \"kangaroo\".\n",
    "\n",
    "\n",
    "So how is this all represented? First of all, we know we can't feed a word just as a text string to a neural network (or probably any machine learning model), i.e. we need a way to represent the words to the network. To do this, we first build a vocabulary of words from our training documents. We'll assume that our corpus has a vocabulary size of 10,000.\n",
    "\n",
    "We’re going to represent an input word like \"ants\" as a one-hot vector. This vector will have 10,000 components (one for every unqiue word in our vocabulary) and we'll place a \"1\" in the position corresponding to the word \"ants\", and 0s in all of the other positions. The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word. Here’s the architecture of our single-layer neural network.\n",
    "\n",
    "<img src=\"img/word2vec_architecture.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "There is no activation function on the hidden layer neurons, but the output neurons use softmax. We’ll come back to this later.\n",
    "\n",
    "When training this network on word pairs, the input is a one-hot vector representing the input word and the training output is also a one-hot vector representing the output word. But when we evaluate the trained network on an input word, the output vector will actually be a probability distribution (i.e., a bunch of floating point values, not a one-hot vector).\n",
    "\n",
    "An alternative diagram that depicts that Skip-gram model architecture well:\n",
    "\n",
    "<img src=\"img/skipgram_architecture.png\" width=\"30%\" height=\"30%\">\n",
    "\n",
    "Where we're using the centre word $w_{(t)}$ to predict the surrounding words and the training objective is to learn word vector representations, a.k.a projections that are good at predicting the nearby words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hidden Layer\n",
    "\n",
    "Let's say that we wish to learn word vectors with 300 features. The number of features is a hyperparameter that we would have to tune to our application to see which one yields the best result. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).\n",
    "\n",
    "Now if we look at what would happen when we multiply the 1 x 10,000 one-hot vector representation of the word with a 10,000 x 300 matrix that represents the hidden layer's weight, it will effectively just select the matrix row corresponding to the \"1\". The following figure is a small example that does a matrix multiplication of a 1 x 5 one hot vector with a 5 x 2 hidden layer's weight to give you a visual. \n",
    "\n",
    "<img src=\"img/hidden_layer.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "This means that the hidden layer of this model is really just operating as a lookup table. The output of the hidden layer is just the \"word vector\" for the input word.\n",
    "\n",
    "## The Output Layer\n",
    "\n",
    "The 1 x 300 word vector for \"ants\" then gets fed to the output layer. The output layer is a softmax regression classifier. There's another documentation on Softmax Regression [here](http://nbviewer.jupyter.org/github/ethen8181/machine-learning/blob/master/deep_learning/softmax.ipynb), but the gist of it is that each output neuron, one per word in our vocabulary will produce an output probability between 0 and 1 and the sum of all these output values will add up to 1.\n",
    "\n",
    "Specifically, each output neuron has a weight vector which it multiplies against the word vector from the hidden layer, then it applies the function `exp(x)` to the result. Finally, in order to get the outputs to sum up to 1, we divide this result by the sum of the results from all 10,000 output nodes. Here’s an illustration of calculating the output probability for the word \"car\".\n",
    "\n",
    "<img src=\"img/output_layer.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "Note that neural network does not know anything about the offset of the output word relative to the input word. In other words, it does not learn a different set of probabilities for the word before the input versus the word after.\n",
    "\n",
    "Recall that in the beginning of the documentation, we mentioned that the goal for word2vec is to represent each word in the corpus as the vector representation while trying to reserve semantic meaning. This means that if two different words have very similar \"contexts\" (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if the word vectors are similar. So to hit the notion home, if two words have similar contexts, then word2vec is motivated to learn similar word vectors for these two words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Word2vec\n",
    "\n",
    "You may have noticed that the skip-gram neural network contains a huge number of weights ... For our example with 300 features and a vocab of 10,000 words, that's 3M weights in the hidden layer and output layer each! Training this on a large dataset would be slow and prone to overfitting, so the word2vec authors introduced a number of tweaks to make training feasible.\n",
    "\n",
    "- Modifying the optimization objective with a technique they called \"Negative Sampling\", which causes each training sample to update only a small percentage of the model’s weights\n",
    "- Subsampling frequent words to decrease the number of training examples\n",
    "- Treating common word pairs or phrases as single \"words\" in their model\n",
    "\n",
    "It’s worth noting that subsampling frequent words and applying Negative Sampling not only reduced the compute burden of the training process, but also improved the quality of their resulting word vectors as well.\n",
    "\n",
    "\n",
    "## Negative Sampling\n",
    "\n",
    "Training a neural network means taking a training example and adjusting all of the neuron weights slightly so that it predicts that training sample more accurately. In other words, each training sample will tweak all of the weights in the neural network. As we discussed above, the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples! Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Here’s how it works:\n",
    "\n",
    "When training the network on the word pair (\"fox\", \"quick\"), we want the \"correct output\" of the network, that is the output neuron corresponding to \"quick\" to output a 1, and for all of the other thousands of output neurons to output a 0.\n",
    "\n",
    "With negative sampling, we are instead going to randomly select just a small number of \"negative\" words (let’s say 5) to update the weights for. (In this context, a \"negative\" word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).\n",
    "\n",
    "> The paper says that selecting 5-20 words works well for smaller datasets, and we can get away with only 2-5 words for large datasets.\n",
    "\n",
    "Recall that the output layer of our model has a weight matrix that's 300 x 10,000. So we will just be updating the weights for our positive word (\"quick\"), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!\n",
    "\n",
    "In the hidden layer, only the weights for the input word are updated (this is true whether you’re using Negative Sampling or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Notation\n",
    "\n",
    "This section goes back and re-visit the mathematical notation for Word2vec skipgram's objective function. Hopefully, the math won't look so daunting after having an understanding of the model from a non-mathematical standpoint.\n",
    "\n",
    "In this model we are given a corpus of words $w$ and their contexts $c$ (the word pair of the targeted word that we've sampled). We consider the conditional probabilities $p(c | w)$ and given a corpus $Text$, the goal is to set the parameters $\\theta$ of $p(c | w; \\theta)$ to maximize our objective function $J_\\theta$, i.e. our the corpus probability, with regard to our model parameters $\\theta$:\n",
    "\n",
    "\\begin{align}\n",
    "J_{\\theta} &= \n",
    "arg\\underset{\\theta}{max} \\prod_{w \\in Text} \\bigg[ \\prod_{c \\in C(w)} p(c | w; \\theta) \\bigg]\n",
    "\\end{align}\n",
    "\n",
    "Here $C(w)$ is word $w$'s set of contexts.\n",
    "\n",
    "One approach for parameterizing the $p(c | w; \\theta)$ part of the skip-gram model is the classic softmax objective function:\n",
    "\n",
    "\\begin{align}\n",
    "p(c | w; \\theta) &= \\dfrac{exp(v_c \\cdot v_w)}{ \\sum_{c' \\in C} exp(v_c' \\cdot v_w)}\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\n",
    "- $v_c$ and $v_w$ are the vector representations for word $c$ and $w$ respectively\n",
    "- $C$ is the set of all available contexts\n",
    "\n",
    "While the objective function above can be computed, it is computationally expensive to do due to the summation $\\sum_{c' \\in C} exp(v_c' \\cdot v_w)$ since there can be thousands if not million of them. And this is where negative sampling comes in. \n",
    "\n",
    "Instead of trying to estimate the probability of the word pair directly, we train a model to differentiate the target word from noise (negative sample). We can thus reduce the problem of predicting the correct word to a binary classification task, where the model tries to distinguish positive/genuine data from noise/negative samples.\n",
    "\n",
    "We know for the word pair we generated from the data we want to maximize its probability, i.e.\n",
    "\n",
    "\\begin{align}\n",
    "arg\\underset{\\theta}{max} \\prod_{(w, c) \\in D} p(D = 1 \\big| c, w; \\theta)\n",
    "\\end{align}\n",
    "\n",
    "Here:\n",
    "\n",
    "- $D$ is the set of all word and context pairs we extract from the text\n",
    "- $p(D = 1 \\big| w, c)$ the probability that the word pair $(w, c)$ came from the corpus data\n",
    "\n",
    "We then generate the set $D'$ of random (w, c) pairs, assuming they are all incorrect. The name \"negative sampling\" stems from the set $D'$ of randomly sampled negative examples. Note that when we pick one random word from the vocabulary, there is some tiny chance that the picked word is actually a valid context. If we consider the large number of vobulary we have, we can argue that the probability is really really tiny, but a lot of the packages still do take care of removing these \"accidents\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a binary classification loss, we can use logistic regression to minimize the negative log-likelihood, leading to the objective function:\n",
    "\n",
    "\\begin{align}\n",
    "J_{\\theta} \n",
    "&= arg\\underset{\\theta}{max} \n",
    "\\prod_{(w, c) \\in D} p(D = 1 \\big| c, w; \\theta) \\prod_{(w, c) \\in D'} p(D = 0 \\big| c, w; \\theta) \\\\\n",
    "&= arg\\underset{\\theta}{max} \n",
    "\\prod_{(w, c) \\in D} p(D = 1 \\big| c, w; \\theta) \\prod_{(w, c) \\in D'} \\big(1 - p(D = 1 \\big| c, w; \\theta)\\big) \\\\\n",
    "&= arg\\underset{\\theta}{max} \n",
    "\\sum_{(w, c) \\in D} log \\big(p(D = 1 \\big| c, w; \\theta)\\big) \\sum_{(w, c) \\in D'} log \\big(1 - p(D = 1 \\big| c, w; \\theta)\\big) \\\\\n",
    "&= arg\\underset{\\theta}{max} \n",
    "\\sum_{(w, c) \\in D} log \\big( \\dfrac{1}{1 + \\text{exp}(-v_c \\cdot v_w)} \\big) \\sum_{(w, c) \\in D'} log \\big (1 - \\dfrac{1}{1 + \\text{exp}(-v_c \\cdot v_w)} \\big) \\\\\n",
    "&= arg\\underset{\\theta}{max}\n",
    "\\sum_{(w, c) \\in D} log \\big( \\dfrac{1}{1 + \\text{exp}(-v_c \\cdot v_w)} \\big) \\sum_{(w, c) \\in D'} log \\big (\\dfrac{1}{1 + \\text{exp}(v_c \\cdot v_w)} \\big) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Note that there're various other approaches to approximate the expensive softmax objective function. Negative sampling is soley discussed here because it is fast and works really really well. If you would like to go deeper with this topic, the following link might be a good place to start. [Blog: On word embeddings - Part 2: Approximating the Softmax](http://ruder.io/word-embeddings-softmax/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Negative Samples\n",
    "\n",
    "One little detail that's missing from the description above is how do we select the negative samples.\n",
    "\n",
    "The negative samples are chosen using the unigram distribution. Essentially, the probability of selecting a word as a negative sample is related to its frequency, with more frequenct words being more likely to be selected as negative samples. Instead of using the raw frequency for $w_i$, $\\text{freq}(w_i)$, in the original word2vec paper, each word is given a weight that's equal to it's frequency (word count) raised to the 3/4 power. The probability for selecting a word is just it's weight divided by the sum of weights for all words.\n",
    "\n",
    "\\begin{align}\n",
    "P(w_i) = \\frac{ {\\text{freq}(w_i)}^{3/4} }{\\sum_{j=0}^{n} \\left({\\text{freq}(w_j)}^{3/4} \\right) }\n",
    "\\end{align}\n",
    "\n",
    "This decision to raise the frequency to the 3/4 power appears to be empirical; as the author claims it outperformed other functions (e.g. just using unigram distribution).\n",
    "\n",
    "Side note: The way this selection is implemented in the original word2vec C code is interesting. They have a large array with 100M elements (which they refer to as the unigram table). They fill this table with the index of each word in the vocabulary multiple times, and the number of times a word’s index appears in the table is given by $P(w_i) \\times \\text{table_size}$. Then, to actually select a negative sample, we just generate a random integer between 0 and 100M, and use the word at that index in the table. Since the higher probability words occur more times in the table, we're more likely to pick those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling Frequenct Words\n",
    "\n",
    "Word2vec has two additional parameters for discarding some of the input words: words appearing less\n",
    "than `min-count` times are not considered as either words or contexts, and in addition frequent words are down-sampled as defined by the `sample` parameter.\n",
    "\n",
    "There are two potential issues with frequently appeared words like \"the\":\n",
    "\n",
    "- When looking at word pairs that includes \"the\", e.g. (\"fox\", \"the\"), \"the\" doesn’t tell us much about the meaning of \"fox\", since it appears in the context of pretty much every word.\n",
    "- We will have more than enough samples of (\"the\", \"the other word for the word pair\") than we need to learn a good vector for \"the\".\n",
    "\n",
    "Word2Vec implements a \"subsampling\" scheme to address this. For each word we encounter in our training text, there is a chance that we will discard it from the text. The probability that we cut the word is related to the word's frequency.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{probability of keeping the word } w_i \n",
    "&= (\\sqrt{\\frac{z(w_i)}{0.001}} + 1) \\cdot \\frac{0.001}{z(w_i)}\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\n",
    "- $z(w_i)$ is the fraction of the total words in the corpus that are that word. For example, if the word \"peanut\" occurs 1,000 times in a 1 billion word corpus, then z(\"peanut\") = 1E-6.\n",
    "- There is also a parameter called `sample` which controls how much subsampling occurs, and the default value is 0.001. Smaller values of `sample` mean words are less likely to be kept\n",
    "\n",
    "Here are some interesting observations of this subsampling function (again this is using the default sample value of 0.001).\n",
    "\n",
    "- $\\text{probability of keeping the word } w_i = 1$ (100% chance of being kept) when $z(w_i) <= 0.0026$. This means that only words which represent more than 0.26% of the total words will be subsampled\n",
    "- $\\text{probability of keeping the word } w_i = 0.5$ (50% chance of being kept) when $z(w_i) <= 0.00746$\n",
    "- $\\text{probability of keeping the word } w_i = 0.033$ (3.3% chance of being kept) when $z(w_i) = 1.0$. That is, if the corpus consisted entirely of word $w_i$, which of course is ridiculous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Phrases\n",
    "\n",
    "Word pair like \"Boston Globe\" (a newspaper) has a much different meaning than the individual words \"Boston\" and \"Globe\". So it makes sense to treat \"Boston Globe\", wherever it occurs in the text, as a single word with its own word vector representation.\n",
    "\n",
    "The formula our phrase models will use to determine whether two tokens $A$ and $B$ constitute a phrase is:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{count(A B) - count_{min}}{count(A) \\cdot count(B)} \\cdot N > threshold\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\n",
    "- $count(A)$ is the number of times token $A$ appears in the corpus\n",
    "- $count(B)$ is the number of times token $B$ appears in the corpus\n",
    "- $count(A B)$ is the number of times the tokens $A B$ appear in the corpus in this specific order\n",
    "- $N$ is the total size of the corpus vocabulary\n",
    "- $count_{min}$ is a user-defined parameter to ensure that accepted phrases occur a minimum number of times,\n",
    "- $threshold$ is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase\n",
    "    \n",
    "As we can infer the formula is designed to make phrases out of words which occur together often relative to the number of individual occurrences. And a higher threshold value will favors phrases made of infrequent words in order to avoid making phrases out of common words like \"and the\" or \"this is\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy\n",
    "\n",
    "\n",
    "The first step to use `spaCy` is to constructs a language processing pipeline, here we're:\n",
    "\n",
    "- Loading the pre-trained english model\n",
    "- Grabbing a sample text and hand it over to spaCy and be prepared to wait..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nice place Better than some reviews give it cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what a surprise What a surprise the Sheraton w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good location       Boston from 17th Floor of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Find an alternative to the Sheraton We stayed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barely Tolerable If it were possible to give o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Nice place Better than some reviews give it cr...\n",
       "1  what a surprise What a surprise the Sheraton w...\n",
       "2  Good location       Boston from 17th Floor of ...\n",
       "3  Find an alternative to the Sheraton We stayed ...\n",
       "4  Barely Tolerable If it were possible to give o..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll be using reviews of a hotel obtained from tripadvisor’s website\n",
    "# to showcase the general process\n",
    "reviews = pd.read_table('hotelreviews.txt', names = ['text'])\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the model/pipeline, once we have\n",
    "# loaded the object, we can call it as\n",
    "# though it were a function\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nice place Better than some reviews give it credit for. Overall, the rooms were a bit small but nice. Everything was clean, the view was wonderful and it is very well located (the Prudential Center makes shopping and eating easy and the T is nearby for jaunts out and about the city). Overall, it was a good experience and the staff was quite friendly. "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab a single document, hand it over to spacy\n",
    "doc = reviews.loc[0, 'text']\n",
    "parsed_doc = nlp(doc)\n",
    "parsed_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...1/20th of a second or so. Although the text looks exactly the same as before, a lot has actually happened under the hood. Let's take a look at what we got during that time. From here, we'll start to look at the functionalities/properties that spaCy provided us out of the box.\n",
    "\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "The first one is sentence detection/segmentation (note that all of these features have already been computed, all we're doing now is accessing it via attribute). Every spaCy document is tokenized into sentences and further into tokens which can be accessed by iterating over the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "Nice place Better than some reviews give it credit for.\n",
      "\n",
      "Sentence 2:\n",
      "Overall, the rooms were a bit small but nice.\n",
      "\n",
      "Sentence 3:\n",
      "Everything was clean, the view was wonderful and it is very well located (the Prudential Center makes shopping and eating easy and the T is nearby for jaunts out and about the city).\n",
      "\n",
      "Sentence 4:\n",
      "Overall, it was a good experience and the staff was quite friendly.\n",
      "\n",
      "tokens:\n",
      "Nice\n"
     ]
    }
   ],
   "source": [
    "# access the sents attribute, which is a\n",
    "# generator that we can loop through\n",
    "for num, sentence in enumerate(parsed_doc.sents):\n",
    "    print('Sentence {}:'.format(num + 1))\n",
    "    print(sentence)\n",
    "    print()\n",
    "\n",
    "# access the first token\n",
    "print('tokens:')\n",
    "print(parsed_doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "\n",
    "Part-of-speech tags (POST) are the properties of the word that are defined by the usage of the word in a grammatically correct sentence. These tags can be used as the text features in information filtering, statistical models and rule based parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nice</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>place</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Better</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>than</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>some</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token_text part_of_speech\n",
       "0       Nice            ADJ\n",
       "1      place           NOUN\n",
       "2     Better          PROPN\n",
       "3       than            ADP\n",
       "4       some            DET"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = [token.orth_ for token in parsed_doc]\n",
    "token_pos = [token.pos_ for token in parsed_doc]\n",
    "\n",
    "post = pd.DataFrame(list(zip(token_text, token_pos)),\n",
    "                    columns = ['token_text', 'part_of_speech'])\n",
    "post.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we can see that the word \"Nice\" is an adjective and so on.\n",
    "\n",
    "\n",
    "## Named Entity Recognition\n",
    "\n",
    "Spacy also consists of a fast entity recognition model which is capable of identifying entitiy phrases from the document. Entities can be of different types, such as – person, location, organization, dates, numericals etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: Better - FAC\n",
      "Entity 2: the Prudential Center - ORG\n"
     ]
    }
   ],
   "source": [
    "# For a given document, the standard way to access entity is\n",
    "# to use the .ents attribute; for each entity, we can then\n",
    "# access the .lable_ attribute to check the entity type for\n",
    "# the entity that got flagged;\n",
    "\n",
    "# please check the documentation to see what the label for\n",
    "# entity means\n",
    "# https://spacy.io/docs/usage/entity-recognition#entity-types\n",
    "for num, entity in enumerate(parsed_doc.ents):\n",
    "    print('Entity {}:'.format(num + 1), entity.orth_, '-', entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform token-level entity analysis. This is basically the name entity recognition that we've already looked at, but at the token by token level. It also provides a inside outside begin indicator. e.g. here \"the Prudential Center\" represents one single entity, so \"the\" is the beginning of the entity (B); \"Prudential Center\" are both inside that entity (I). And the one that does not belong to an entity gets labeled as outside (O)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>inside_outside_begin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>wonderful</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>and</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>it</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>is</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token_text entity_type inside_outside_begin\n",
       "29  wonderful                                O\n",
       "30        and                                O\n",
       "31         it                                O\n",
       "32         is                                O"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_entity_type = [token.ent_type_ for token in parsed_doc]\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_doc]\n",
    "\n",
    "entity = pd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob)),\n",
    "                      columns = ['token_text', 'entity_type', 'inside_outside_begin'])\n",
    "entity.iloc[29:33]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Level Attribute\n",
    "\n",
    "What about a variety of other token-level attributes, such as the relative frequency of tokens (how frequently does each token/word appears in the english vocabulary), and whether or not a token matches any of the following categories?\n",
    "\n",
    "- stopword (grammatically functional words that don't contribute too much to the context)\n",
    "- punctuation\n",
    "- whitespace\n",
    "- number\n",
    "- whether the token is included in spaCy's default vocabulary or not?\n",
    "- In terms of the token's relative frequency, spaCy expresses it as the log probability, so a negative number closer to 0 means it appears more often. Or we can say a smaller absolute value means it commonly appears\n",
    "\n",
    "Please refer to the [documentation page](https://spacy.io/docs/api/token) to see all the available attrbutes at the token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop</th>\n",
       "      <th>punctuation</th>\n",
       "      <th>whitespace</th>\n",
       "      <th>number</th>\n",
       "      <th>out_of_vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>staff</td>\n",
       "      <td>staff</td>\n",
       "      <td>-10.720455</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>-5.404201</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>quite</td>\n",
       "      <td>quite</td>\n",
       "      <td>-8.256200</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>friendly</td>\n",
       "      <td>friendly</td>\n",
       "      <td>-10.444580</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>-3.072948</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text     lemma  log_probability stop punctuation whitespace number  \\\n",
       "68     staff     staff       -10.720455                                      \n",
       "69       was        be        -5.404201  Yes                                 \n",
       "70     quite     quite        -8.256200  Yes                                 \n",
       "71  friendly  friendly       -10.444580                                      \n",
       "72         .         .        -3.072948              Yes                     \n",
       "\n",
       "   out_of_vocab  \n",
       "68               \n",
       "69               \n",
       "70               \n",
       "71               \n",
       "72               "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_attrs = [(token.orth_,\n",
    "                token.lemma_,\n",
    "                token.prob,\n",
    "                token.is_stop,\n",
    "                token.is_punct,\n",
    "                token.is_space,\n",
    "                token.like_num,\n",
    "                token.is_oov)\n",
    "                for token in parsed_doc]\n",
    "\n",
    "df = pd.DataFrame(token_attrs,\n",
    "                  columns = ['text',\n",
    "                             'lemma',\n",
    "                             'log_probability',\n",
    "                             'stop',\n",
    "                             'punctuation',\n",
    "                             'whitespace',\n",
    "                             'number',\n",
    "                             'out_of_vocab'])\n",
    "\n",
    "# we convert the boolean columns to only showing Yes for True\n",
    "# and a blank string for False for a cleaner output\n",
    "df.loc[:, 'stop':'out_of_vocab'] = (df.loc[:, 'stop':'out_of_vocab']\n",
    "                                      .applymap(lambda x: 'Yes' if x else ''))\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing\n",
    "\n",
    "Spacy also offers a fast and accurate dependency parser. Let's parse the dependency tree of all the sentences which contains a targeted term that we specified and check what are the adjectives that were commonly used with that term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target word: place\n",
      "depency:\n",
      "Nice\n"
     ]
    }
   ],
   "source": [
    "# toy example of how to get the depency\n",
    "token = parsed_doc[1]\n",
    "print('target word:', token)\n",
    "print('depency:')\n",
    "\n",
    "# to get the dependency for a token\n",
    "# we can access the .children attribute\n",
    "# and iterate through them\n",
    "for child in token.children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid_word(token):\n",
    "    \"\"\"\n",
    "    Returns False if the spacy token is either\n",
    "    a punctuation, whitespace, number or a pronoun\n",
    "    (indicated by the '-PRON-' flag)\n",
    "    \"\"\"\n",
    "    pron_flag = token.lemma_ != '-PRON-'\n",
    "    word_flag = not (token.is_punct or token.is_space or token.like_num)\n",
    "    valid = word_flag and pron_flag\n",
    "    return valid\n",
    "\n",
    "\n",
    "def post_words(document, target_token, post, topn = 5):\n",
    "    \"\"\"\n",
    "    given a document/corpus, look for the most commonly\n",
    "    associated part of speech tag associated with the specified token\n",
    "    \"\"\"\n",
    "    target_sents = [sent for sent in document.sents if target_token in sent.lower_]    \n",
    "    words = []\n",
    "    for sentence in target_sents:\n",
    "        for token in sentence: \n",
    "            words.extend([child.lemma_\n",
    "                          for child in token.children\n",
    "                          if child.pos_ == post and valid_word(child)])\n",
    "\n",
    "    common_words = Counter(words).most_common(topn)\n",
    "    return common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse: 3.283233880996704\n"
     ]
    }
   ],
   "source": [
    "# lump all the documents into one giant document\n",
    "start = time()\n",
    "corpus = ' '.join(reviews['text'])\n",
    "document = nlp(corpus)\n",
    "elapse = time() - start\n",
    "print('elapse:', elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 41), ('good', 28), ('which', 13), ('small', 13), ('fantastic', 11)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words = post_words(document, target_token = 'view', post = 'ADJ')\n",
    "common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the text you'd like to process is general-purpose English language text (i.e., not domain-specific, like medical literature), spaCy is ready to use out-of-the-box. It will probably become a core part of the Python data science ecosystem — it will do for natural language computing what other great libraries have done for numerical computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import cpu_count\n",
    "\n",
    "\n",
    "def valid_word(token):\n",
    "    \"\"\"\n",
    "    Returns False if the spacy token is either\n",
    "    a punctuation, whitespace, number or a pronoun\n",
    "    (indicated by the '-PRON-' flag)\n",
    "    \"\"\"\n",
    "    pron_flag = token.lemma_ != '-PRON-'\n",
    "    word_flag = not (token.is_punct or token.is_space or token.like_num)\n",
    "    word_len_flag = len(token) >= 2\n",
    "    return pron_flag and word_flag and word_len_flag\n",
    "\n",
    "\n",
    "def clean_corpus(texts, parser, stopwords, batch_size = 10000, n_jobs = -1):\n",
    "    \"\"\"\n",
    "    Generator function using spaCy to parse reviews:\n",
    "    - lemmatize the text\n",
    "    - remove punctuation, whitespace and number\n",
    "    - remove pronoun, e.g. 'it'\n",
    "    \"\"\"\n",
    "    n_threads = cpu_count()\n",
    "    if n_jobs > 0 and n_jobs < n_threads:\n",
    "        n_threads = n_jobs\n",
    "\n",
    "    # use the .pip to process texts as a stream;\n",
    "    # this functionality supports using multi-threads\n",
    "    for parsed_text in parser.pipe(texts, n_threads = n_threads,\n",
    "                                   batch_size = batch_size):\n",
    "        tokens = []\n",
    "        for token in parsed_text:\n",
    "            if valid_word(token) and token.lemma_ not in stopwords:\n",
    "                tokens.append(token.lemma_)\n",
    "\n",
    "        cleaned_text = ' '.join(tokens)\n",
    "        yield cleaned_text\n",
    "        \n",
    "\n",
    "def export_unigrams(unigram_path, texts, parser, stopwords):\n",
    "    \"\"\"\n",
    "    Clean up the text and export it to a .txt file,\n",
    "    where each line is one document\n",
    "    \"\"\"\n",
    "    with open(unigram_path, 'w', encoding = 'utf_8') as f:\n",
    "        for cleaned_text in clean_corpus(texts, parser, stopwords):\n",
    "            f.write(cleaned_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text preprocessing, elapse 226.95283699035645\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "\n",
    "# a set of stopwords built-in to spacy,\n",
    "# we can always expand this set for the\n",
    "# problem that we are working on, here we include\n",
    "# python built-in string punctuation mark\n",
    "stopwords = spacy.en.STOP_WORDS | set(punctuation)\n",
    "\n",
    "# create a directory called 'model' to\n",
    "# store all outputs in later section\n",
    "MODEL_DIR = 'model'\n",
    "if not os.path.isdir(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "\n",
    "UNIGRAM_PATH = os.path.join(MODEL_DIR, 'unigram.txt')\n",
    "if not os.path.exists(UNIGRAM_PATH):\n",
    "    start = time()\n",
    "    export_unigrams(UNIGRAM_PATH, texts = newsgroups_train.data,\n",
    "                    parser = nlp, stopwords = stopwords)\n",
    "    elapse = time() - start\n",
    "    print('text preprocessing, elapse', elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training phrase model, elapse 4.6221442222595215\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "\n",
    "def train_phrase_model(unigram_path, phrase_model_path):\n",
    "    \"\"\"\n",
    "    stream the unigram text file and\n",
    "    train the phrase model to train the phrases\n",
    "    from the unigrams and also save it to disk\n",
    "    \"\"\"\n",
    "\n",
    "    # use LineSetence to stream text\n",
    "    unigram_sentences = LineSentence(unigram_path)\n",
    "    phrase_model = Phrases(unigram_sentences)\n",
    "    phrase_model.save(phrase_model_path)\n",
    "    return phrase_model\n",
    "\n",
    "\n",
    "PHRASE_MODEL_PATH = os.path.join(MODEL_DIR, 'phrase_model')\n",
    "if os.path.exists(PHRASE_MODEL_PATH):\n",
    "    phrase_model = Phrases.load(PHRASE_MODEL_PATH)\n",
    "else:\n",
    "    start = time()\n",
    "    phrase_model = train_phrase_model(UNIGRAM_PATH, PHRASE_MODEL_PATH)\n",
    "    elapse = time() - start\n",
    "    print('training phrase model, elapse', elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting phrase, elapse 5.395565032958984\n"
     ]
    }
   ],
   "source": [
    "def export_bigrams(unigram_path, bigram_path, phrase_model):\n",
    "    \"\"\"\n",
    "    use the learned phrase model to create (potential) bigrams,\n",
    "    and output the text that contains bigrams to disk\n",
    "    \"\"\"\n",
    "    with open(bigram_path, 'w') as fout, open(unigram_path) as fin:\n",
    "        for text in fin:\n",
    "            unigram = text.split()\n",
    "\n",
    "            # after training the Phrase model, it will \n",
    "            # transform any sentence (list of token strings)\n",
    "            # and glue unigrams together into bigrams\n",
    "            bigram = phrase_model[unigram]\n",
    "            bigram_sentence = ' '.join(bigram)\n",
    "            fout.write(bigram_sentence + '\\n')\n",
    "                \n",
    "\n",
    "BIGRAM_PATH = os.path.join(MODEL_DIR, 'bigram.txt')\n",
    "if not os.path.exists(BIGRAM_PATH):\n",
    "    start = time()\n",
    "    export_bigrams(UNIGRAM_PATH, BIGRAM_PATH, phrase_model)\n",
    "    elapse = time() - start\n",
    "    print('converting phrase, elapse', elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def get_size(obj, seen = None):\n",
    "    \"\"\"\n",
    "    Recursively finds size of objects\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    https://goshippo.com/blog/measure-real-size-any-python-object/\n",
    "    \"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    \n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    \n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original size uses 1.677652480574959 times the amount of memory\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "with open(BIGRAM_PATH) as f:\n",
    "    for line in f:\n",
    "        X.append(line)\n",
    "\n",
    "original_size = get_size(newsgroups_train.data)\n",
    "preprocessed_size = get_size(X)\n",
    "ratio = original_size / preprocessed_size\n",
    "print('original size uses {} times the amount of memory'.format(ratio))\n",
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset = 'train')\n",
    "data = newsgroups_train.data\n",
    "\n",
    "# gensim’s Word2vec expects a sequence of sentences as its input,\n",
    "# where each sentence a list of words. We'll be lazy and not perform\n",
    "# any sort of text preprocessing\n",
    "documents = [doc.strip().split() for doc in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['anarchism', 'originated', 'as', 'a']\n",
      "data size 17005207\n"
     ]
    }
   ],
   "source": [
    "from subprocess import call\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "def read_data():\n",
    "    \"\"\"Read data into a list of tokens/words\"\"\"\n",
    "    filename = 'text8.zip'\n",
    "    base_url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        call('wget ' + base_url + filename, shell = True)\n",
    "\n",
    "    with ZipFile(filename) as f:\n",
    "        file = f.namelist()[0]\n",
    "\n",
    "        # ensure compatibility each python2 and python3's str type\n",
    "        # https://stackoverflow.com/questions/37689802/what-is-tensorflow-compat-as-str\n",
    "        data = tf.compat.as_str(f.read(file)).split()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "words = read_data()\n",
    "print('data:', words[:4])\n",
    "print('data size {}'.format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "\n",
    "# words = [word.lower() \n",
    "#          for doc in documents\n",
    "#          for word in doc\n",
    "#          if word not in ENGLISH_STOP_WORDS]\n",
    "# print('data:', words[:4])\n",
    "# print('data size {}'.format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['lerxst@wam.umd.edu', 'thing', 'subject', 'car']\n",
      "data size 1417284\n"
     ]
    }
   ],
   "source": [
    "with open(BIGRAM_PATH) as f:\n",
    "    words = f.read().split()\n",
    "\n",
    "print('data:', words[:4])\n",
    "print('data size {}'.format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words, vocab_size = None):\n",
    "    \n",
    "    word_count = [['UNK', -1]]\n",
    "    word_count.extend(Counter(words).most_common(vocab_size))\n",
    "    word_index = {word: idx for idx, (word, _) in enumerate(word_count)}\n",
    "\n",
    "    # build up word index and replaced the words by its assigned indices\n",
    "    data = []\n",
    "    unknown_count = 0\n",
    "    for word in words:\n",
    "        if word in word_index:\n",
    "            idx = word_index[word]\n",
    "        else:\n",
    "            idx = 0\n",
    "            unknown_count += 1\n",
    "\n",
    "        data.append(idx)\n",
    "\n",
    "    # 'UNK' flag for out of vocabulary word\n",
    "    # unknown = 'UNK', unknown_count\n",
    "    # word_count.append(unknown)\n",
    "    word_count[0][1] = unknown_count\n",
    "    word_index_rev = {idx: word for word, idx in word_index.items()}\n",
    "    return data, word_count, word_index, word_index_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words [['UNK', 0], ('subject', 9520), ('use', 8750), (\"'s\", 7893), ('write', 6884)]\n",
      "Sample data [51815, 16, 1, 57, 40, 21960, 47, 15170, 1074, 698]\n"
     ]
    }
   ],
   "source": [
    "# TODO : ??? do we need to return a 4 element tuple\n",
    "data, word_count, word_index, word_index_rev = build_dataset(words)\n",
    "\n",
    "print('Most common words', word_count[:5])\n",
    "print('Sample data', data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(indexed_words, window):\n",
    "    \"\"\"\n",
    "    Form training pairs according to the skip-gram model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    indexed_words : list\n",
    "        List of index that represents the words, e.g. [5243, 3083, 11],\n",
    "        and 5243 might represent the word \"Today\"\n",
    "        \n",
    "    window : int\n",
    "        Window size of the skip-gram model, where word is sampled before\n",
    "        and after the center word according to this window size\n",
    "    \"\"\"\n",
    "    for index, center in enumerate(indexed_words):\n",
    "        # random integers from `low` (inclusive) to `high` (exclusive)\n",
    "        context = np.random.randint(1, window + 1)\n",
    "\n",
    "        # get a random target before the center word\n",
    "        for target in indexed_words[max(0, index - context):index]:\n",
    "            yield center, target\n",
    "\n",
    "        # get a random target after the center word\n",
    "        for target in indexed_words[(index + 1):(index + 1 + context)]:\n",
    "            yield center, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data: [51815, 16, 1, 57, 40, 21960]\n",
      "skip gram sample:\n",
      "(51815, 16)\n",
      "(16, 51815)\n",
      "(16, 1)\n",
      "(16, 57)\n",
      "(1, 51815)\n",
      "(1, 16)\n",
      "(1, 57)\n",
      "(1, 40)\n"
     ]
    }
   ],
   "source": [
    "iterator = generate_sample(indexed_words = data, window = 3)\n",
    "\n",
    "print('original data:', data[:6])\n",
    "print('skip gram sample:')\n",
    "\n",
    "# we start off by using the first word as the center word,\n",
    "# and since there's no word before it, we will not have any\n",
    "# sampled word before it; after that we keep sliding the center\n",
    "# word and generate word pairs\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(iterator, batch_size):\n",
    "    \"\"\"\n",
    "    Group a numerical stream of centered and targeted\n",
    "    word into batches and yield them as numpy arrays\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype = np.int32)\n",
    "        target_batch = np.zeros((batch_size, 1), dtype = np.int32)\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(iterator)\n",
    "\n",
    "        yield center_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51815 51815 51815    16    16]\n",
      "[[   16]\n",
      " [    1]\n",
      " [   57]\n",
      " [51815]\n",
      " [    1]]\n"
     ]
    }
   ],
   "source": [
    "window = 3\n",
    "batch_size = 5\n",
    "iterator = generate_sample(indexed_words = data, window = window)\n",
    "batches = get_batch(iterator, batch_size)\n",
    "\n",
    "# e.g. generate a batch\n",
    "center_batch, target_batch = next(batches)\n",
    "print(center_batch)\n",
    "print(target_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Define placeholders for input and output. Input is the center word and output is the target (context) word. Instead of using one-hot vectors, we input the index of those words directly.\n",
    "\n",
    "```python\n",
    "# explicitly naming our operations will make it easier later to track them\n",
    "center_words = tf.placeholder(\n",
    "    tf.int32, shape = [BATCH_SIZE], name = 'center_words')\n",
    "\n",
    "# for target_words:\n",
    "# we will use this with tensorflow's loss function later, and the function requires rank 2\n",
    "# input, that's why there's an extra dimension in the shape\n",
    "target_words = tf.placeholder(\n",
    "    tf.int32, shape = [BATCH_SIZE, 1], name = 'target_words')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Define the weight/variable. In this case, the embedding matrix. Each row corresponds to the representation vector of one word. If one word is represented with a vector of size EMBED_SIZE, then the embedding matrix will have shape [VOCAB_SIZE, EMBED_SIZE]. We initialize the embedding matrix to value from a random distribution. In this case, let’s choose uniform distribution.\n",
    "\n",
    "```python\n",
    "# word vectors\n",
    "embed_matrix = tf.Variable(\n",
    "    tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0), name = 'embed_matrix')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Inference (compute the forward path of the graph). Recall that the hidden layer serves as a lookup table and its purpose is to get the vector representations of words in our dictionary.\n",
    "\n",
    "<img src=\"img/hidden_layer.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "i.e. The output of the hidden layer is just the \"word vector\" for the input word. Our embed_matrix has dimension [VOCAB_SIZE x EMBED_SIZE], with each row of the embedding matrix corresponds to the vector representation of the word at that index. So to get the representation of all the center words in the batch, we get the slice of all corresponding rows in the embedding matrix. TensorFlow provides a convenient method to do so called `tf.nn.embedding_lookup()`. This method is really useful when it comes to matrix multiplication with one-hot vectors because it saves us from doing a bunch of unnecessary computation that will return 0 anyway.\n",
    "\n",
    "```python\n",
    "# input -> hidden layer\n",
    "embed = tf.nn.embedding_lookup(embed_matrix, center_words)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Define the loss function and optimizer** For nce_loss, we need weights and biases for the hidden layer to calculate negative sampling loss.\n",
    "\n",
    "```python\n",
    "# hidden layer -> output layer's weights\n",
    "output_weight = tf.Variable(\n",
    "    tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE], stddev = 1.0 / EMBED_SIZE ** 0.5))\n",
    "\n",
    "output_bias = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "\n",
    "# hidden layer -> output layer + negative sampling loss\n",
    "loss = tf.nn.sampled_softmax_loss(\n",
    "    weights = output_weight, biases = output_bias,\n",
    "    labels = target_words, inputs = embed,\n",
    "    num_sampled = NUM_SAMPLED, num_classes = VOCAB_SIZE)\n",
    "\n",
    "avg_loss = tf.reduce_mean(loss, name = 'loss')\n",
    "\n",
    "# choose an optimizer to perform the heavy lifting\n",
    "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "optimize = optimizer.minimize(avg_loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the operations we will create the session to execute the computation, including feeding in the inputs, running the optimizer to minimize the objective function we just defined and fetch the loss value so we can check convergence. The following code chunk pretty much dumped everything into one giant function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tf_word2vec_no_frill import tf_word2vec\n",
    "\n",
    "\n",
    "VOCAB_SIZE = len(word2vec.wv.vocab)  # 50000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 128  # dimension of the word embedding vectors\n",
    "WINDOW_SIZE = 5  # the context window \n",
    "NUM_SAMPLED = 64  # Number of negative examples to sample\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 10\n",
    "TENSORBOARD = './graphs/no_frills/'\n",
    "word2vec_param = {'vocab_size': VOCAB_SIZE, 'batch_size': BATCH_SIZE,\n",
    "                  'embed_size': EMBED_SIZE, 'num_sampled': NUM_SAMPLED,\n",
    "                  'learning_rate': LEARNING_RATE, 'epochs': EPOCHS,\n",
    "                  'tensorboard': TENSORBOARD, 'window_size': WINDOW_SIZE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[23] = 44593 is not in [0, 44593)\n\t [[Node: loss/sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, _class=[\"loc:@loss/output_bias\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](loss/output_bias/read, loss/sampled_softmax_loss/concat)]]\n\nCaused by op 'loss/sampled_softmax_loss/embedding_lookup_1', defined at:\n  File \"/Users/ethen/anaconda/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/ethen/anaconda/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-50-5313f884b874>\", line 15, in <module>\n    word_vectors, history = tf_word2vec(data, **word2vec_param)\n  File \"/Users/ethen/programming/word2vec/tf_word2vec_no_frill.py\", line 59, in tf_word2vec\n    num_sampled = num_sampled, num_classes = vocab_size), name = 'loss')\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py\", line 1268, in sampled_softmax_loss\n    name=name)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py\", line 993, in _compute_sampled_logits\n    biases, all_ids, partition_strategy=partition_strategy)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 122, in embedding_lookup\n    return maybe_normalize(_do_gather(params[0], ids, name=name))\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 42, in _do_gather\n    return array_ops.gather(params, ids, name=name)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1179, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[23] = 44593 is not in [0, 44593)\n\t [[Node: loss/sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, _class=[\"loc:@loss/output_bias\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](loss/output_bias/read, loss/sampled_softmax_loss/concat)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[23] = 44593 is not in [0, 44593)\n\t [[Node: loss/sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, _class=[\"loc:@loss/output_bias\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](loss/output_bias/read, loss/sampled_softmax_loss/concat)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-5313f884b874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# actual model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# word_vectors, history = tf_word2vec(batch_gen, **word2vec_param)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mword_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mword2vec_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/programming/word2vec/tf_word2vec_no_frill.py\u001b[0m in \u001b[0;36mtf_word2vec\u001b[0;34m(data, epochs, learning_rate, num_sampled, window_size, batch_size, embed_size, vocab_size, tensorboard)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcenter_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[23] = 44593 is not in [0, 44593)\n\t [[Node: loss/sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, _class=[\"loc:@loss/output_bias\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](loss/output_bias/read, loss/sampled_softmax_loss/concat)]]\n\nCaused by op 'loss/sampled_softmax_loss/embedding_lookup_1', defined at:\n  File \"/Users/ethen/anaconda/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/ethen/anaconda/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-50-5313f884b874>\", line 15, in <module>\n    word_vectors, history = tf_word2vec(data, **word2vec_param)\n  File \"/Users/ethen/programming/word2vec/tf_word2vec_no_frill.py\", line 59, in tf_word2vec\n    num_sampled = num_sampled, num_classes = vocab_size), name = 'loss')\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py\", line 1268, in sampled_softmax_loss\n    name=name)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py\", line 993, in _compute_sampled_logits\n    biases, all_ids, partition_strategy=partition_strategy)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 122, in embedding_lookup\n    return maybe_normalize(_do_gather(params[0], ids, name=name))\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 42, in _do_gather\n    return array_ops.gather(params, ids, name=name)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1179, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/ethen/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[23] = 44593 is not in [0, 44593)\n\t [[Node: loss/sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, _class=[\"loc:@loss/output_bias\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](loss/output_bias/read, loss/sampled_softmax_loss/concat)]]\n"
     ]
    }
   ],
   "source": [
    "# words = read_data()\n",
    "# words = [word.lower() \n",
    "#          for doc in documents\n",
    "#          for word in doc\n",
    "#          if word not in ENGLISH_STOP_WORDS]\n",
    "with open(BIGRAM_PATH) as f:\n",
    "    words = f.read().split()\n",
    "\n",
    "data, word_count, word_index, word_index_rev = build_dataset(words, VOCAB_SIZE)\n",
    "# iterator = generate_sample(indexed_words = data, window = WINDOW_SIZE)\n",
    "# batch_gen = get_batch(iterator, BATCH_SIZE)\n",
    "\n",
    "# actual model training\n",
    "# word_vectors, history = tf_word2vec(batch_gen, **word2vec_param)\n",
    "word_vectors, history = tf_word2vec(data, **word2vec_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the convergence or course\n",
    "# we can also do this within tensorboard\n",
    "plt.rcParams['figure.figsize'] = 8, 6\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "plt.plot(history)\n",
    "plt.title('Convergence Plot')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('network', 0.8365573883056641),\n",
       " ('machine', 0.8307294249534607),\n",
       " ('keyboard', 0.8290441632270813),\n",
       " ('modem', 0.8137936592102051),\n",
       " ('board', 0.811998724937439)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar(positive = ['computer'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 5\n",
    "positive = ['computer']\n",
    "negative = []\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "normed = normalize(word2vec.wv.syn0)\n",
    "\n",
    "positive = [\n",
    "    (word, 1.0) if isinstance(word, str) else word\n",
    "    for word in positive\n",
    "]\n",
    "negative = [\n",
    "    (word, -1.0) if isinstance(word, str) else word\n",
    "    for word in negative\n",
    "]\n",
    "positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the weighted average of all words\n",
    "all_words, mean = set(), []\n",
    "for word, weight in positive + negative:\n",
    "    word_idx = word2vec.wv.vocab[word].index\n",
    "    vector = normed[word_idx]\n",
    "    mean.append(weight * vector)\n",
    "    if word in word2vec.wv.vocab:\n",
    "        all_words.add(word_idx)      \n",
    "\n",
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vector = np.mean(mean, axis = 0)\n",
    "mean_vector /= np.sqrt(np.sum(mean_vector ** 2))\n",
    "dists = np.dot(normed, mean_vector)\n",
    "best = np.argsort(dists)[::-1][:(topn + len(all_words))]\n",
    "result = [(word2vec.wv.index2word[sim], float(dists[sim]))\n",
    "          for sim in best if sim not in all_words]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "def most_similar(word_vectors, word_index, word_index_rev,\n",
    "                 positive = None, negative = None, topn = 5):\n",
    "    normed = normalize(word_vectors)\n",
    "\n",
    "    if positive is None:\n",
    "        positive = []\n",
    "    else:\n",
    "        positive = [\n",
    "            (word, 1.0) if isinstance(word, str) else word\n",
    "            for word in positive\n",
    "        ]\n",
    "\n",
    "    if negative is None:\n",
    "        negative = []\n",
    "    else:\n",
    "        negative = [\n",
    "            (word, -1.0) if isinstance(word, str) else word\n",
    "            for word in negative\n",
    "        ]\n",
    "\n",
    "    # compute the weighted average of all words\n",
    "    all_words, mean = set(), []\n",
    "    for word, weight in positive + negative:\n",
    "        word_idx = word_index[word]\n",
    "        word_vector = normed[word_idx]\n",
    "        mean.append(weight * word_vector)\n",
    "        all_words.add(word_idx) \n",
    "\n",
    "    mean_vector = np.mean(mean, axis = 0)\n",
    "    mean_vector /= np.sqrt(np.sum(mean_vector ** 2))\n",
    "    dists = np.dot(normed, mean_vector)\n",
    "    best = np.argsort(dists)[::-1][:(topn + len(all_words))]\n",
    "    result = [\n",
    "        (word_index_rev[sim], float(dists[sim]))\n",
    "        for sim in best if sim not in all_words\n",
    "    ]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['network']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'computer'\n",
    "# word = 'network'\n",
    "most_similar(word_vectors, word_index, word_index_rev, positive = [word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer \n",
      "\n",
      "[ 0.67916571  0.68050226  0.68619972  0.69257105  0.69547772]\n",
      "stan_kerr\n",
      "timer_hour\n",
      "quarterback\n",
      "display_:0\n",
      "gia\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "top_k = 5\n",
    "idx = word_index['computer']\n",
    "\n",
    "eval_word = word_index_rev[idx]\n",
    "print(eval_word, '\\n')\n",
    "\n",
    "# remember the cdist returns a the cosine distance,\n",
    "# so when doing the argsort, which is sorting by ascending\n",
    "# order, the top k most similar word will be the first k one;\n",
    "# and since the most similar word will always be itself, we\n",
    "# exclude that from the returned result\n",
    "vector = word_vectors[idx].reshape(1, -1)\n",
    "sim = cdist(word_vectors, vector, metric = 'cosine').ravel()\n",
    "nearest_indices = np.argsort(sim)[1:(top_k + 1)]\n",
    "print(1 - sim[nearest_indices])\n",
    "\n",
    "for nearest_idx in nearest_indices:\n",
    "    sim_word = word_index_rev[nearest_idx]\n",
    "    print(sim_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network\n",
      "machine\n",
      "keyboard\n",
      "modem\n",
      "board\n"
     ]
    }
   ],
   "source": [
    "w2v = {w: vec for w, vec in zip(word2vec.wv.index2word, word2vec.wv.syn0)}\n",
    "vector = w2v['computer'].reshape(1, -1)\n",
    "sim = cdist(vector, word2vec.wv.syn0, metric = 'cosine').ravel()\n",
    "nearest_indices = np.argsort(sim)[1:(top_k + 1)]\n",
    "\n",
    "for nearest_idx in nearest_indices:\n",
    "    sim_word = word2vec.wv.index2word[nearest_idx]\n",
    "    print(sim_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 500\n",
    "\n",
    "embedding = word_vectors[:top]\n",
    "\n",
    "metadata_file = 'top_vocab.tsv'\n",
    "top_words = pd.DataFrame([word_index_rev[i] for i in range(top)])\n",
    "top_words.to_csv(metadata_file, sep = '\\t', index = False, header = False)\n",
    "top_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from subprocess import call\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "\n",
    "def launch_tensorboard(embedding, log_dir, metadata_file = None):\n",
    "    \"\"\"\n",
    "    Mainly use for visualizing embedding for now\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding : str\n",
    "\n",
    "    log_dir : str\n",
    "\n",
    "    metadata_file : str\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Tensorflow Documentation: TensorBoard: Embedding Visualization\n",
    "    - https://www.tensorflow.org/get_started/embedding_viz\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    if isinstance(embedding, np.ndarray):\n",
    "        embedding_var = tf.Variable(embedding, name = 'embedding')\n",
    "    else:\n",
    "        embedding = np.genfromtxt(embedding, dtype = np.float32, delimiter = '\\t')\n",
    "        embedding_var = tf.Variable(embedding, name = embedding_file.split('.')[0])\n",
    "\n",
    "    # do a fake run of the model (not really running anything),\n",
    "    # simply initialize the variable and save it to checkpoint\n",
    "    saver = tf.train.Saver()\n",
    "    init = tf.global_variables_initializer()\n",
    "    model_checkpoint = os.path.join(log_dir, 'model.ckpt')\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        saver.save(sess, model_checkpoint, global_step = 1)\n",
    "\n",
    "    # boilerplate setup:\n",
    "    # we can add multiple embeddings, here we add only one\n",
    "    config = projector.ProjectorConfig()\n",
    "    config_embedding = config.embeddings.add()\n",
    "    config_embedding.tensor_name = embedding_var.name\n",
    "\n",
    "    if metadata_file is not None:\n",
    "        # Link to its metadata file (e.g. labels that provides descriptions\n",
    "        # for the each data points in the embeddings)\n",
    "        config_embedding.metadata_path = metadata_file\n",
    "\n",
    "    # use the same log_dir where we stored our model checkpoint\n",
    "    summary_writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    # the next line writes a projector_config.pbtxt in the log_dir\n",
    "    # TensorBoard will read this file during startup\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "    # lauch TensorBoard\n",
    "    call('tensorboard --logdir={}'.format(log_dir), shell = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_tensorboard(embedding, log_dir = TENSORBOARD, metadata_file = metadata_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To terminate the tensorboard visualization in jupyter notebook we can go to the dropdown menu at the top: `Kernel -> Interrupt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "The most crucial decisions that affect the performance are the choice of the model\n",
    "architecture, the size of the vectors, the subsampling rate, and the size of the training window\n",
    "\n",
    "## Applications\n",
    "\n",
    "Word2Vec and the concept of word embeddings originated in the domain of NLP, however the idea of words in the context of a sentence or a surrounding word window can be generalized to other problem domain dealing with sequences or sets of related data points. For example:\n",
    "\n",
    "- A direct application of Word2Vec to a classical engineering task was recently presented by Spotify. They abstracted the ideas behind Word2Vec to apply them not simply to words in sentences but to any object in any sequence, in this case to songs in a playlist. Songs are treated as words and other songs in a playlist as their surrounding context, depending on whether the playlists in question were genre specific the vocabulary encompassed a number of songs from that collection. Now in order to recommend songs to a user one merely has to examine a neighborhood of the 'song embeddings' of songs the user already likes\n",
    "- Similarly, we could recommend a user who to connect to in a social network setting by examining the graph of relationships, where the nodes represent words and a path through the graph represents a sentence, then nodes that occur in the context of similar other nodes, would be close together in the vector space.\n",
    "\n",
    "These examples show that the general applicability of Word2Vec based algorithms is very rich and that it behooves practitioners to examine their problem domain with respect to sequences of objects that occur in some meaningful context.\n",
    "\n",
    "## Resources\n",
    "\n",
    "A lot of the non-mathematical notes were taken from Chris McCormick's blog post. If you wish to learn more about the topic, the following link also contains different resources that Chris has curated (including a commented version of word2vec's original C code, derivation of word2vec's gradient update, etc.). [Blog: Word2Vec Resources](http://mccormickml.com/2016/04/27/word2vec-resources/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Blog: Demystifying Word2Vec](http://www.deeplearningweekly.com/blog/demystifying-word2vec)\n",
    "- [Blog: Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "- [Blog: Word2Vec Tutorial Part 2 - Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n",
    "- [Note: CS 20SI Lecture note 4: How to structure your model in TensorFlow](http://web.stanford.edu/class/cs20si/lectures/notes_04.pdf)\n",
    "- [Paper: Yoav Goldberg, Omer Levy (2014) word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method](https://arxiv.org/abs/1402.3722)\n",
    "- [Paper: Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean (2013) Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "\n",
    "---\n",
    "\n",
    "- [Blog: Natural Language Processing Made Easy – using SpaCy (in Python)](https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
