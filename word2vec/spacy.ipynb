{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# download the package first\n",
    "pip install spacy\n",
    "\n",
    "# after that download the trained english model\n",
    "python -m spacy download en\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nice place Better than some reviews give it cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what a surprise What a surprise the Sheraton w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good location       Boston from 17th Floor of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Find an alternative to the Sheraton We stayed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barely Tolerable If it were possible to give o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Nice place Better than some reviews give it cr...\n",
       "1  what a surprise What a surprise the Sheraton w...\n",
       "2  Good location       Boston from 17th Floor of ...\n",
       "3  Find an alternative to the Sheraton We stayed ...\n",
       "4  Barely Tolerable If it were possible to give o..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_table('hotelreviews.txt', names = ['text'])\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to use `spaCy` is to constructs a language processing pipeline, here we're:\n",
    "\n",
    "- Loading the pre-trained english model\n",
    "- Grabbing a sample text and hand it over to spaCy and be prepared to wait..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the model/pipeline, once we have\n",
    "# loaded the object, we can call it as\n",
    "# though it were a function\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nice place Better than some reviews give it credit for. Overall, the rooms were a bit small but nice. Everything was clean, the view was wonderful and it is very well located (the Prudential Center makes shopping and eating easy and the T is nearby for jaunts out and about the city). Overall, it was a good experience and the staff was quite friendly. "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab a single document\n",
    "doc = reviews.loc[0, 'text']\n",
    "parsed_doc = nlp(doc)\n",
    "parsed_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...1/20th of a second or so. Although the text looks exactly the same as before, a lot has actually happened under the hood. Let's take a look at what we got during that time. From here, we'll start to look at the functionalities/properties that spaCy provided us out of the box.\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "The first one is sentence detection/segmentation (note that all of these features have already been computed, all we're doing now is accessing it via attribute). Every spaCy document is tokenized into sentences and further into tokens which can be accessed by iterating over the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "Nice place Better than some reviews give it credit for.\n",
      "\n",
      "Sentence 2:\n",
      "Overall, the rooms were a bit small but nice.\n",
      "\n",
      "Sentence 3:\n",
      "Everything was clean, the view was wonderful and it is very well located (the Prudential Center makes shopping and eating easy and the T is nearby for jaunts out and about the city).\n",
      "\n",
      "Sentence 4:\n",
      "Overall, it was a good experience and the staff was quite friendly.\n",
      "\n",
      "tokens:\n",
      "Nice\n"
     ]
    }
   ],
   "source": [
    "# access the sents attribute, which is a\n",
    "# generator that we can loop through\n",
    "for num, sentence in enumerate(parsed_doc.sents):\n",
    "    print('Sentence {}:'.format(num + 1))\n",
    "    print(sentence)\n",
    "    print()\n",
    "\n",
    "# access the first token\n",
    "print('tokens:')\n",
    "print(parsed_doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "\n",
    "Part-of-speech tags (POST) are the properties of the word that are defined by the usage of the word in a grammatically correct sentence. These tags can be used as the text features in information filtering, statistical models and rule based parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nice</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>place</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Better</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>than</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>some</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token_text part_of_speech\n",
       "0       Nice            ADJ\n",
       "1      place           NOUN\n",
       "2     Better          PROPN\n",
       "3       than            ADP\n",
       "4       some            DET"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the orth_ attribute will give us\n",
    "# the string representation of the\n",
    "# token as oppose to a spacy type token\n",
    "token_text = [token.orth_ for token in parsed_doc]\n",
    "token_pos = [token.pos_ for token in parsed_doc]\n",
    "\n",
    "post = pd.DataFrame(list(zip(token_text, token_pos)),\n",
    "                    columns = ['token_text', 'part_of_speech'])\n",
    "post.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we can see that the word \"Nice\" is an adjective and so on.\n",
    "\n",
    "\n",
    "## Named Entity Recognition\n",
    "\n",
    "Spacy consists of a fast entity recognition model which is capable of identifying entitiy phrases from the document. Entities can be of different types, such as â€“ person, location, organization, dates, numericals, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: Better - FAC\n",
      "Entity 2: the Prudential Center - ORG\n"
     ]
    }
   ],
   "source": [
    "# For a given document, the standard way to access entity is\n",
    "# to use the .ents attribute; for each entity, we can then\n",
    "# access the .lable_ attribute to check the entity type for\n",
    "# the entity that got flagged;\n",
    "\n",
    "# please check the documentation to see what the label for\n",
    "# entity means\n",
    "# https://spacy.io/docs/usage/entity-recognition#entity-types\n",
    "for num, entity in enumerate(parsed_doc.ents):\n",
    "    print('Entity {}:'.format(num + 1), entity.orth_, '-', entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform token-level entity analysis. This is basically the name entity recognition that we've already looked at, but at the token by token level. It also provides a inside outside begin indicator. e.g. here \"the Prudential Center\" represents one single entity, so \"the\" is the beginning of the entity (B); \"Prudential Center\" are both inside that entity (I). And the one that does not belong to an entity gets labeled as outside (O)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>inside_outside_begin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>the</td>\n",
       "      <td>ORG</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Prudential</td>\n",
       "      <td>ORG</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Center</td>\n",
       "      <td>ORG</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>makes</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text entity_type inside_outside_begin\n",
       "37         the         ORG                    B\n",
       "38  Prudential         ORG                    I\n",
       "39      Center         ORG                    I\n",
       "40       makes                                O"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_entity_type = [token.ent_type_ for token in parsed_doc]\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_doc]\n",
    "\n",
    "entity = pd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob)),\n",
    "                      columns = ['token_text', 'entity_type', 'inside_outside_begin'])\n",
    "entity.iloc[37:41]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Level Attribute\n",
    "\n",
    "What about a variety of other token-level attributes, such as the relative frequency of tokens (how frequently does each token/word appears in the english vocabulary), and whether or not a token matches any of the following categories?\n",
    "\n",
    "- stopword (grammatically functional words that don't contribute too much to the context)\n",
    "- punctuation\n",
    "- whitespace\n",
    "- number\n",
    "- whether the token is included in spaCy's default vocabulary or not?\n",
    "- In terms of the token's relative frequency, spaCy expresses it as the log probability, so a negative number closer to 0 means it appears more often. Or we can say a smaller absolute value means it commonly appears\n",
    "\n",
    "Please refer to the [documentation page](https://spacy.io/docs/api/token) to see all the available attrbutes at the token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop</th>\n",
       "      <th>punctuation</th>\n",
       "      <th>whitespace</th>\n",
       "      <th>number</th>\n",
       "      <th>out_of_vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nice</td>\n",
       "      <td>nice</td>\n",
       "      <td>-9.845901</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>place</td>\n",
       "      <td>place</td>\n",
       "      <td>-8.045827</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Better</td>\n",
       "      <td>better</td>\n",
       "      <td>-10.571031</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>than</td>\n",
       "      <td>than</td>\n",
       "      <td>-6.372464</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>some</td>\n",
       "      <td>some</td>\n",
       "      <td>-6.402781</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>reviews</td>\n",
       "      <td>review</td>\n",
       "      <td>-11.378132</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>give</td>\n",
       "      <td>give</td>\n",
       "      <td>-7.725083</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>it</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>-4.506450</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>credit</td>\n",
       "      <td>credit</td>\n",
       "      <td>-8.618998</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>-4.913970</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>-3.072948</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Overall</td>\n",
       "      <td>overall</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>-3.391480</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>-3.425446</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rooms</td>\n",
       "      <td>room</td>\n",
       "      <td>-11.772737</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text    lemma  log_probability stop punctuation whitespace number  \\\n",
       "0      Nice     nice        -9.845901                                      \n",
       "1     place    place        -8.045827                                      \n",
       "2    Better   better       -10.571031                                      \n",
       "3      than     than        -6.372464  Yes                                 \n",
       "4      some     some        -6.402781  Yes                                 \n",
       "5   reviews   review       -11.378132                                      \n",
       "6      give     give        -7.725083  Yes                                 \n",
       "7        it   -PRON-        -4.506450  Yes                                 \n",
       "8    credit   credit        -8.618998                                      \n",
       "9       for      for        -4.913970  Yes                                 \n",
       "10        .        .        -3.072948              Yes                     \n",
       "11  Overall  overall       -19.579313                                      \n",
       "12        ,        ,        -3.391480              Yes                     \n",
       "13      the      the        -3.425446  Yes                                 \n",
       "14    rooms     room       -11.772737                                      \n",
       "\n",
       "   out_of_vocab  \n",
       "0                \n",
       "1                \n",
       "2                \n",
       "3                \n",
       "4                \n",
       "5                \n",
       "6                \n",
       "7                \n",
       "8                \n",
       "9                \n",
       "10               \n",
       "11               \n",
       "12               \n",
       "13               \n",
       "14               "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_attrs = [(token.orth_,\n",
    "                token.lemma_,\n",
    "                token.prob,\n",
    "                token.is_stop,\n",
    "                token.is_punct,\n",
    "                token.is_space,\n",
    "                token.like_num,\n",
    "                token.is_oov)\n",
    "                for token in parsed_doc]\n",
    "\n",
    "df = pd.DataFrame(token_attrs,\n",
    "                  columns = ['text',\n",
    "                             'lemma',\n",
    "                             'log_probability',\n",
    "                             'stop',\n",
    "                             'punctuation',\n",
    "                             'whitespace',\n",
    "                             'number',\n",
    "                             'out_of_vocab'])\n",
    "\n",
    "# we convert the boolean columns to only showing Yes for True\n",
    "# and a blank string for False for a cleaner output\n",
    "df.loc[:, 'stop':'out_of_vocab'] = (df.loc[:, 'stop':'out_of_vocab']\n",
    "                                      .applymap(lambda x: 'Yes' if x else ''))\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing\n",
    "\n",
    "Spacy also offers a fast and accurate dependency parser. Let's parse the dependency tree of all the sentences which contains the a targeted term and check what are the adjectival tokens are used with that term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target word: place\n",
      "depency:\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# toy example of how to get the depency\n",
    "token = parsed_doc[1]\n",
    "print('target word:', token)\n",
    "print('depency:')\n",
    "\n",
    "# to get the dependency for a token\n",
    "# we can access the .children attribute\n",
    "# and iterate through them\n",
    "for c in token.children:\n",
    "    print(c.is_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def post_words(document, token, post, topn = 5):\n",
    "    \"\"\"\n",
    "    given a document/corpus, look for the most commonly\n",
    "    associated part of speech tag associated with the specified token\n",
    "    \"\"\"\n",
    "    target_sents = [sent for sent in document.sents if token in sent.lower_]    \n",
    "    words = []\n",
    "    for sentence in target_sents:\n",
    "        for word in sentence: \n",
    "            words.extend([child.lemma_\n",
    "                          for child in word.children\n",
    "                          if child.pos_ == post and child.lemma_ != '-PRON-'])\n",
    "\n",
    "    common_words = Counter(words).most_common(topn)\n",
    "    return common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 41), ('good', 28), ('which', 13), ('small', 13), ('fantastic', 11)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lump all the documents into one giant document\n",
    "corpus = ' '.join([review for review in reviews['text']])\n",
    "document = nlp(corpus)\n",
    "common_words = post_words(document, token = 'view', post = 'ADJ')\n",
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import cpu_count\n",
    "\n",
    "\n",
    "def valid_word(token):\n",
    "    \"\"\"\n",
    "    Returns False if the spacy token is either\n",
    "    a punctuation, whitespace, number or a pronoun\n",
    "    (indicated by the '-PRON-' flag)\n",
    "    \"\"\"\n",
    "    pron_flag = token.lemma_ != '-PRON-'\n",
    "    word_flag = not (token.is_punct or token.is_space or token.like_num)\n",
    "    return word_flag and pron_flag\n",
    "\n",
    "\n",
    "def clean_corpus(texts, parser, stopwords, batch_size = 10000, n_jobs = -1):\n",
    "    \"\"\"\n",
    "    Generator function using spaCy to parse reviews:\n",
    "    - lemmatize the text\n",
    "    - remove punctuation, whitespace and number\n",
    "    - remove pronoun, e.g. 'it'\n",
    "    \"\"\"\n",
    "    n_threads = cpu_count()\n",
    "    if n_jobs > 0 and n_jobs < n_threads:\n",
    "        n_threads = n_jobs\n",
    "    \n",
    "    # use the .pip to process texts as a stream;\n",
    "    # this functionality supports using multi-threads\n",
    "    for parsed_text in parser.pipe(texts, batch_size = batch_size, n_threads = n_threads):\n",
    "        tokens = []\n",
    "        for token in parsed_text:\n",
    "            if valid_word(token) and token.lemma_ not in stopwords:\n",
    "                tokens.append(token.lemma_)\n",
    "        \n",
    "        cleaned_text = ' '.join(tokens)\n",
    "        yield cleaned_text\n",
    "        \n",
    "\n",
    "def export_unigrams(unigram_path, texts, parser, stopwords):\n",
    "    \"\"\"\n",
    "    Clean up the text and export it to a .txt file,\n",
    "    where each line is one document\n",
    "    \"\"\"\n",
    "    with open(unigram_path, 'w', encoding = 'utf_8') as f:\n",
    "        for cleaned_text in clean_corpus(texts, parser, stopwords):\n",
    "            f.write(cleaned_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a set of stopwords built-in to spacy\n",
    "stopwords = spacy.en.STOP_WORDS\n",
    "\n",
    "texts = reviews['text']\n",
    "UNIGRAM_PATH = 'unigram.txt'\n",
    "if not os.path.exists(UNIGRAM_PATH):\n",
    "    export_unigrams(UNIGRAM_PATH, texts = texts, parser = nlp, stopwords = stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Blog: Natural Language Processing Made Easy â€“ using SpaCy (in Python)](https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
