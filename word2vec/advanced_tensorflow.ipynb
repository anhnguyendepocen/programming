{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2017-07-24 15:00:48 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 5.4.1\n",
      "\n",
      "numpy 1.13.1\n",
      "tensorflow 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from subprocess import call\n",
    "\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard\n",
    "\n",
    "In [Googleâ€™s own words](https://www.tensorflow.org/get_started/summaries_and_tensorboard): \"The computations you'll use TensorFlow for - like training a massive deep neural network - can be complex and confusing. To make it easier to understand,\n",
    "debug, and optimize TensorFlow programs, we've included a suite of visualization tools called TensorBoard.\n",
    "\n",
    "The following code chunk contains a couple important concepts. When specify our computational graph, there are two tricks that can make our lives easier later on.\n",
    "\n",
    "- The first one is giving names to our operations (a.k.a ops), e.g. `tf.add(a, b, name = 'y')`\n",
    "- The other one is to group similar nodes together with `tf.name_scope()`. In the simple example below, we group all ops related to input together and all ops related to output together. It might seem like an overkill for this trivial example, but when we start to build models that are more and more complex, the debugging process can grow to be extremely daunting.\n",
    "\n",
    "Then it's about collecting summary data along the way. To elaborate, in machine learning models, we often wish to track how the objective/loss function or accuracy is changing over iterations to evaluate whether to model converged or not. With Tensorflow, we can collect these by attaching `tf.summary.scalar` ops to the nodes that output the scalar/value that we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "5.0\n",
      "8.0\n",
      "13.0\n",
      "21.0\n",
      "34.0\n",
      "55.0\n",
      "89.0\n",
      "144.0\n",
      "233.0\n"
     ]
    }
   ],
   "source": [
    "LOG_PATH = './tmp/example-1/'\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    a = tf.placeholder(tf.float32, name = 'a')\n",
    "    b = tf.placeholder(tf.float32, name = 'b')\n",
    "\n",
    "with tf.name_scope('output'):\n",
    "    y = tf.add(a, b, name = 'y')\n",
    "\n",
    "# create a summary to monitor y,\n",
    "# we can create other summary by just\n",
    "# adding more tf.summary.scalar\n",
    "tf.summary.scalar('y_value', y)\n",
    "\n",
    "# merge all summaries into a single op\n",
    "# that generates all the summary data\n",
    "# so we don't have to handle them individually\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # create log writer object and pass in the log path\n",
    "    # the logdir is quite important, it's the directory where all of the \n",
    "    # events will be written out. The FileWriter can optionally take a Graph \n",
    "    # in its constructor, which in our case will be the computational graph \n",
    "    # for the current session. If it receives a Graph object, then TensorBoard\n",
    "    # will visualize our graph along with tensor shape information\n",
    "    writer = tf.summary.FileWriter(logdir = LOG_PATH, graph = sess.graph)\n",
    "\n",
    "    # perform calculation, where we just keep adding numbers\n",
    "    a_value = 1\n",
    "    b_value = 2\n",
    "    for i in range(10):\n",
    "        summary, y_value = sess.run([merged, y], feed_dict = {a: a_value, b: b_value})\n",
    "\n",
    "        # we write logs at every iteration, note that we could also consider writing\n",
    "        # the summary log for every n iterations instead\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "        a_value = b_value\n",
    "        b_value = y_value\n",
    "        print(y_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mimick command line call to lauch tensorboard\n",
    "# tensorboard --logdir=[Our log path]\n",
    "call('tensorboard --logdir={}'.format(LOG_PATH), shell = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can navigate to http://0.0.0.0:6006 to look at our tensorboard. Or we can add an additional argument to the command line call to specify the port for the tensorboard, e.g. if we add `--port=8001` then the tensorboard will be located at `localhost:8001` in our browser.\n",
    "\n",
    "The tensorboard should look very similar to the screenshot below:\n",
    "\n",
    "Under the scalars tab: We've only logged the value for y, so there will only be one scalar summary.\n",
    "\n",
    "<img src=\"images/tensorboard_summary.png\" width=\"60%\" height=\"60%\">\n",
    "\n",
    "Under the graph tab, since we have grouped two ops under the input name scope, if we click on the plus sign on top of its name scope block, we can see all the ops inside that block.\n",
    "\n",
    "<img src=\"images/tensorboard_graph.png\" width=\"60%\" height=\"60%\">\n",
    "\n",
    "Note that we might want to delete the original logdir directory if we were to re-run the code several times. Or else Tensorboard might include duplicate things in it or display warning since there will be multiple event files.\n",
    "\n",
    "To learn more, the following link has a more comprehensive guide for configuring Tensorboard. [Tensorflow Documentation: TensorBoard: Visualizing Learning](https://www.tensorflow.org/get_started/summaries_and_tensorboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queues and Coordinators\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.80146751,  -9.16441146,  -8.95531492,   9.18517489],\n",
       "       [ -0.85832641,  -9.27301211,   1.4138157 ,   3.31116201],\n",
       "       [-18.77402339,   7.97979842,  22.10523166, -12.26695909],\n",
       "       ..., \n",
       "       [ -7.70182672,  -5.23895938,  -5.03024986,   8.77285877],\n",
       "       [  6.80702113,  -2.51661907, -13.29981891,   0.79241951],\n",
       "       [  3.99524144,   3.32551964, -11.65769596,   5.71774662]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_SAMPLES = 1000\n",
    "N_THREADS = 4\n",
    "\n",
    "# Generating some simple data\n",
    "# create 1000 random samples, each is a 1D array from the normal distribution\n",
    "data = 10 * np.random.randn(N_SAMPLES, 4) + 1\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create 1000 random labels of 0 and 1\n",
    "target = np.random.randint(0, 2, size = N_SAMPLES) \n",
    "target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queue = tf.FIFOQueue(capacity = 50, dtypes = [tf.float32, tf.int32], shapes=[[4], []])\n",
    "enqueue_op = queue.enqueue_many([data, target])\n",
    "data_sample, label_sample = queue.dequeue()\n",
    "\n",
    "# create NUM_THREADS to do enqueue\n",
    "queue_runner = tf.train.QueueRunner(queue, [enqueue_op] * N_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.80146754 -9.16441154 -8.95531464  9.18517494]\n",
      "1\n",
      "[-0.85832644 -9.27301216  1.41381574  3.31116199]\n",
      "1\n",
      "[-18.77402306   7.97979832  22.10523224 -12.26695919]\n",
      "1\n",
      "[ -1.58828139  -4.32889605  11.34604073 -10.0936327 ]\n",
      "1\n",
      "[ -5.84605885  -4.87499094   5.8274188  -16.76111603]\n",
      "1\n",
      "[  5.37085438  -5.12846851  31.64710999   2.97502017]\n",
      "0\n",
      "[-15.67945766   1.69698191   8.96185017 -12.79179668]\n",
      "1\n",
      "[ -1.01402497   3.75115848   3.21570444  10.64387035]\n",
      "1\n",
      "[ 15.26858902 -12.67862034  -4.838974   -15.67602539]\n",
      "1\n",
      "[  1.97815228 -11.94149208  -4.73292971  -1.97893286]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# create ops that do something with data_sample and label_sample\n",
    "with tf.Session() as sess:\n",
    "    # create a coordinator, launch the queue runner threads.\n",
    "    coord = tf.train.Coordinator()\n",
    "    enqueue_threads = queue_runner.create_threads(sess, coord = coord, start = True)\n",
    "    try:\n",
    "        # just do 10 iterations\n",
    "        for step in range(10):\n",
    "            if coord.should_stop():\n",
    "                break\n",
    "            \n",
    "            data_batch, label_batch = sess.run([data_sample, label_sample])\n",
    "            print(data_batch)\n",
    "            print(label_batch)\n",
    "    except Exception as e:\n",
    "        coord.request_stop(e)\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        coord.join(enqueue_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Blog: Just another Tensorflow beginner guide (Part1)](http://liufuyang.github.io/2017/03/12/just-another-tensorflow-beginner-guide-1.html)\n",
    "- [Tensorflow Documentation: TensorBoard: Visualizing Learning](https://www.tensorflow.org/get_started/summaries_and_tensorboard)\n",
    "- [Note: CS 20SI Lecture note 9:  Input Pipeline](http://web.stanford.edu/class/cs20si/lectures/notes_09.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
