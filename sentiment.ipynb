{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = 'aclImdb'\n",
    "labels = { 'pos': 1, 'neg': 0 }\n",
    "\n",
    "df_all = []\n",
    "for i in ( 'test', 'train' ):\n",
    "    for j in ( 'pos', 'neg' ):\n",
    "        \n",
    "        path = os.path.join( basepath, i, j )\n",
    "        for file in os.listdir(path):\n",
    "            \n",
    "            with open( os.path.join( path, file ), 'r', encoding = 'utf-8' ) as f:\n",
    "                txt = f.read()\n",
    "            df = pd.DataFrame( [[ txt, labels[j] ]] )\n",
    "            df_all.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I went and saw this movie last night after bei...          1\n",
       "1  Actor turned director Bill Paxton follows up h...          1\n",
       "2  As a recreational golfer with some knowledge o...          1\n",
       "3  I saw this film in a sneak preview, and it is ...          1\n",
       "4  Bill Paxton has taken the true story of the 19...          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie = pd.concat( df_all, ignore_index = True )\n",
    "movie.columns = [ 'review', 'sentiment' ]\n",
    "movie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to use nltk.downoad(\"stopwords\") to actually download the stopword corpus\n",
    "import os\n",
    "import string\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize( folder, ngrams ):\n",
    "    \"\"\"all the tokenized word for each ngram and each text file in the folder\"\"\"\n",
    "    tokens = []\n",
    "    for file in os.listdir(folder):\n",
    "        with open( os.path.join( folder, file ), 'r' ) as f:\n",
    "            text = f.read()\n",
    "            \n",
    "            # remove numbers, punctuation marks, leading and trailing whitespaces\n",
    "            # http://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "            table = str.maketrans({ key: None for key in string.punctuation + string.digits })\n",
    "            text = text.translate(table).strip()\n",
    "            \n",
    "            # convert to lower case, remove stop words\n",
    "            words = text.lower().split()\n",
    "            stop_words = set( stopwords.words(\"english\") )\n",
    "            words = [ w for w in words if not w in stop_words ]\n",
    "                        \n",
    "            for ngram in set(ngrams):\n",
    "                if ngram == 1:\n",
    "                    tokens.extend(words)\n",
    "                else:\n",
    "                    for i in range( len(words) - ngram + 1 ):\n",
    "                        word = '_'.join( words[ i:(i + ngram) ] )\n",
    "                        tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def build_dict( folder, ngrams ):\n",
    "    \"\"\"return the word count for each ngrams\"\"\"\n",
    "    word_counts = Counter()    \n",
    "    tokens = tokenize( folder, ngrams )   \n",
    "    word_counts.update(tokens)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_ratio( pos_counts, neg_counts, alpha = 1 ):\n",
    "\n",
    "    # sets's .union return unique elements that are in either of the two sets\n",
    "    all_tokens = set( pos_counts.keys() ).union( set( neg_counts.keys() ) )\n",
    "\n",
    "    # assign each token an index number\n",
    "    word_dict = { token: index for index, token in enumerate(all_tokens) }\n",
    "    word_count = len(word_dict)\n",
    "\n",
    "    p, q = np.ones(word_count) * alpha, np.ones(word_count) * alpha\n",
    "    for t in all_tokens:\n",
    "        p[ word_dict[t] ] += pos_counts[t]\n",
    "        q[ word_dict[t] ] += neg_counts[t]\n",
    "        \n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "    r = np.log( p / q )\n",
    "    return word_dict, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = 'aclImdb'\n",
    "pos_path = os.path.join( basepath, 'train', 'pos1' )\n",
    "neg_path = os.path.join( basepath, 'train', 'neg1' )\n",
    "\n",
    "ngrams = ( 1, 2 )\n",
    "pos_counts = build_dict( pos_path, ngrams )\n",
    "neg_counts = build_dict( neg_path, ngrams ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict, r = compute_ratio( pos_counts, neg_counts, alpha = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "https://github.com/mesnilgr/nbsvm\n",
    "\n",
    "https://github.com/lrei/nbsvm\n",
    "\n",
    "https://github.com/vivekn/sentiment\n",
    "\n",
    "https://github.com/vsl9/Sentiment-Analysis-with-Convolutional-Networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
