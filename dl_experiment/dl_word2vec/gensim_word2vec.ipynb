{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "    \n",
       "    div.cell {\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 12pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "    }\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir('../../notebook_format')\n",
    "from formats import load_style\n",
    "load_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Ethen 2017-04-07 20:27:18 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 5.3.0\n",
      "\n",
      "numpy 1.12.1\n",
      "pandas 0.19.2\n",
      "gensim 1.0.1\n",
      "tqdm 4.11.2\n",
      "numba 0.31.0\n",
      "requests 2.13.0\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.figsize'] = 8, 6 # change default figure size\n",
    "\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import joblib\n",
    "from operator import itemgetter\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,gensim,sklearn,joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "\n",
    "`Word2Vec` is a unsupervised learning algorithm that uses a shallow neural network (with one hidden layer) to learn the vectorial representations of all the term words/phrases for a given corpus. The advantage that word2vec offers is tries to preserve the semantic meaning behind those terms. For example, a document may employ the words \"dog\" and \"canine\" to mean the same thing, but never use them together in a sentence. Ideally, the word2vec algorithm would be able to learn the context and place them together in similar vector semantic space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim’s `Word2vec` expects a sequence of sentences as its input, where each sentence a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "model = Word2Vec(sentences, min_count = 2, size = 200, workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model also accepts several key parameters that affect both training speed and quality.\n",
    "\n",
    "- `min_count`: For pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them. A reasonable value for min_count is between 0-100, depending on the size of the dataset.\n",
    "- `size`: Refers to the hidden layers size. Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds\n",
    "- `workers`: Number of cores/threads used for training\n",
    "- `window`: Only terms hat occur within a window-neighbourhood of a term, in a sentence, are associated with it during training. The usual value is 4. Unless your text contains big sentences, leave it at that.\n",
    "- `sg` – This defines the algorithm. If equal to 1, the skip-gram technique is used. Else, the CBoW method is employed\n",
    "\n",
    "The full list of parameters can be obtained [here](http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)\n",
    "\n",
    "---\n",
    "\n",
    "In the example above, keeping the input as a Python built-in list is convenient, but can use up a lot of RAM when the input is large.\n",
    "\n",
    "Gensim only requires that the input must provide sentences sequentially, hence if our input files are scattered across several different places then instead of loading everything into an in-memory list, we can process the input file by file, line by line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentences:\n",
    "    \"\"\"\n",
    "    iterate over files in a directory, and read in each line\n",
    "    as a list of words. Used with gensim's Word2Vec\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    dirname: str\n",
    "        directory that contains the file of text\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    # a memory-friendly iterator\n",
    "    dirname = 'test'\n",
    "    sentences = Sentences(dirname)\n",
    "    model = Word2Vec(sentences)\n",
    "    \"\"\"\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for file in os.listdir(self.dirname):\n",
    "            fname = os.path.join(self.dirname, file)\n",
    "            with open(fname) as f:\n",
    "                for line in f:\n",
    "                    # we can also do other text preprocessing\n",
    "                    # such as remove stop words, lower-case\n",
    "                    # the strings, etc. here\n",
    "                    yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset = 'train')\n",
    "newsgroups_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model_path = 'mymodel'\n",
    "\n",
    "workers = joblib.cpu_count()\n",
    "documents = [doc.strip().split() for doc in newsgroups_train.data]\n",
    "\n",
    "model = Word2Vec(documents, min_count = 2, size = 200, workers = workers)\n",
    "model.save(word2vec_model_path)\n",
    "model = Word2Vec.load(word2vec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118461, 200)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.114811</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.052656</td>\n",
       "      <td>0.040745</td>\n",
       "      <td>0.023850</td>\n",
       "      <td>-0.118442</td>\n",
       "      <td>0.119151</td>\n",
       "      <td>0.066576</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>-0.016908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044477</td>\n",
       "      <td>-0.011778</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.070038</td>\n",
       "      <td>-0.172101</td>\n",
       "      <td>0.089449</td>\n",
       "      <td>-0.018589</td>\n",
       "      <td>-0.010821</td>\n",
       "      <td>-0.104495</td>\n",
       "      <td>-0.055577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.057079</td>\n",
       "      <td>0.097144</td>\n",
       "      <td>0.017742</td>\n",
       "      <td>0.057017</td>\n",
       "      <td>0.011044</td>\n",
       "      <td>-0.136049</td>\n",
       "      <td>0.099916</td>\n",
       "      <td>-0.064588</td>\n",
       "      <td>-0.067024</td>\n",
       "      <td>0.004666</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091574</td>\n",
       "      <td>-0.021603</td>\n",
       "      <td>-0.000227</td>\n",
       "      <td>-0.008603</td>\n",
       "      <td>-0.033955</td>\n",
       "      <td>0.041081</td>\n",
       "      <td>-0.003849</td>\n",
       "      <td>0.083453</td>\n",
       "      <td>-0.118852</td>\n",
       "      <td>-0.063270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.015743</td>\n",
       "      <td>0.057191</td>\n",
       "      <td>-0.051213</td>\n",
       "      <td>0.092693</td>\n",
       "      <td>-0.098047</td>\n",
       "      <td>-0.018424</td>\n",
       "      <td>0.047705</td>\n",
       "      <td>0.018564</td>\n",
       "      <td>0.045533</td>\n",
       "      <td>-0.062696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.008302</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.057572</td>\n",
       "      <td>-0.080246</td>\n",
       "      <td>0.073219</td>\n",
       "      <td>0.082902</td>\n",
       "      <td>-0.070216</td>\n",
       "      <td>-0.046113</td>\n",
       "      <td>-0.053717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.133431</td>\n",
       "      <td>0.064562</td>\n",
       "      <td>0.143721</td>\n",
       "      <td>0.075947</td>\n",
       "      <td>-0.060838</td>\n",
       "      <td>-0.095630</td>\n",
       "      <td>0.044568</td>\n",
       "      <td>-0.082651</td>\n",
       "      <td>0.074516</td>\n",
       "      <td>0.056178</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009355</td>\n",
       "      <td>0.026335</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.022681</td>\n",
       "      <td>0.013456</td>\n",
       "      <td>-0.068525</td>\n",
       "      <td>-0.038995</td>\n",
       "      <td>0.065747</td>\n",
       "      <td>0.004777</td>\n",
       "      <td>-0.082451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.165271</td>\n",
       "      <td>0.184787</td>\n",
       "      <td>0.010603</td>\n",
       "      <td>0.068729</td>\n",
       "      <td>-0.006450</td>\n",
       "      <td>-0.031344</td>\n",
       "      <td>-0.060679</td>\n",
       "      <td>0.042055</td>\n",
       "      <td>-0.044052</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037406</td>\n",
       "      <td>0.010678</td>\n",
       "      <td>0.020201</td>\n",
       "      <td>-0.054917</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>0.118520</td>\n",
       "      <td>-0.019917</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>-0.051698</td>\n",
       "      <td>-0.028886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "the  0.114811  0.059105  0.052656  0.040745  0.023850 -0.118442  0.119151   \n",
       "to   0.057079  0.097144  0.017742  0.057017  0.011044 -0.136049  0.099916   \n",
       "of   0.015743  0.057191 -0.051213  0.092693 -0.098047 -0.018424  0.047705   \n",
       "a    0.133431  0.064562  0.143721  0.075947 -0.060838 -0.095630  0.044568   \n",
       "and  0.165271  0.184787  0.010603  0.068729 -0.006450 -0.031344 -0.060679   \n",
       "\n",
       "          7         8         9      ...          190       191       192  \\\n",
       "the  0.066576  0.063207 -0.016908    ...     0.044477 -0.011778  0.003200   \n",
       "to  -0.064588 -0.067024  0.004666    ...    -0.091574 -0.021603 -0.000227   \n",
       "of   0.018564  0.045533 -0.062696    ...     0.000653  0.008302  0.000931   \n",
       "a   -0.082651  0.074516  0.056178    ...    -0.009355  0.026335  0.000145   \n",
       "and  0.042055 -0.044052  0.037975    ...    -0.037406  0.010678  0.020201   \n",
       "\n",
       "          193       194       195       196       197       198       199  \n",
       "the  0.070038 -0.172101  0.089449 -0.018589 -0.010821 -0.104495 -0.055577  \n",
       "to  -0.008603 -0.033955  0.041081 -0.003849  0.083453 -0.118852 -0.063270  \n",
       "of   0.057572 -0.080246  0.073219  0.082902 -0.070216 -0.046113 -0.053717  \n",
       "a    0.022681  0.013456 -0.068525 -0.038995  0.065747  0.004777 -0.082451  \n",
       "and -0.054917  0.005005  0.118520 -0.019917  0.003818 -0.051698 -0.028886  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acess the vocabulary attribute of \n",
    "# the word vector to build a list of the \n",
    "# terms, integer indices and term counts from model's vocabulary\n",
    "ordered_vocab = [(term, info.index, info.count)\n",
    "                 for term, info in model.wv.vocab.items()]\n",
    "\n",
    "# sort by the term counts, so the most common terms appear first\n",
    "ordered_vocab = sorted(ordered_vocab, key = itemgetter(2), reverse = True)\n",
    "\n",
    "# unzip the terms, integer indices, and counts into separate lists\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "\n",
    "# create a DataFrame with the learnt vectors as data,\n",
    "# and the terms as row labels\n",
    "word_vectors = pd.DataFrame(model.wv.syn0norm[term_indices, :],\n",
    "                            index = ordered_terms)\n",
    "print(word_vectors.shape)\n",
    "word_vectors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use them for is to look up related words and phrases (words that have similar semantic meaning) for a given term of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('keyboard', 0.8721893429756165),\n",
       " ('application', 0.8527505993843079),\n",
       " ('manual', 0.8435473442077637),\n",
       " ('network', 0.8413074016571045),\n",
       " ('hardware', 0.8397271633148193)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the top 5 most similar term\n",
    "model.wv.most_similar(positive = ['computer'], topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Blog: Word2vec API Tutorial](http://rare-technologies.com/word2vec-tutorial/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
