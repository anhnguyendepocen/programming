{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "# glone the repo\n",
    "git clone https://github.com/alexarchambault/jupyter-scala.git\n",
    "\n",
    "# navigate to the directory\n",
    "cd jupyter-scala\n",
    "\n",
    "# run the installation script\n",
    "./jupyter-scala\n",
    "\n",
    "# check to ensure the kernel is now installed\n",
    "jupyter kernelspec list\n",
    "\n",
    "# run the jupyter notebook, we should be able to\n",
    "# see it from the dropdown menu\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "https://github.com/spark-notebook/spark-notebook/issues/541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd14.sc:17: not found: value edu\n",
      "import edu.umd.cloud9.collection.XMLInputFormat\n",
      "       ^cmd14.sc:20: not found: value edu\n",
      "import edu.umd.cloud9.collection.wikipedia.language._\n",
      "       ^cmd14.sc:21: not found: value edu\n",
      "import edu.umd.cloud9.collection.wikipedia._\n",
      "       ^"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "import $profile.`hadoop-2.7` // adjust hadoop version\n",
    "import $ivy.`org.apache.spark::spark-sql:2.2.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._\n",
    "\n",
    "val spark = JupyterSparkSession.\n",
    "    builder().  // important - call this rather than SparkSession.builder()\n",
    "    jupyter().  // this method must be called straightaway after builder()\n",
    "    appName(\"notebook\").\n",
    "    master(\"local[*]\").\n",
    "    config(\"spark.jars\", \"/Users/ethen/aas/ch06-lsa/target/ch06-lsa-2.0.0-jar-with-dependencies.jar\").  // https://stackoverflow.com/questions/41722993/spark-2-0-set-jars\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd14.sc:4: not found: value edu\n",
      "import edu.umd.cloud9.collection.wikipedia.language._\n",
      "       ^cmd14.sc:5: not found: value edu\n",
      "import edu.umd.cloud9.collection.wikipedia._\n",
      "       ^cmd14.sc:1: not found: value edu\n",
      "import edu.umd.cloud9.collection.XMLInputFormat\n",
      "       ^cmd14.sc:10: not found: value XMLInputFormat\n",
      "val res14_8 = conf.set(XMLInputFormat.START_TAG_KEY, \"<page>\")\n",
      "                       ^cmd14.sc:11: not found: value XMLInputFormat\n",
      "val res14_9 = conf.set(XMLInputFormat.END_TAG_KEY, \"</page>\")\n",
      "                       ^cmd14.sc:12: not found: type XMLInputFormat\n",
      "val kvs = spark.sparkContext.newAPIHadoopFile(path, classOf[XMLInputFormat],\n",
      "                                                            ^cmd14.sc:24: not found: type EnglishWikipediaPage\n",
      "    val page = new EnglishWikipediaPage()\n",
      "                   ^cmd14.sc:25: not found: value WikipediaPage\n",
      "    WikipediaPage.readPage(page, hackedPageXml)\n",
      "    ^"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "import edu.umd.cloud9.collection.XMLInputFormat\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.io._\n",
    "import edu.umd.cloud9.collection.wikipedia.language._\n",
    "import edu.umd.cloud9.collection.wikipedia._\n",
    "\n",
    "val basepath = \"/Users/ethen/programming/advanced_analytics_spark/ch06-lsa/\"\n",
    "val path = basepath + \"Wikipedia-20170920184300.xml\"\n",
    "@transient val conf = new Configuration()\n",
    "conf.set(XMLInputFormat.START_TAG_KEY, \"<page>\")\n",
    "conf.set(XMLInputFormat.END_TAG_KEY, \"</page>\")\n",
    "val kvs = spark.sparkContext.newAPIHadoopFile(path, classOf[XMLInputFormat],\n",
    "    classOf[LongWritable], classOf[Text], conf)\n",
    "val rawXmls = kvs.map(_._2.toString).toDS()\n",
    "\n",
    "\n",
    "def wikiXmlToPlainText(pageXml: String): Option[(String, String)] = {\n",
    "    // Wikipedia has updated their dumps slightly since Cloud9 was written,\n",
    "    // so this hacky replacement is sometimes required to get parsing to work.\n",
    "    val hackedPageXml = pageXml.replaceFirst(\n",
    "        \"<text xml:space=\\\"preserve\\\" bytes=\\\"\\\\d+\\\">\",\n",
    "        \"<text xml:space=\\\"preserve\\\">\")\n",
    "\n",
    "    val page = new EnglishWikipediaPage()\n",
    "    WikipediaPage.readPage(page, hackedPageXml)\n",
    "    if (page.isEmpty) None\n",
    "    else Some((page.getTitle, page.getContent))\n",
    "}\n",
    "\n",
    "val docTexts = rawXmls.filter(_ != null).flatMap(wikiXmlToPlainText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
