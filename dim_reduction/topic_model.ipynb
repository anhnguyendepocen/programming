{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Latent-Dirichlet-Allocation\" data-toc-modified-id=\"Latent-Dirichlet-Allocation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Latent Dirichlet Allocation</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2017-11-28 21:13:39 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 6.2.1\n",
      "\n",
      "numpy 1.13.3\n",
      "scipy 1.0.0\n",
      "pandas 0.20.3\n",
      "matplotlib 2.1.0\n",
      "sklearn 0.19.1\n"
     ]
    }
   ],
   "source": [
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,scipy,pandas,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space']\n",
    "\n",
    "dataset = fetch_20newsgroups(\n",
    "    subset = 'all', categories = categories,\n",
    "    shuffle = True, random_state = 42)\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "docs = [x.lower().split() for x in X]\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "# temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "# id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(corpus=corpus, id2word=dictionary, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -1.5282.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([(0.01171794697448247, 'or'),\n",
       "   (0.011116718234493452, 'can'),\n",
       "   (0.011097833130223802, 'would'),\n",
       "   (0.01097761960891468, 'as'),\n",
       "   (0.010938531529557802, 'what'),\n",
       "   (0.01089389961893316, 'they'),\n",
       "   (0.0097290204553938751, 'we'),\n",
       "   (0.0083837276268509901, 'do'),\n",
       "   (0.0078476279482066966, 'your'),\n",
       "   (0.0078375440167425953, 'there'),\n",
       "   (0.0078114036142310885, 'an'),\n",
       "   (0.0078099478829628055, \"don't\"),\n",
       "   (0.0072464807973690228, 'at'),\n",
       "   (0.0067497057260154742, 'so'),\n",
       "   (0.00671241233577668, 'my'),\n",
       "   (0.0066963885795746185, 'about'),\n",
       "   (0.0066581780756464837, 'just'),\n",
       "   (0.0064984385667549351, 'some'),\n",
       "   (0.0061233407229750398, 'think'),\n",
       "   (0.0060786441966135894, 'one')],\n",
       "  -0.7673092978049878),\n",
       " ([(0.014688841725788486, 'as'),\n",
       "   (0.014374182877034588, 'he'),\n",
       "   (0.013675221318565798, 'was'),\n",
       "   (0.010512103374662045, 'by'),\n",
       "   (0.0095215057077302698, 'or'),\n",
       "   (0.0093433809140619842, 'his'),\n",
       "   (0.0091696597160859494, 'they'),\n",
       "   (0.0089523995548556091, 'who'),\n",
       "   (0.0082982357380672785, 'from'),\n",
       "   (0.0071847044105997175, 'what'),\n",
       "   (0.0071659625415750805, 'your'),\n",
       "   (0.0071340493407982932, 'we'),\n",
       "   (0.0070886675940477128, 'god'),\n",
       "   (0.006946873381934446, 'one'),\n",
       "   (0.0069381874693686457, 'all'),\n",
       "   (0.0066312218858877725, 'an'),\n",
       "   (0.0065375889406452071, 'will'),\n",
       "   (0.0064542555174416346, 'my'),\n",
       "   (0.0061084908613869161, 'do'),\n",
       "   (0.0057677799683691971, 'there')],\n",
       "  -0.78119619457922729),\n",
       " ([(0.18606457159036802, '>'),\n",
       "   (0.025803699127924493, '>>'),\n",
       "   (0.019012826363611412, 'article'),\n",
       "   (0.011971780002027902, '*'),\n",
       "   (0.0094190686166343036, '--'),\n",
       "   (0.0086344097101372329, 'was'),\n",
       "   (0.0071561424108146946, 'what'),\n",
       "   (0.0070226385658931356, 'they'),\n",
       "   (0.0060937593487239471, '>|>'),\n",
       "   (0.0057675205504318476, 'just'),\n",
       "   (0.0056393394442874403, 'all'),\n",
       "   (0.0054050267567796748, 'so'),\n",
       "   (0.0052592381069203305, 'would'),\n",
       "   (0.004754003662260896, 'about'),\n",
       "   (0.0046955144113854895, 'as'),\n",
       "   (0.0046578211583107685, 'there'),\n",
       "   (0.0045562110691349522, 'do'),\n",
       "   (0.0045104908701861608, 'know'),\n",
       "   (0.0043482739973107008, 'at'),\n",
       "   (0.0042930703078321803, 'by')],\n",
       "  -0.90183667588044258),\n",
       " ([(0.11441399853091133, '|>'),\n",
       "   (0.011682095630947503, 'article'),\n",
       "   (0.010450974797307553, 'nntp-posting-host:'),\n",
       "   (0.010091701359944114, 'we'),\n",
       "   (0.009984100654354169, 'what'),\n",
       "   (0.0094767249430007805, 'an'),\n",
       "   (0.008957823001926846, '(keith'),\n",
       "   (0.0079239306505547343, 'do'),\n",
       "   (0.007614889832194883, 'moral'),\n",
       "   (0.00746672979828031, \"don't\"),\n",
       "   (0.0068842770769563548, '(jon'),\n",
       "   (0.0067928806271432924, 'think'),\n",
       "   (0.0067712638974751632, 'they'),\n",
       "   (0.0066642899237651749, 'livesey)'),\n",
       "   (0.006659570649877238, 'livesey@solntze.wpd.sgi.com'),\n",
       "   (0.0065841535970353792, 'morality'),\n",
       "   (0.0064579344284640379, 'as'),\n",
       "   (0.0064050240786777636, 'or'),\n",
       "   (0.0063876087930533993, 'one'),\n",
       "   (0.0060530050239146873, 'can')],\n",
       "  -1.2570084859941784),\n",
       " ([(0.01466827237968543, 'any'),\n",
       "   (0.014392421916501259, 'university'),\n",
       "   (0.013792607987354979, 'nntp-posting-host:'),\n",
       "   (0.011575394405046551, '--'),\n",
       "   (0.011135024214929458, 'can'),\n",
       "   (0.010784455256792504, 'would'),\n",
       "   (0.010769409485000395, 'me'),\n",
       "   (0.010624735720227088, 'or'),\n",
       "   (0.010450740877575047, 'am'),\n",
       "   (0.010291478708604588, 'anyone'),\n",
       "   (0.0094034622805953833, 'there'),\n",
       "   (0.0091747580455985491, 'know'),\n",
       "   (0.0082663796444765666, \"i'm\"),\n",
       "   (0.0080797531135089733, 'looking'),\n",
       "   (0.0080323429826474682, 'graphics'),\n",
       "   (0.0076203829685111257, 'please'),\n",
       "   (0.0075972800680955948, 'need'),\n",
       "   (0.0075268328168982658, 'my'),\n",
       "   (0.0069227941520287667, 'some'),\n",
       "   (0.006918873784180159, 'like')],\n",
       "  -1.2947008953176042),\n",
       " ([(0.022127122602502842, 'image'),\n",
       "   (0.018228773435342852, 'jpeg'),\n",
       "   (0.01567061938959265, 'file'),\n",
       "   (0.012680258374385774, 'can'),\n",
       "   (0.012556620453040658, 'from'),\n",
       "   (0.012381301020554715, 'as'),\n",
       "   (0.012258288923449337, 'or'),\n",
       "   (0.010078117968357775, 'images'),\n",
       "   (0.0096500936868966838, 'gif'),\n",
       "   (0.0092730295337469879, 'will'),\n",
       "   (0.0081173475041555476, 'color'),\n",
       "   (0.008048678349523445, 'files'),\n",
       "   (0.0078830235302414595, 'use'),\n",
       "   (0.0072660819442779048, 'an'),\n",
       "   (0.0072631327650442019, 'by'),\n",
       "   (0.0071496743942047539, 'version'),\n",
       "   (0.006941398986525231, 'than'),\n",
       "   (0.0068850634436556906, 'format'),\n",
       "   (0.0064609875517149303, 'some'),\n",
       "   (0.0062599994787362421, 'display')],\n",
       "  -1.4862595869603799),\n",
       " ([(0.021377852498001202, '-'),\n",
       "   (0.013193448647629079, 'or'),\n",
       "   (0.011284596419106053, 'from'),\n",
       "   (0.010229743758682339, 'by'),\n",
       "   (0.0099465811256457086, 'an'),\n",
       "   (0.0086803501151732822, 'as'),\n",
       "   (0.0080680824068312638, 'at'),\n",
       "   (0.0078983741494288648, 'can'),\n",
       "   (0.0069486719610286169, 'data'),\n",
       "   (0.0064641617131450781, 'will'),\n",
       "   (0.005930708204999276, 'other'),\n",
       "   (0.0057588746032850094, 'also'),\n",
       "   (0.0055763168825038014, 'which'),\n",
       "   (0.0055641643504970714, 'available'),\n",
       "   (0.0054265456994523134, 'graphics'),\n",
       "   (0.0053913104474196446, 'has'),\n",
       "   (0.0049820093551041335, 'information'),\n",
       "   (0.004746899890292677, 'software'),\n",
       "   (0.004702219673484615, 'image'),\n",
       "   (0.004534703940140189, 'computer')],\n",
       "  -1.540749459886035),\n",
       " ([(0.026504360549253331, 'space'),\n",
       "   (0.013206940229625323, 'was'),\n",
       "   (0.011753967755337812, 'as'),\n",
       "   (0.011020636403384999, 'at'),\n",
       "   (0.010476564539455297, 'from'),\n",
       "   (0.0095128063829940691, 'by'),\n",
       "   (0.0086749438774294425, '-'),\n",
       "   (0.0085721686600306966, 'will'),\n",
       "   (0.0081148738969121035, 'launch'),\n",
       "   (0.0070071821385134427, 'nasa'),\n",
       "   (0.0069669036665882802, 'shuttle'),\n",
       "   (0.0062239558937764068, '--'),\n",
       "   (0.0056526536119205339, 'would'),\n",
       "   (0.0049611323700992803, 'solar'),\n",
       "   (0.0049137983268047174, 'about'),\n",
       "   (0.0048934999297302389, 'mission'),\n",
       "   (0.0048697355934554385, 'first'),\n",
       "   (0.0048425677313171343, 'were'),\n",
       "   (0.0048132692714194028, 'orbit'),\n",
       "   (0.0046706442707209585, 'satellite')],\n",
       "  -1.6787265861534857),\n",
       " ([(0.091313229441072494, ':'),\n",
       "   (0.031003187640250018, '='),\n",
       "   (0.011251901056570836, 'x-newsreader:'),\n",
       "   (0.011232911240957487, 'tin'),\n",
       "   (0.011100367916942157, '+'),\n",
       "   (0.010518732692095681, '--'),\n",
       "   (0.010509461855424064, '1.1'),\n",
       "   (0.009322532840953527, '[version'),\n",
       "   (0.0086312330829591138, 'wrote:'),\n",
       "   (0.0070992520749508809, \"don't\"),\n",
       "   (0.0062423198141621991, 'or'),\n",
       "   (0.0061234503308411053, 'as'),\n",
       "   (0.0058263871444888824, '>:'),\n",
       "   (0.0058136656332263177, 'what'),\n",
       "   (0.0054485957330982207, 'just'),\n",
       "   (0.0053438769734643768, '}'),\n",
       "   (0.0050341762186759172, 'do'),\n",
       "   (0.0048875417771807195, '(bill'),\n",
       "   (0.0048117388428012752, 'can'),\n",
       "   (0.0047639194083203681, '-')],\n",
       "  -1.7506276995760297),\n",
       " ([(0.20239280076552976, '|'),\n",
       "   (0.025933241566372219, '/'),\n",
       "   (0.011944392251561972, '--'),\n",
       "   (0.01143248894510388, 'o'),\n",
       "   (0.010285079945779705, 'nntp-posting-host:'),\n",
       "   (0.010035234164832164, '\\\\'),\n",
       "   (0.0080421446018981487, '-'),\n",
       "   (0.0069206610500184432, 'usa'),\n",
       "   (0.006705618910049946, 'access'),\n",
       "   (0.0053511693676999367, '='),\n",
       "   (0.0052334996303553755, 'article'),\n",
       "   (0.0049788630455262434, 'express'),\n",
       "   (0.0048892063058575286, 'online'),\n",
       "   (0.0048442048334539373, 'distribution:'),\n",
       "   (0.0048123115331748121, '(pat)'),\n",
       "   (0.0046943168194655913, 'access.digex.net'),\n",
       "   (0.0046720522152578787, 'p'),\n",
       "   (0.0045811140994294012, 'would'),\n",
       "   (0.0045679909252106122, 'pat'),\n",
       "   (0.0045212583122273328, '*')],\n",
       "  -3.8236237678786624)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus, topn=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "top_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i was wondering if anyone out there could enlighten me on this car i saw\\nthe other day. it was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. it was called a bricklin. the doors were really small. in addition,\\nthe front bumper was separate from the rest of the body. this is \\nall i know. if anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# we only want to keep the body of the documents!\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "# fetch train and test data\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=remove)\n",
    "\n",
    "# a list of 18,846 cleaned news in string format\n",
    "# only keep letters & make them all lower case\n",
    "news = [raw.lower() for raw in\n",
    "        newsgroups_train.data + newsgroups_test.data]\n",
    "news[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethen/anaconda/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 5\n",
      "iteration: 2 of max_iter: 5\n",
      "iteration: 3 of max_iter: 5\n",
      "iteration: 4 of max_iter: 5\n",
      "iteration: 5 of max_iter: 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_topics = 20 # number of topics\n",
    "n_iter = 5 # number of iterations\n",
    "\n",
    "# vectorizer: ignore English stopwords & words that occur less than 5 times\n",
    "cvectorizer = CountVectorizer(min_df=5, stop_words='english')\n",
    "X_train = cvectorizer.fit_transform(news)\n",
    "\n",
    "# train an LDA model\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=n_iter, verbose=1, n_jobs=-1)\n",
    "X_topics = lda.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(429, 20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.8\n",
    "_idx = np.amax(X_topics, axis = 1) > threshold  # idx of doc that above the threshold\n",
    "X_topics_subset = X_topics[_idx]\n",
    "topic = np.argmax(X_topics_subset, axis = 1)\n",
    "X_topics_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# export the item factors/embeddings and the corresponding metadata into .tsv format\n",
    "# here we can more than 1 column as the metadata, thus we can include a header, but\n",
    "# if we were to only have 1 column as the metadata, then no header is allowed in tensorboard\n",
    "# tf_embedding_dir = 'tf_embedding'\n",
    "# if not os.path.isdir(tf_embedding_dir):\n",
    "#     os.mkdir(tf_embedding_dir)\n",
    "    \n",
    "# metadata_file = os.path.join(tf_embedding_dir, 'metadata.tsv')\n",
    "# embedding_file = os.path.join(tf_embedding_dir, 'topics.tsv')\n",
    "metadata_file = 'metadata.tsv'\n",
    "embedding_file = 'topics.tsv'\n",
    "\n",
    "np.savetxt(metadata_file, topic, delimiter = '\\t')\n",
    "# df_diag_desc[['friendly_desc', 'cluster']].to_csv(metadata_file, index = False, sep = '\\t')\n",
    "np.savetxt(embedding_file, X_topics_subset, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../topics.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7beadc91c6f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0membedding_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     metadata_file = os.path.join('..', metadata_file))\n\u001b[0m",
      "\u001b[0;32m~/programming/dim_reduction/tf_utils.py\u001b[0m in \u001b[0;36mlaunch_tensorboard\u001b[0;34m(embedding_file, log_dir, metadata_file)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0membedding_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows)\u001b[0m\n\u001b[1;32m   1549\u001b[0m                 \u001b[0mfhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rbU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m                 \u001b[0mfhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m             \u001b[0mown_fhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bz2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_file_openers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../topics.tsv'"
     ]
    }
   ],
   "source": [
    "from tf_utils import launch_tensorboard\n",
    "\n",
    "log_dir = './logs/' # os.path.join(tf_embedding_dir, './logs/')\n",
    "launch_tensorboard(\n",
    "    embedding_file = os.path.join('..', embedding_file),\n",
    "    log_dir = log_dir,\n",
    "    metadata_file = os.path.join('..', metadata_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 18846 samples in 0.019s...\n",
      "[t-SNE] Computed neighbors for 18846 samples in 9.358s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 18846 / 18846\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 90.440552\n",
      "[t-SNE] Error after 1000 iterations: 2.128333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# a t-SNE model\n",
    "# angle value close to 1 means sacrificing accuracy for speed\n",
    "# pca initializtion usually leads to better results \n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "\n",
    "# 20-D -> 2-D\n",
    "tsne_lda = tsne_model.fit_transform(X_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export the item factors/embeddings and the corresponding metadata into .tsv format\n",
    "# here we can more than 1 column as the metadata, thus we can include a header, but\n",
    "# if we were to only have 1 column as the metadata, then no header is allowed in tensorboard\n",
    "tf_embedding_dir = 'tf_embedding'\n",
    "if not os.path.isdir(tf_embedding_dir):\n",
    "    os.mkdir(tf_embedding_dir)\n",
    "    \n",
    "metadata_file = os.path.join(tf_embedding_dir, 'metadata.tsv')\n",
    "embedding_file = os.path.join(tf_embedding_dir, 'item_factors.tsv')\n",
    "\n",
    "df_diag_desc[['friendly_desc', 'cluster']].to_csv(metadata_file, index = False, sep = '\\t')\n",
    "np.savetxt(embedding_file, als.item_factors_, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
