{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2018-02-17 11:45:58 \n",
      "\n",
      "CPython 3.6.3\n",
      "IPython 6.1.0\n",
      "\n",
      "numpy 1.14.0\n",
      "pandas 0.22.0\n",
      "sklearn 0.19.1\n",
      "matplotlib 2.1.0\n"
     ]
    }
   ],
   "source": [
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# change default style figure and font size\n",
    "plt.rcParams['figure.figsize'] = 8, 6\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,sklearn,matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Entropy** is measuring the average amount of information needed to communicate something. If we knew for sure what was going to happen, we wouldn't have to send any messages at all, on the other hand if there's two things that could happen with a 50/50 probability, we would need to send 1 bit of message to communicate that. Hence, the more concentrated the probability, the more we can craft a clever code with shorter average message length. Another way of viewing entropy, is that it measures the variation in the data, the larger the entropy the greater the variation we have in our data.\n",
    "- **Cross Entropy** Measures the average length of communicating an event from one distribution with the optimal code for another distribution.\n",
    "- **Kullbackâ€“Leibler divergence** or commonly just referred to as KL divergence measures the difference between cross entropy and entropy. This difference measures how much longer our messages are going to be because we used a code that's optimized for a different distribution. If the distributions are the same, the difference will be zero. On the other hand, the it will become bigger if the distribution's difference grows.\n",
    "\n",
    "Cross entropy and KL divergence are incredibly useful in machine learning. Often, we want one distribution to be close to another. For example, we might want a predicted distribution to be close to the ground truth. Cross entropy and KL divergence gives us a natural way to do this, that's why we see it showing up everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset = 'train')\n",
    "newsgroups_test = fetch_20newsgroups(subset = 'test')\n",
    "X_train = newsgroups_train.data\n",
    "y_train = newsgroups_train.target\n",
    "X_test = newsgroups_test.data\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9728654764009192"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "logistic = LogisticRegression(class_weight = 'balanced')\n",
    "pipeline1 = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('logistic', logistic)\n",
    "]).fit(X_train, y_train)\n",
    "\n",
    "pipeline_pred_train = pipeline1.predict(X_train)\n",
    "pipeline_pred_test = pipeline1.predict(X_test)\n",
    "accuracy_score(y_train, pipeline_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8271375464684015"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pipeline_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-291924ebd3ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mpipeline_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpipeline_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_pred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous targets"
     ]
    }
   ],
   "source": [
    "from kaggler.online_model import FTRL\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "logistic = FTRL(a=.1,                # alpha in the per-coordinate rate\n",
    "           b=1,                 # beta in the per-coordinate rate\n",
    "           l1=1.,               # L1 regularization parameter\n",
    "           l2=1.,               # L2 regularization parameter\n",
    "           n=2**20,             # number of hashed features\n",
    "           epoch=1,             # number of epochs\n",
    "           interaction=True)\n",
    "pipeline3 = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('logistic', logistic)\n",
    "]).fit(X_train, y_train)\n",
    "\n",
    "pipeline_pred_train = pipeline3.predict(X_train)\n",
    "pipeline_pred_test = pipeline3.predict(X_test)\n",
    "accuracy_score(y_train, pipeline_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pipeline_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9052501325791056"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "feature_selection = SelectKBest(chi2, k=5000)\n",
    "logistic = LogisticRegression(class_weight = 'balanced')\n",
    "pipeline2 = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('logistic', logistic)\n",
    "]).fit(X_train, y_train)\n",
    "\n",
    "pipeline_pred_train = pipeline2.predict(X_train)\n",
    "pipeline_pred_test = pipeline2.predict(X_test)\n",
    "accuracy_score(y_train, pipeline_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7952734997344663"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pipeline_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9728654764009192"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "elastic_net = ElasticNet(l1_ratio = 0.8)\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_selection = SelectFromModel(elastic_net, threshold = 'mean', prefit = False)\n",
    "logistic = LogisticRegression(class_weight = 'balanced')\n",
    "pipeline3 = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('logistic', logistic)\n",
    "]).fit(X_train, y_train)\n",
    "\n",
    "pipeline_pred_train = pipeline3.predict(X_train)\n",
    "pipeline_pred_test = pipeline3.predict(X_test)\n",
    "accuracy_score(y_train, pipeline_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8271375464684015"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pipeline_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer()\n",
    "# feature_selection = SelectKBest(mutual_info_classif, k=5000)\n",
    "# logistic = LogisticRegression(class_weight = 'balanced')\n",
    "# pipeline2 = Pipeline([\n",
    "#     ('tfidf', tfidf),\n",
    "#     ('feature_selection', feature_selection),\n",
    "#     ('logistic', logistic)\n",
    "# ]).fit(X_train, y_train)\n",
    "\n",
    "# pipeline_pred_train = pipeline2.predict(X_train)\n",
    "# pipeline_pred_test = pipeline2.predict(X_test)\n",
    "# accuracy_score(y_train, pipeline_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy_score(y_test, pipeline_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'A'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-00da26fc96a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'feature_selection'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_selector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'logistic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m ]).fit(X_train.A, y_train)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpipeline_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'A'"
     ]
    }
   ],
   "source": [
    "import mifs\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "feat_selector = mifs.MutualInformationFeatureSelector()\n",
    "pipeline3 = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('feature_selection', feat_selector),\n",
    "    ('logistic', logistic)\n",
    "]).fit(X_train, y_train)\n",
    "\n",
    "pipeline_pred_train = pipeline3.predict(X_train)\n",
    "pipeline_pred_test = pipeline3.predict(X_test)\n",
    "accuracy_score(y_train, pipeline_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pipeline_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from itertools import chain\n",
    "from nltk.corpus import brown\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentences = brown.sents(categories='news')\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(chain(*sentences))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/20491028/optimal-way-to-compute-pairwise-mutual-information-using-numpy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_MI(X,Y,bins):\n",
    "    c_XY = np.histogram2d(X,Y,bins)[0]\n",
    "    c_X = np.histogram(X,bins)[0]\n",
    "    c_Y = np.histogram(Y,bins)[0]\n",
    "\n",
    "    H_X = shan_entropy(c_X)\n",
    "    H_Y = shan_entropy(c_Y)\n",
    "    H_XY = shan_entropy(c_XY)\n",
    "\n",
    "    MI = H_X + H_Y - H_XY\n",
    "    return MI\n",
    "\n",
    "\n",
    "def shan_entropy(c):\n",
    "    c_normalized = c / float(np.sum(c))\n",
    "    c_normalized = c_normalized[np.nonzero(c_normalized)]\n",
    "    H = -sum(c_normalized* np.log2(c_normalized))  \n",
    "    return H\n",
    "\n",
    "A = np.array([[ 2.0,  140.0,  128.23, -150.5, -5.4  ],\n",
    "              [ 2.4,  153.11, 130.34, -130.1, -9.5  ],\n",
    "              [ 1.2,  156.9,  120.11, -110.45,-1.12 ]])\n",
    "\n",
    "bins = 5 # ?\n",
    "n = A.shape[1]\n",
    "matMI = np.zeros((n, n))\n",
    "\n",
    "for ix in np.arange(n):\n",
    "    for jx in np.arange(ix+1,n):\n",
    "        matMI[ix,jx] = calc_MI(A[:,ix], A[:,jx], bins)\n",
    "        \n",
    "matMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mutual_info_score(A[:, 0], A[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def calc_MI(x, y, bins):\n",
    "    c_xy = np.histogram2d(x, y, bins)[0]\n",
    "    mi = mutual_info_score(None, None, contingency=c_xy)\n",
    "    return mi\n",
    "\n",
    "\n",
    "bins = 5 # ?\n",
    "n = A.shape[1]\n",
    "matMI = np.zeros((n, n))\n",
    "\n",
    "for ix in np.arange(n):\n",
    "    for jx in np.arange(ix+1,n):\n",
    "        matMI[ix,jx] = calc_MI(A[:,ix], A[:,jx], bins)\n",
    "        \n",
    "matMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X.shape\n",
    "\n",
    "feature_selection = SelectKBest(chi2, k=2)\n",
    "X_new = feature_selection.fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457 Âµs Â± 38.2 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "feature_selection = SelectKBest(chi2, k=2)\n",
    "X_new = feature_selection.fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.81782088,   3.59449902, 116.16984746,  67.24482759])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.48 ms Â± 848 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "feature_selection = SelectKBest(mutual_info_classif, k=2)\n",
    "X_new = feature_selection.fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49593281, 0.25590719, 0.98758062, 0.97811196])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mifs\n",
    "feat_selector = mifs.MutualInformationFeatureSelector(n_features = 2)\n",
    "X_new = feat_selector.fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.2 ms Â± 4.84 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "feat_selector = mifs.MutualInformationFeatureSelector(n_features = 2)\n",
    "X_new = feat_selector.fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mutual_info_score(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [Blog: Visual Information Theory](http://colah.github.io/posts/2015-09-Visual-Information/)\n",
    "- http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = 'https://raw.githubusercontent.com/h2oai/app-consumer-loan/master/data/loan.csv'\n",
    "data = pd.read_csv(filepath)\n",
    "data = data.dropna(how = 'any')\n",
    "print('dimension:', data.shape)\n",
    "data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.hist(bins = 50, figsize = (20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_col = 'bad_loan'\n",
    "num_cols = [\n",
    "    'loan_amnt', 'int_rate', 'emp_length',\n",
    "    'annual_inc', 'dti', 'delinq_2yrs',\n",
    "    'revol_util', 'total_acc', 'longest_credit_length']\n",
    "cat_cols = [\n",
    "    'term', 'home_ownership', 'purpose',\n",
    "    'addr_state', 'verification_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract target variable, perform\n",
    "# a quick check of the target variable's skewness\n",
    "label = data[label_col].values\n",
    "data = data.drop(label_col, axis = 1)\n",
    "print('labels distribution:', np.bincount(label) / label.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train/validation stratified split\n",
    "val_size = 0.1\n",
    "test_size = 0.1\n",
    "split_random_state = 1234\n",
    "df_train, df_test, y_train, y_test = train_test_split(\n",
    "    data, label, test_size = test_size,\n",
    "    random_state = split_random_state, stratify = label)\n",
    "\n",
    "df_train, df_val, y_train, y_val = train_test_split(\n",
    "    df_train, y_train, test_size = val_size,\n",
    "    random_state = split_random_state, stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from mlutils.transformers import Preprocessor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "feature_selection = SelectKBest(chi2, k=6)\n",
    "preprocessor = Preprocessor(num_cols = num_cols, cat_cols = cat_cols)\n",
    "rf = RandomForestClassifier(class_weight = 'balanced')\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('rf', rf)\n",
    "]).fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from skrebate import ReliefF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "genetic_data = pd.read_csv('https://github.com/EpistasisLab/scikit-rebate/raw/master/data/'\n",
    "                           'GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz',\n",
    "                           sep='\\t', compression='gzip')\n",
    "\n",
    "features, labels = genetic_data.drop('class', axis=1), genetic_data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = make_pipeline(ReliefF(n_features_to_select=2, n_neighbors=100),\n",
    "                    RandomForestClassifier(n_estimators=100))\n",
    "\n",
    "print(np.mean(cross_val_score(clf, features, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "print(np.mean(cross_val_score(clf, features, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "clf = make_pipeline(BoostARoota(clf = rf, metric='logloss'),\n",
    "                    RandomForestClassifier(n_estimators=100))\n",
    "\n",
    "print(np.mean(cross_val_score(clf, features, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from boostaroota import BoostARoota\n",
    "import urllib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "#################\n",
    "#Madelon Dataset\n",
    "train_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/madelon/MADELON/madelon_train.data'\n",
    "# download the file\n",
    "raw_data = urllib.request.urlopen(train_url)\n",
    "train = pd.read_csv(raw_data, delim_whitespace=True, header=None)\n",
    "train.columns = [\"Var\"+str(x) for x in range(len(train.columns))]\n",
    "labels_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/madelon/MADELON/madelon_train.labels'\n",
    "raw_data = urllib.request.urlopen(labels_url)\n",
    "labels = pd.read_csv(raw_data, delimiter=\",\", header=None)\n",
    "labels.columns = [\"Y\"]\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "#\n",
    "#  Test that BoostARoota is working\n",
    "#\n",
    "########################################################################################################################\n",
    "br = BoostARoota(clf = clf, metric='logloss')\n",
    "\n",
    "br.fit(train,labels)\n",
    "len(train.columns)\n",
    "len(br.keep_vars_)\n",
    "new_train = br.transform(train)\n",
    "new_train2 = br.fit_transform(train,labels)\n",
    "\n",
    "\n",
    "#Dimension Reduction\n",
    "print(\"Original training set has \" + str(train.shape) + \" dimensions. \\n\" +\\\n",
    "\"BoostARoota with .fit() and .transform() reduces to \" + str(new_train.shape) + \" dimensions. \\n\" +\\\n",
    "\"BoostARoota with .fit_transform() reduces to \" + str(new_train2.shape) + \" dimensions.\\n\" +\\\n",
    "\"The two methods may give a slightly different dimensions because of random variation as it is being refit\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
