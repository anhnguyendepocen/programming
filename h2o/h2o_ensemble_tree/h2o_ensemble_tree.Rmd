---
title: "H2o Ensemble Tree"
author: "Ethen Liu"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: pygments
---

<style type="text/css">
p { /* Normal  */
   font-size: 18px;
}
body { /* Normal  */
   font-size: 18px;
}
td {  /* Table  */
   font-size: 14px;
}
h1 { /* Header 1 */
 font-size: 32px;
}
h2 { /* Header 2 */
 font-size: 26px;
}
h3 { /* Header 3 */
 font-size: 22px;
}
code.r { /* Code block */
  font-size: 14px;
}
pre { /* Code block */
  font-size: 14px
}
</style>

> [R code](https://github.com/ethen8181/machine-learning/blob/master/h2o/h2o_ensemble_tree/h2o_ensemble_tree.R) to the documentation for those that wish to follow along.

# Basic Background Information

Decision Tree's strength: 

- Capture non-linear relationship. Lots of real world data contains non-linear relationship. e.g. For the titanic survival data, maybe younger and older people have similar rate of surviving and middle age people don't.
- Robust to correlated features, feature distribution, missing values.

With that being said it's performance is usually not very top-tiered. So there're two different categories of solutions for this problem, one is through bagging (Random Forest) and the second is through boosting (Gradient Boosting Machine).

**Random Forest:**

- Each tree is built on a sample of the features and on a sample of the observations (to increase the variance of the trees). Trees are independent with one another.
- It is easy to use since it has very few hyperparameters to tune. And it runs pretty well with the default parameters.
- Though it is slow when it comes to scoring.

**Gradient Boosting Machine:**

- Boosting algorithm iteratively learns weak classifiers and adds them to a final strong classifier. After a weak learner is added, the data is reweighted: examples that are misclassified gain weight and examples that are classified correctly lose weight. Thus, future weak learners focus more on the examples that previous weak learners misclassified. Trees are dependent with one another causing it to be not very robust to noisy data and outliers.
- Often times it is the best single model, though you need to find the proper stopping point to prevent overfitting.

One thing worth noticing is that Random Forest will most likely not be the best choice for the very unbalanced class situation because of the way it works. Since if one of the output label is rare, each tree might not contain enough sampled observation of that output to train. As for boosting methods since it increase its focus on badly learned observations and therefore should be able to catch the characteristics of the rare observations. 

# H2o Hands On

```{r, message=FALSE, warning=FALSE}

library(h2o)
setwd("/Users/ethen/machine-learning/h2o")

# -1: use all available threads and allocate memory to the cluster,
# the cluster size should be about 4 times larger than your dataset 
h2o.init(nthreads = -1, max_mem_size = "2G")

# disable progress bar so it doesn't clutter up the document
h2o.no_progress()

```

After loading in the data, we'll perform train/test split. We create splits of 60% and 20%. H2O will create one more split of 1 - (sum of these parameters), so we will get 0.6 / 0.2 / 1 - (0.6 + 0.2) = 0.6 / 0.2 / 0.2, setting a seed will ensure reproducible results (not R's seed).

```{r}

# import the file and perform train/test split
df <- h2o.importFile(path = "covtype.full.csv")
splits <- h2o.splitFrame( df, c(0.6, 0.2), seed = 1234 )

# assign a h2o id name train to the training data 
train <- h2o.assign( splits[[1]], "train" )  
valid <- h2o.assign( splits[[2]], "valid" )
test  <- h2o.assign( splits[[3]], "test" )

# use a subset of the training data for demonstration speed
train <- train[1:100000, ]

```

## Random Forest 

Run our first randomforest model

- `x` The input columns. You can also specify it by column index or by passing in strings of characters
- `y` The target column
- `model_id` Name the model in H2O (optional)
- `ntrees` Use a maximum of 200 trees to create the model, default = 50
- `stopping_rounds` Set this so that there will be a early stopping criteria. It's essentially telling the model to stop when it's sufficiently accurate. To elaborate, it will stop fitting new trees when the n-tree's average is within 0.001 (default) of the prior n-tree's averages. We can change the 0.001 with the `stopping_tolerance` parameter. And change the evaluation metric with `stopping_metric`

```{r}

# run our first predictive model
rf1 <- h2o.randomForest(
	training_frame = train,
	validation_frame = valid,
	x = 1:12, 
	y = 13, 
	model_id = "rf_covType_v1",
	ntrees = 200,
	stopping_rounds = 2,
	seed = 1000000
)

```

Some observations of the model's output

```{r}

# hit ratio table tells you if you give the model n number of shots at guessing the output
# variable's class, how likely is it going to get it correct. Thus, 
# the first row of the hit_ratop table is basically the accuracy of the classification
h2o.hit_ratio_table(rf1, valid = TRUE)[1, 2]

# the variable importance shows you that about 52 percent of the model 
# is captured by Elevation and Soil_Type 
h2o.varimp(rf1)

```

## Gradient Boosting Machine

We'll first use all default settings and then make some changes.

```{r}

# Use all default settings and then make some changes.
gbm1 <- h2o.gbm(
	training_frame = train,
	validation_frame = valid,
	x = 1:12,
	y = 13,
	model_id = "gbm_covType1",
	seed = 2000000
)
h2o.hit_ratio_table(gbm1, valid = TRUE)[1, 2]
```

This default GBM is much worse than our original random forest because it's is far from converging and there are three primary knobs to adjust.

1. `ntrees` Adding trees will help. The default is 50.
2. `learn_rate` Increasing the learning rate will also help. The contribution of each tree will be stronger, so the model will move further away from the overall mean.
3. `max_depth` Increasing the depth will help. Adding depth makes each tree fit the data closer. 

```{r}

gbm2 <- h2o.gbm(
	training_frame = train,
	validation_frame = valid,
	model_id = "gbm_covType2",
	x = 1:12,
	y = 13,
	ntrees = 20, 
	learn_rate = 0.2, # increase the learning rate (from 0.1)
	max_depth = 10, # increase the depth (from 5)
	stopping_rounds = 2, 
	stopping_tolerance = 0.01, 
	seed = 2000000
)

# review the new model's accuracy
h2o.hit_ratio_table(gbm2, valid = TRUE)[1, 2]

```

So even though we ran fewer trees, you can see that by adding the depth, making each tree have a greater impact gave us a net gain in the overall accuracy.

This has moved us in the right direction, but still lower accuracy than the random forest and it still has not converged, so we can make it more aggressive. We can now add some of the nature of random forest into the GBM using some of the new settings. This will help generalize the model's performance.

```{r}

gbm3 <- h2o.gbm(
	training_frame = train,
	validation_frame = valid,
	x = 1:12,
	y = 13,
	ntrees = 30, # add a few trees (from 20, though default is 50)
	learn_rate = 0.3, # increase the learning rate even further
	max_depth = 10,
	sample_rate = 0.7, # use a random 70% of the rows to fit each tree
	col_sample_rate = 0.7, # use 70% of the columns to fit each tree
	stopping_rounds = 2,
	stopping_tolerance = 0.01,
	model_id = "gbm_covType3",
	seed = 2000000
)

# review the newest model's accuracy
h2o.hit_ratio_table(gbm3, valid = TRUE )[1, 2]
```

Now the GBM is close to the initial random forest. However, we used a default random forest. And while there are only a few parameters to tune, we can experiment with those to see if it will make a difference. The main parameters to tune are the tree depth and the mtries, which is the number of predictors to use. The default depth of trees is 20. Note that the default mtries depends on whether classification or regression is being run. The default for classification is one-third of the columns, while the default for regression is the square root of the number of columns.

```{r}

rf2 <- h2o.randomForest( 
	training_frame = train,
	validation_frame = valid,
	x = 1:12,
	y = 13,
	model_id = "rf_covType2",
	ntrees = 200,
	max_depth = 30, # Increase depth, from 20
	stopping_rounds = 2,
	stopping_tolerance = 1e-2,
	score_each_iteration = TRUE,
	seed = 3000000 
)

# newest random forest accuracy
h2o.hit_ratio_table(rf2, valid = TRUE)[1, 2]
```

While the model is doing its training, you can also look at the real-time performance through [H2o Flow](http://localhost:54321/flow/index.html). Just click getModels from the GUI and you'll find a list of model id that you've ran or is running. This is also why it's usually a good idea to manually give each single model a corresponding model id that's human distinguishable. Note that you have to have a h2o cluster running to see it or else the link won't work.

Lastly we'll create predictions using our latest RF model against the test set to check if there're signs of overfitting.

```{r}

# create predictions using our latest RF model against the test set.
rf2_pred <- h2o.predict(rf2, newdata = test)

# Glance at what that prediction set looks like
# We see a final prediction in the "predict" column,
# and then the predicted probabilities per class.
rf2_pred

# test set accuracy
mean(rf2_pred$predict == test$Cover_Type)  

```

We have very similar error rates on both sets, so it would not seem that we have overfit the validation set through our experimentation.

**Takeaways:**

Some other things worth trying if we were to continue.

- We could further experiment with deeper trees or a higher percentage of columns used (mtries). Also we could experiment with the `nbins` and `nbins_cats` settings to control the H2O splitting. The general guidance is to lower the number to increase generalization (avoid overfitting). A good example of adjusting this value is for `nbins_cats` to be increased to match the number of values in a category. Though usually unnecessary, if a problem has a very important categorical predictor, this can improve performance.
- In a production setting where fine-grain accuracy is beneficial, it is common to set the learning rate to a very small number, such as 0.01 or less and add trees to match. Use of early stopping is very powerful to allow the setting of a low learning rate and then building as many trees as needed until the desired convergence is met.


# R Session Information

```{r}
devtools::session_info()
```

# Reference 

- [Youtube: H2o Gradient Boosting Method and Random Forest](https://www.youtube.com/watch?v=9wn1f-30_ZY)

