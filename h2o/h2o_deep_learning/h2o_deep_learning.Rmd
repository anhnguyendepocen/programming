---
title: "H2o Deep Learning"
author: "Ethen Liu"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: pygments
---

<style type="text/css">
p { /* Normal  */
   font-size: 18px;
}
body { /* Normal  */
   font-size: 18px;
}
td {  /* Table  */
   font-size: 14px;
}
h1 { /* Header 1 */
 font-size: 32px;
}
h2 { /* Header 2 */
 font-size: 26px;
}
h3 { /* Header 3 */
 font-size: 22px;
}
code.r { /* Code block */
  font-size: 14px;
}
pre { /* Code block */
  font-size: 14px
}
</style>

> [R code](https://github.com/ethen8181/machine-learning/tree/master/h2o/h2o_deep_learning/h2o_deep_learning.R) to the documentation for those that wish to follow along.

# H2o Deep Learning Hands On

H2o is great because all of the actual computation is performed inside the H2O cluster, rather than in R memory.

## Environment Setting

```{r, message=FALSE, warning=FALSE}

library(h2o)
library(data.table)
setwd("/Users/ethen/machine-learning/h2o")

# h2o.init : Start up a 1-node H2O server on your local machine 
#            and allow it to use all CPU cores ( nthreads = -1 )
#            you can specify the @max_mem_size = "2G" to make it use no more than 2GB of memory
h2o.init(nthreads = -1)

# h2o will also give you a progress bar of the training, which is nice,
# but here we disable it so it doesn't clutter up the document
h2o.no_progress()

```

For this example we want to predict the Cover_Type column, a categorical feature with 7 levels and the Deep Learning model will be tasked to perform multi-class classification. It uses the other 12 predictors of the dataset, of which 10 are numerical and 2 are categorical with a total of 44 levels. For more desciptions of the dataset, refer to the following link. [[UCI Machine Learning Repository: Covertype Data Set](https://archive.ics.uci.edu/ml/datasets/Covertype)]

The following section import the full cover type dataset (581k rows, 13 columns, 10 numerical, 3 categorical); Split the data 3 ways: 60% for training, 20% for validation (hyper parameter tuning) and 20% for final testing.

```{r}

# h2o.importFile : imports the file to the h2o cluster 
df <- h2o.importFile(path = "covtype.full.csv")
list( dimension = dim(df), head = df )

# specify input and output features
output <- "Cover_Type"
input  <- setdiff( names(df), output )

# h2o.splitFrame : Split an existing H2O data set according to user-specified ratios
# h2o.assign : makes a copy of / rename the dataset 
split <- h2o.splitFrame( df, c(0.6, 0.2), seed = 1234 )
train <- h2o.assign( split[[1]], "train" ) # 60%
valid <- h2o.assign( split[[2]], "valid" ) # 20%
test  <- h2o.assign( split[[3]], "test" )  # 20%

```

## Deep Learning Models

We try our first deep learning model with the basic parameters.

```{r}

model_dl_1 <- h2o.deeplearning(
	model_id = "dl_1", # (optional) assign a user-specified id to the model
	training_frame = train, 
	validation_frame = valid, # validation dataset: used for scoring and early stopping
	x = input,
	y = output,
	# activation = "Rectifier", # default (a.k.a Relu)
	# hidden = c(200, 200),    # default = 2 hidden layers with 200 neurons each
	epochs = 1, # How many times the dataset should be iterated
	variable_importances = TRUE # allows obtaining the variable importance, not enabled by default
)

# h2o.varimp : obtaining the variable importance
head( as.data.table( h2o.varimp(model_dl_1) ) )

# validation accuracy
h2o.hit_ratio_table(model_dl_1, valid = TRUE)[1, 2]

```

Other Tuning Parameters: 

- For sampling: 
    - `score_validation_samples` : The number of samples that does the validation. The samples can be randomly sampled or stratified if `balance_classes` is set to TRUE and `score validation_sampling` is "Stratified". To select the entire validation dataset, specify 0, which is the default
    - `score_validation_sampling` : Specifies the method used to sample the validation dataset for scoring. The options are "Uniform" and "Stratified". The default is Uniform
    - `balance_classes` : For imbalanced data, setting to TRUE can result in improved predictive accuracy. The default is FALSE
- For early stopping:
    - `stopping_rounds`, `stopping_metric`, `stopping_tolerance`. 
    - e.g. `stopping_rounds` = 5, `stopping_metric` = "MSE", `stopping_tolerance` = 1e-3 means that to stop as soon as the moving average of length 5 of the validation MSE does not improve by at least 1e-3 for 5 consecutive scoring events. Refer to documentation for other types of `stopping metric`
- Others: 
    - `l1` L1 regularization, improves generalization and prevents overfitting
    - `sparse` : Sparse data handling (more efficient for data with lots of 0 values). Default to FALSE

Now we run another, smaller network, and we let it stop automatically once the misclassification rate converges (specifically, if the moving average of length 2 does not improve by at least 1% for 2 consecutive scoring events). We also sample the validation set to 10,000 rows for faster scoring.

```{r}

# second model 
model_dl_2 <- h2o.deeplearning(
	model_id = "dl_2", 
	training_frame = train, 
	validation_frame = valid,
	x = input,
	y = output,
	hidden = c(32, 32, 32), # smaller network, runs faster
	epochs = 100, # hopefully converges earlier...
	score_validation_samples = 10000, # sample the validation dataset (faster)
	stopping_rounds = 5,
	stopping_metric = "misclassification",
	stopping_tolerance = 0.01
)

```

Next we use `h2o.predict`, which returns an H2O Frame object with default predictions predict in the first column (for classification) probabilites that each observation belongs to each classes to obtain the prediction and compare the accuracy between the two models. 

```{r}

# evaluate the two models on the test set
pred1 <- h2o.predict(model_dl_1, test)
pred2 <- h2o.predict(model_dl_2, test)
list( model_dl_1 = mean(pred1$predict == test$Cover_Type), 
      model_dl_2 = mean(pred2$predict == test$Cover_Type) )

```

## Hyperparamters Tuning 

Since there are a lot of parameters that can impact model accuracy, hyper-parameter tuning is especially important for Deep Learning. One way to do it is with **grid search** using `h2o.grid` and specifying the hyper_params. For speed, we will only train on the first 10,000 rows of the training dataset.

```{r}

# train samples of the training data for speed 
sampled_train <- train[1:10000, ]

# specify the list of paramters 
hyper_params <- list(
	hidden = list( c(32, 32, 32), c(64, 64) ),
	input_dropout_ratio = c(0, 0.05),
	l1 = c(1e-4, 1e-3)
)

# performs the grid search
grid_id <- "dl_grid"
model_dl_grid <- h2o.grid(
	algorithm = "deeplearning", # name of the algorithm 
	grid_id = grid_id, 
	training_frame = sampled_train,
	validation_frame = valid, 
	x = input, 
	y = output,
	epochs = 10,
	stopping_metric = "misclassification",
	stopping_tolerance = 1e-2, # stop when logloss does not improve by >=1% for 2 scoring events
	stopping_rounds = 2,
	score_validation_samples = 10000, # downsample validation set for faster scoring
	hyper_params = hyper_params
)

# find the best model and evaluate its performance
stopping_metric <- 'accuracy'
sorted_models <- h2o.getGrid(
	grid_id = grid_id, 
	sort_by = stopping_metric,
	decreasing = TRUE
)
best_model <- h2o.getModel(sorted_models@model_ids[[1]])
pred3 <- h2o.predict(best_model, test)
mean(pred3$predict == test$Cover_Type)

```

Note that we're using less training data to perform the grid search, thus the accuracy is lower.

Though, often times, hyper-parameter search for more than 4 parameters can be done more efficiently with random parameter search than with grid search. Basically, chances are good to find one of many good models in less time than performing an exhaustive grid search. We can do this by changing the parameter `search_criteria`.

Some other functionality such as saving, loading the model and shutting down the cluster.

```{r}

# storing and loading the model 
# path <- h2o.saveModel(model, path = "mybest_deeplearning_covtype_model", force = TRUE)
# print(path)
# loaded <- h2o.loadModel(path)

h2o.shutdown(prompt = FALSE)

```

## Tips and Tricks for Tuning Deep Learning 

### Understanding Model Complexity

**Model Size Depends on Features and is Independent of Number of Rows or Training Time**

Suppose you're given the dataset with 1111 predictor columns, where 1100 of them are numerical and 10 are categorical variables with 400 levels. And you also have 1 binary response column. 

**Question:** Given two models. Model1 : with 4 layers of 400 neurons [400, 400, 400, 400] and Model2 with 1 layer that has 500 neurons which one is more complex?

**Answer:** Model2. 

Because categorical variables are one-hot encoded into 4000 dummy variables of 0 and 1s (10 categorical variables times 400 levels each), hence there will be a total of 5100 input neurons (1100 numerical variables plus 4000 dummy variables). Having our 5100 input neurons, for model1, our second layer has 400 neurons so after going through the second layer you get 5100 X 400. Where as for model2 our second layer has 500 neurons so after going through the second layer you get 5100 X 500. And this little difference is all that matters. 

The total model complexity of model1 will be 5100 * 400 + 400 * 400 * 3 + 400 * 2, a total of `r 5100 * 400 + 400 * 400 * 3 + 400 * 2` and model2 will be 5100 * 500 + 500 * 2, a total of `r 5100 * 500 + 500 * 2` (the times 2 is the last layer's neuron number times the binary outcome, which is 2).

### Knowing Your Data

From the last point we know that if the number of input neurons (input variables) is large, then training can be inefficient and slow, especially if there're many sparse features produced by high-level categorical variables. Thus try the following remedies:

1. Ignore categorical columns with high factor count.
2. Use **GLRM** to reduce the dimensionality of the dataset and make sure to apply the same model to the test set.
 
### Use Early Stopping 

Using early stopping will prevent you from taking a long time to train a model that does not improve its performance over time, which will save you tons of time. It's on by default for h2o's deep learning ( the `overwrite_with_best_model` is set to TRUE by default ), but it's not on by default for GBM and RandomForest. 

You can also specify additional convergence criteria with `stopping_rounds`, `stopping_metric`, `stopping_tolerance`. e.g. `stopping_rounds` = 5, `stopping_metric` = "MSE", `stopping_tolerance` = 1e-3 means that to stop as soon as the moving average of length 5 of the validation MSE does not improve by at least 1e-3 for 5 consecutive scoring events.

### Control Scoring Events 

Note that the validation data determines scoring speed and early stopping. So if you provided a very large validation dataset, then even scoring can take a long time. The way to solve this is to : 

1. Provide a smaller validation dataset.
2. Specify the number of validation set samples for scoring using `score_validation_samples`.

### Perform HyperParameter Search

There're a lot of parameters for H2o's deep learning, thus it is not recommended to do a grid search because it is a cartesian product of all of your parameters and there'll just be too many combinations to try out. So just find one of the many good models. Some parameters worth trying :

- `activation` For activation functions "Rectifier" and "RectifierWithDropout" is recommended.
- `hidden` Try 2 to 5 layers deep, 10 to 2000 neurons per layer.
- `l1` and `l2` Regularization to prevent overfitting, L1 and L2 penalties can be applied by specifying their parameters. Intuition: L1 lets only strong weights survive ( constant pulling force towards zero ), while L2 prevents any single weight from getting too big.

### Do Ensembling 

To obtain highest accuracy, it's often more effective to average a few fast and diverse models than to build one large model. Thus we can use different network architecture, regularization schemes to keep the variance high before ensembling. So given a set of diverse and good (but not the best) models use:

- Blending : Just Average the models
- Gut-Feel Blending : Assign your own weights that add up to 1
- Add Other Models : Add GLM, GBM, RandomForest into the mix


# R Session Information

```{r}
devtools::session_info()
```

# Reference

- [Youtube: Top 10 Deep Learning Tips and Tricks](https://www.youtube.com/watch?v=LM255qs8Zsk)

