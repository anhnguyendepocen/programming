---
title: "H2o Generalized Linear Model"
author: "Ethen Liu"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: pygments
---

<style type="text/css">
p { /* Normal  */
   font-size: 18px;
}
body { /* Normal  */
   font-size: 18px;
}
td {  /* Table  */
   font-size: 14px;
}
h1 { /* Header 1 */
 font-size: 32px;
}
h2 { /* Header 2 */
 font-size: 26px;
}
h3 { /* Header 3 */
 font-size: 22px;
}
code.r { /* Code block */
  font-size: 14px;
}
pre { /* Code block */
  font-size: 14px
}
</style>

> [R code](https://github.com/ethen8181/machine-learning/blob/master/h2o/h2o_glm/h2o_glm.R) to the documentation for those that wish to follow along.

# Basic Background Information

The strength of Generalized Linear Models are that they are easy to fit; easy to understand. Though it does not deal very well with correlated variables, thus, often times you need to add L1 (Lasso) or L2 (Ridge) regularization or so called penalty, which is controlled by the `alpha` and `lambda` value to prevent overfitting.

- `alpha` A value of 1.0 for `alpha` represents Lasso, and a value of 0.0 produces Ridge regression.
- `lambda` This is the most important piece of information for someone learning about regularization. It controls how complex the model is allowed to be. The intuition is: when lambda is large, you penalize your model very heavily when it starts to get more complex, on the opposite when the value is 0, you don't penalize it at all and the `alpha` parameter is ignored.

In h2o you can specify to use a `lambda_search` = TRUE. Lambda search enables efficient and automatic search for the optimal value of the lambda parameter. When lambda search is enabled, GLM will first fit a model with maximum regularization and then keep decreasing it until overfitting occurs (If running lambda search with a validation dataset, the chosen lambda value corresponds to the lambda with the lowest validation error). The resulting model is based on the best lambda value. By performing lambda search, it can efficiently filter out inactive predictors (known as noise) and only build models for a small subset of predictors.

# Quick Hands On

Our task is to predicting forest cover type, the response is the "Cover_Type" column and has 7 classes. But for illustration purpose, we'll simply try a simpler binary classification. We'll take a subset of the data with only class_1 and class_2 (the two majority classes) and build a binomial model on them using all the input variables.

```{r, message=FALSE, warning=FALSE}

library(h2o)
library(ggplot2)
library(data.table)
setwd("/Users/ethen/machine-learning/h2o")

# initialize the cluster with all the threads available
h2o.init(nthreads = -1)

# disable progress bar so it doesn't clutter up the document
h2o.no_progress()

# import and convert to binomial data
df <- h2o.importFile( path = normalizePath("covtype.full.csv") )
y <- "Cover_Type"
x <- setdiff( names(df), y )
df_binomial <- df[ df$Cover_Type %in% c("class_1", "class_2"), ]
h2o.setLevels( df_binomial$Cover_Type, c("class_1","class_2") )

# split to train / test / validation
# use smaller dataset for testing 
data_binomial <- h2o.splitFrame( df_binomial, ratios = c(.6, 0.15) )
names(data_binomial) <- c('train', 'valid', 'test')
data_binomial$train

```

Grid search over different alpha values. By default h2o's glm will standardize the input variables.

```{r}

# perform grid search, it's best to give the model
# a id so retrieving information on them will be easier later
grid_id <- 'glm_grid'
hyper_parameters <- list( alpha = c(0, .5, 1) )
model_glm_grid <- h2o.grid(
	algorithm = "glm", 
	grid_id = grid_id,
	hyper_params = hyper_parameters,
	training_frame = data_binomial$train, 
	validation_frame = data_binomial$valid, 
	x = x, 
	y = y,
	lambda_search = TRUE,
	family = "binomial"
)

```

We'll take the best performing model and look at various outputs.

```{r, fig.height=6, fig.width=8}

# sort the model by the specified evaluation metric
# and obtain the top one (the best model)
stopping_metric <- 'accuracy'
sorted_models <- h2o.getGrid(
	grid_id = grid_id, 
	sort_by = stopping_metric,
	decreasing = TRUE
)
best_model <- h2o.getModel(sorted_models@model_ids[[1]])

# for binomial output, h2o will choose the cutoff threshold by 
# maximizing the f1 score by default, we can change the metric
# to change that behavior
h2o.confusionMatrix(best_model, valid = TRUE, metrics = 'accuracy')

# coefficients (standardized and non-standardized)
# or we can use the short-cut below
# h2o.coef(best_model)
# h2o.coef_norm(best_model)
best_model@model$coefficients

# obtain the regularization, alpha and lambda 
best_model@model$model_summary$regularization

# area under the curve
auc <- h2o.auc(best_model, valid = TRUE)
fpr <- h2o.fpr( h2o.performance(best_model, valid = TRUE) )[['fpr']]
tpr <- h2o.tpr( h2o.performance(best_model, valid = TRUE) )[['tpr']]
ggplot( data.table(fpr = fpr, tpr = tpr), aes(fpr, tpr) ) + 
geom_line() + theme_bw() + ggtitle( sprintf('AUC: %f', auc) )

# remember to shutdown the cluster once we're done
h2o.shutdown(prompt = FALSE)

```

# R Session Information

```{r}
devtools::session_info()
```

# Reference 

- [Youtube: H2o GLM](https://www.youtube.com/watch?v=VJPltxh5Q6Q)

