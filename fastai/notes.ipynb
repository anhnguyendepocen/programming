{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Lecture-3\" data-toc-modified-id=\"Lecture-3-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Lecture 3</a></span></li><li><span><a href=\"#Lesson-4\" data-toc-modified-id=\"Lesson-4-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Lesson 4</a></span></li><li><span><a href=\"#Lesson-5\" data-toc-modified-id=\"Lesson-5-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Lesson 5</a></span></li><li><span><a href=\"#Lesson-6\" data-toc-modified-id=\"Lesson-6-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Lesson 6</a></span></li><li><span><a href=\"#Lesson-7\" data-toc-modified-id=\"Lesson-7-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Lesson 7</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- why fastai course http://www.fast.ai/2018/01/26/v2-launch/\n",
    "- setup https://github.com/reshamas/fastai_deeplearn_part1/blob/master/tools/paperspace.md\n",
    "- kernel http://setosa.io/ev/image-kernels/\n",
    "\n",
    "---\n",
    "\n",
    "- learning rate finder, at the optimal loss, the learning is probably too high already, thus we go back one magnitude.\n",
    "    - https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n",
    "    - http://teleported.in/posts/cyclic-learning-rate/\n",
    "    - optimizer without learning rate http://teleported.in/posts/cocob/\n",
    "- after unfreezing (some changes to the way we're training), we'll could run it again\n",
    "- cosine annealing is a popular method for annealing learning rate\n",
    "- learning rate restart to hop out of the spiky part so it will generalize better (cycle_len=1, reset the learning rate every iteration, cycle_mult, prolong the cycle); we can also have the weights at each restart and ensemble them\n",
    "- different learning rate for different layers (then we don't need to worry about unfreezing different layer; the place where we might want to only unfreeze certain layer is when we're faced with computation difficulties)\n",
    "- test time augmentation for improving performance\n",
    "- if we're underfitting, i.e. the validation loss is a lot lower than the training loss, then we should increase our iteration\n",
    "\n",
    "---\n",
    "\n",
    "- ensembling convolution https://miguel-data-sc.github.io/2017-11-23-second/\n",
    "- reflection padding\n",
    "- continue training with larger dataset to avoid overfitting\n",
    "\n",
    "---\n",
    "\n",
    "- make copy of minority class when faced with extremely unbalanced dataset\n",
    "- segment out object from the image and crop that part if the image is not close to the center of the image\n",
    "\n",
    "---\n",
    "\n",
    "resnet decoded http://teleported.in/posts/decoding-resnet-architecture/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- curlwget for downloading data\n",
    "- FileLink for passing file from server to local ...\n",
    "- numpy index with None, add additional dimension\n",
    "\n",
    "---\n",
    "\n",
    "- ? precompute = true meaning\n",
    "- unfreeze vs batchnorm unfreeze (useful for regular image, target object takes up most of the image)\n",
    "- resize the image and save it to another folder to speed up training in the future\n",
    "- multiclass classification, sigmoid activation\n",
    "- lecture 3: 1:51 explanation of why training the last layer before unfreezing pretrained weights makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- another learning rate post https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b\n",
    "- post on validation set http://www.fast.ai/2017/11/13/validation-sets/\n",
    "- set up a formula for determining the size of the embedding matrix for categorical variables\n",
    "- add datepart function for creating a bunch of features that is related to dates\n",
    "- rossman: structured deep learning https://towardsdatascience.com/structured-deep-learning-b8ca4138b848\n",
    "- rossman: train an embedding for the categorical variables and use them instead as features. We can also treat these embeddings as pre-trained embeddings as use them in other projects. (Lesson 6: 30:00 visualization of embedding, e.g. the distance of the actual store correlates with the distance of the learned embeddings)\n",
    "- rossman: visualization of the stores shows that its closed (lesson 6: 47:40)\n",
    "- rossman: clamp prediction\n",
    "- language model, a model that can predict the next word of a sentence. applications including swiftkey\n",
    "- after training a language model that can understand the language, we can then use it on nlp related machine learning task, e.g. sentiment analysis, churn analysis (input text comes from customer support ticket)\n",
    "- for language model, we probably don't want to stemming\n",
    "- torchtext, randomly add/subtract the number of input character per batch\n",
    "- dropouts for LSTM language model https://arxiv.org/abs/1708.02182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- initialization: http://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/\n",
    "- in collaborative filtering, concatenate the user and item factors and build a deep learning method on it.\n",
    "- optimization http://ruder.io/deep-learning-optimization-2017/\n",
    "- video 1:54 optimizer\n",
    "- adamw optimizer instead of adam for better performance, short version, don't let the regularization mess up the gradient and gradient squared (sgd generalizes better?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to_np function that converts tensor to numpy regardless of whether it's a Tensor or Variable that's on the CPU or GPU.\n",
    "- autoencoder, kaggle insurance claim\n",
    "- adaptive max pooling\n",
    "- for RNN used in translation, we can train a word-level model and character-level model, where the character-level can be used to handle words unseen during training. Note there's also the notion of byte pair encoding, which is somewhere in between.\n",
    "- A Simple Way to Initialize Recurrent Networks of Rectified Linear Units https://arxiv.org/pdf/1504.00941.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 14:00 stateful RNN\n",
    "- 50:00 GRU: the sigmoid determines how much of the hidden state should i remember\n",
    "- 57:00 cosine annealing callback\n",
    "- when training images from scratch instead of using pre-trained models, we normalize the images ourselves, i.e. we need to compute the mean and standard deviation ourselves.\n",
    "- data augmentation: pad the image and pick a random spot at the padded image\n",
    "- whenever we create a list of layers in pytorch, we need to use ModuleList to register them as the model class's attributes.\n",
    "- exploring optimizer, implemented in PyTorch https://github.com/KeremTurgutlu/deeplearning/blob/master/Exploring%20Optimizers.ipynb\n",
    "- stride 2 convolution instead of maxpooling\n",
    "- 1:17:40 adaptive max pooling and extract the largest value?\n",
    "- 1:27 batchnorm, another way of introducing regularization, i.e. we don't have to worry too much about setting the weight regularization and dropout.\n",
    "- 1:40 putting batchnorm after the activation gives better performance?\n",
    "- 1:50 a single conv layer at the beginning that has more filter with larger kernel size for capturing richer features from the beginning\n",
    "- 1:53:30 resnet\n",
    "- 1:59 bottleneck layer (stride 2)?\n",
    "- pre-act resnet gives better performance (different ordering in the resnet block)\n",
    "- 2:09 class activation map, which part of the image turned out to be important"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "201px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
