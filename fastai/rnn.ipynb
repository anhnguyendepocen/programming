{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off with a toy example using PyTorch, we will ignore the details that are happening within the RNN cell as of now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence input size torch.Size([1, 5, 4]) out size torch.Size([1, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# suppose we have a\n",
    "# One hot encoding for each char in 'hello'\n",
    "# and the sequence for the word 'hello' is 5\n",
    "seq_len = 5\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "\n",
    "# here we specify a single RNN cell with the property of\n",
    "# input_dim (4) -> output_dim (2)\n",
    "# batch_first explained in the following\n",
    "rnn_cell = nn.RNN(input_size = 4, hidden_size = 2, batch_first = True)\n",
    "\n",
    "# our input shape should be of shape\n",
    "# (batch, seq_len, input_size) when batch_first=True;\n",
    "# the input size basically referrs to the number of feature's dimension\n",
    "# (seq_len, batch_size, input_size) when batch_first=False (default)\n",
    "# thus we reshape our input to the appropriate size, torch.view is\n",
    "# equivalent to numpy.reshape\n",
    "inputs = Variable(torch.Tensor([h, e, l, l, o]))\n",
    "inputs = inputs.view(1, 5, -1)\n",
    "\n",
    "# our hidden is the weights that gets passed along the cells,\n",
    "# here we initialize some random values for it:\n",
    "# (batch, num_layers * num_directions, hidden_size) for batch_first=True\n",
    "# disregard the second argument as of now\n",
    "hidden = Variable(torch.randn(1, 1, 2))\n",
    "out, hidden = rnn_cell(inputs, hidden)\n",
    "print('sequence input size', inputs.size(), 'out size', out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll teach our RNN to produce \"ihello\" from \"hihell\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an index to character mapping\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]    # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "\n",
    "y_data = [1, 0, 2, 3, 3, 4]      # ihello\n",
    "\n",
    "# As we have one batch of samples, we will change them to variables only once\n",
    "inputs = Variable(torch.Tensor(x_one_hot))\n",
    "labels = Variable(torch.LongTensor(y_data))\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "seq_len = 6      # |ihello| == 6\n",
    "input_size = 5   # one-hot size\n",
    "batch_size = 1   # one sentence\n",
    "num_layers = 1   # one-layer rnn\n",
    "num_classes = 5  # predicting 5 distinct character\n",
    "hidden_size = 4  # output from the RNN\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model will be a RNN followed by a linear layer,\n",
    "    i.e. a fully-connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, num_classes, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first = True)\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # assuming batch_first = True for RNN cells\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        x = x.view(batch_size, self.seq_len, self.input_size)\n",
    "        \n",
    "        # apart from the output, rnn also gives us the hidden\n",
    "        # cell, this gives us the opportunity to pass it to\n",
    "        # the next cell if needed; we won't be needing it here\n",
    "        # because the nn.RNN already computed all the time steps\n",
    "        # for us\n",
    "        rnn_out, _ = self.rnn(x, hidden)\n",
    "        linear_out = self.linear(rnn_out.view(-1, hidden_size))\n",
    "        return linear_out\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize hidden cell states, assuming\n",
    "        batch_first = True for RNN cells\n",
    "        \"\"\"\n",
    "        hidden = Variable(torch.zeros(\n",
    "            batch_size, self.num_layers, self.hidden_size))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(5, 4, batch_first=True)\n",
      "  (linear): Linear(in_features=4, out_features=5)\n",
      ")\n",
      "epoch: 1, loss: 1.756\n",
      "Predicted string:  eeeeee\n",
      "epoch: 2, loss: 1.626\n",
      "Predicted string:  ehhhhh\n",
      "epoch: 3, loss: 1.485\n",
      "Predicted string:  elllll\n",
      "epoch: 4, loss: 1.405\n",
      "Predicted string:  llllll\n",
      "epoch: 5, loss: 1.293\n",
      "Predicted string:  illlll\n",
      "epoch: 6, loss: 1.217\n",
      "Predicted string:  iiilll\n",
      "epoch: 7, loss: 1.057\n",
      "Predicted string:  iollll\n",
      "epoch: 8, loss: 0.967\n",
      "Predicted string:  ielllo\n",
      "epoch: 9, loss: 0.837\n",
      "Predicted string:  ihlllo\n",
      "epoch: 10, loss: 0.696\n",
      "Predicted string:  ihello\n",
      "epoch: 11, loss: 0.615\n",
      "Predicted string:  ihello\n",
      "epoch: 12, loss: 0.535\n",
      "Predicted string:  ihhllo\n",
      "epoch: 13, loss: 0.452\n",
      "Predicted string:  ihhllo\n",
      "epoch: 14, loss: 0.387\n",
      "Predicted string:  ihello\n",
      "epoch: 15, loss: 0.322\n",
      "Predicted string:  ihello\n"
     ]
    }
   ],
   "source": [
    "# Set loss, optimizer and the RNN model\n",
    "torch.manual_seed(777)\n",
    "rnn = RNN(seq_len, num_classes, input_size, hidden_size, num_layers)\n",
    "print(rnn)\n",
    "\n",
    "# train the model\n",
    "num_epochs = 15\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr = 0.1)\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = rnn(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # check the current predicted string\n",
    "    _, idx = outputs.max(dim = 1)\n",
    "    idx = idx.data.numpy()\n",
    "    result_str = [idx2char[c] for c in idx]\n",
    "    print('epoch: {}, loss: {:1.3f}'.format(epoch, loss.data[0]))\n",
    "    print('Predicted string: ', ''.join(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those interested the following link has a nice blog post that implements RNN in numpy. [Blog: Recurrent Neural Networks Tutorial, Part 2 â€“ Implementing a RNN with Python, Numpy and Theano](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Github: Simple PyTorch Tutorials Zero to ALL!](https://github.com/hunkim/PyTorchZeroToAll)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
