{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation \n",
    "\n",
    "Download pyspark from anaconda.\n",
    "\n",
    "```shell\n",
    "conda install -y -c anaconda-cluster spark\n",
    "```\n",
    "\n",
    "Download the latest version of apache spark from this [link](https://spark.apache.org/downloads.html). For the package type choose the package `pre-built for Hadoop 2.6` and leave the rest untouched. After downloading it, unzip it and place the folder in your home directory and change the folder name to just `spark`.\n",
    "\n",
    "Then you need to define these environment variables before starting the notebook.\n",
    "\n",
    "```shell\n",
    "export SPARK_HOME=~/spark\n",
    "export PYSPARK_PYTHON=python3\n",
    "export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\n",
    "export PACKAGES=\"com.databricks:spark-csv_2.11:1.4.0\"\n",
    "export PYSPARK_SUBMIT_ARGS=\"--packages ${PACKAGES} pyspark-shell\"\n",
    "```\n",
    "\n",
    "In Unix/Mac, this can be done in .bashrc or .bash_profile.\n",
    "\n",
    "- [link](http://people.duke.edu/~ccc14/sta-663-2016/21A_Introduction_To_Spark.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "When you run a Spark program, it actually consists of two programs. There's a driver program and there's a workers program. The driver program runs on one machine. And the worker program runs either on cluster nodes or in local threads on the same machine.\n",
    "\n",
    "A Spark program first creates a `SparkContext` object. And the `SparkContext` tells Spark how and where\n",
    "to access a cluster. If we're writing a Spark program, separated from the pySpark shell or the Databricks Community Edition environment, then you have to create a new `SparkContext` ourselves. Next the program creates a SQL context object that's used to create data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sc = pyspark.SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run a Spark program, it actually consists of two programs. There's a driver program and there's a workers program. The driver program runs on one machine. And the worker program runs either on cluster nodes or in local threads on the same machine. The data frames that we create are automatically distributed across all of the workers and pySpark provides an easy to use programming abstraction and parallel runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|      John|    Smith|\n",
      "|      Ravi|    Singh|\n",
      "|     Julia|    Jones|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame( [(\"John\", \"Smith\"), (\"Ravi\", \"Singh\"), (\"Julia\", \"Jones\")], \n",
    "                                 (\"first_name\", \"last_name\") )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first_name', 'last_name']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(first_name='John', last_name='Smith'), Row(first_name='Ravi', last_name='Singh'), Row(first_name='Julia', last_name='Jones')]\n",
      "John\n",
      "John\n"
     ]
    }
   ],
   "source": [
    "# each DataFrame is a list of Row object \n",
    "print( df.select( '*' ).collect() )\n",
    "\n",
    "# index [0] will access the first Row (data point) and another [0]\n",
    "# will access the first data point's first feature\n",
    "print( df.select( '*' ).collect()[0][0] )\n",
    "print( df.select( 'first_name' ).collect()[0][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or we can use first, if we only wish to access\n",
    "# the first element\n",
    "df.first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot encode \n",
    "sample_ohe_dict_auto = create_one_hot_dict(sample_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "?LogisticRegression(  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "model = LinearRegression()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "?PolynomialExpansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "?Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "[('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)]\n",
      "[('elephant', 1), ('rat', 2), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList)\n",
    "# Print out the type of wordsRDD\n",
    "print( type(wordsRDD) )\n",
    "\n",
    "# simple word count\n",
    "wordPairs = wordsRDD.map( lambda x: ( x, 1 ) )\n",
    "print( wordPairs.collect() )\n",
    "\n",
    "wordCounts = wordPairs.reduceByKey( lambda x, y: x + y )\n",
    "print( wordCounts.collect() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string with punctuation inside of it Does this work I hope so\n"
     ]
    }
   ],
   "source": [
    "# http://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate\n",
    "import string\n",
    "\n",
    "# Create a dictionary using a comprehension - this maps every character from\n",
    "# string.punctuation to None. Initialize a translation object from it.\n",
    "translator = str.maketrans({ key: None for key in string.punctuation })\n",
    "\n",
    "s = 'string with \"punctuation\" inside of it! Does this work? I hope so.'\n",
    "\n",
    "# pass the translator to the string's translate method.\n",
    "print(s.translate(translator))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
