{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn\n",
    "\n",
    "## Grid Search\n",
    "\n",
    "Learning how to implement grid search from [Source code: scikit-learn's model selection](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_search.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted parameters, values:  [('a', [1, 2]), ('b', [True, False])]\n",
      "\n",
      "parameters:  ('a', 'b')\n",
      "values ([1, 2], [True, False])\n",
      "\n",
      "grid search parameters\n",
      "{'b': True, 'a': 1}\n",
      "{'b': False, 'a': 1}\n",
      "{'b': True, 'a': 2}\n",
      "{'b': False, 'a': 2}\n"
     ]
    }
   ],
   "source": [
    "# Grid search\n",
    "from itertools import product\n",
    "from collections import Mapping\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "params_grid = {'a': [1, 2], 'b': [True, False]}\n",
    "\n",
    "# ensures that it also supports list of dictionary,\n",
    "# Mapping ensures a object has keys, values, items, etc. methods\n",
    "# which matches a dictionary\n",
    "# https://docs.python.org/3/library/collections.abc.html\n",
    "if isinstance(params_grid, Mapping):\n",
    "    params_grid = [params_grid]\n",
    "    \n",
    "for p in params_grid:\n",
    "    # for reproducibility, always sort the keys of a dictionary\n",
    "    # this will become a list of paired tuples\n",
    "    items = sorted(p.items())\n",
    "    print('sorted parameters, values: ', items)\n",
    "    print()\n",
    "    \n",
    "    # unpack the list of tuples into two lists tuples, so what's originally \n",
    "    # a list of items [('a', [1, 2]), ('b', [True, False])], becomes\n",
    "    # two lists ('a', 'b'), ([1, 2], [True, False]), with all the keys being the parameter\n",
    "    # and the value being the list of possible values that the parameter can take\n",
    "    # http://stackoverflow.com/questions/7558908/unpacking-a-list-tuple-of-pairs-into-two-lists-tuples\n",
    "    key, value = zip(*items)\n",
    "    print('parameters: ', key)\n",
    "    print('values', value)\n",
    "    print()\n",
    "    \n",
    "    # unpack the list of values to compute the cartesian product\n",
    "    # [(1, True), (1, False), (2, True), (2, False)], and zip it\n",
    "    # back to the original key\n",
    "    print('grid search parameters')\n",
    "    cartesian = product(*value)\n",
    "    for v in cartesian:\n",
    "        params = dict(zip(key, v))\n",
    "        print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 1, 'b': True},\n",
       " {'a': 1, 'b': False},\n",
       " {'a': 2, 'b': True},\n",
       " {'a': 2, 'b': False}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm with scikit-learn's output\n",
    "list( ParameterGrid(params_grid) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': True, 'a': 1}\n",
      "{'b': False, 'a': 1}\n",
      "{'b': True, 'a': 2}\n",
      "{'b': False, 'a': 2}\n"
     ]
    }
   ],
   "source": [
    "# making our function\n",
    "def _get_params_grid(params_grid):\n",
    "    \"\"\"\n",
    "    create cartesian product of parameters (grid search),\n",
    "    this will be a generator that will allow looping through\n",
    "    all possible parameter combination, note if we want to\n",
    "    expand this to cross validation we'll have to turn it to a list\n",
    "    \"\"\"\n",
    "    # for reproducibility, always sort the keys of a dictionary\n",
    "    items = sorted(params_grid.items())\n",
    "    \n",
    "    # unpack parameter and the range of values\n",
    "    # into separate list; then unpack the range \n",
    "    # of values to compute the cartesian product\n",
    "    # and zip it back to the original key\n",
    "    key, value = zip(*items)\n",
    "    cartesian = product(*value)\n",
    "    for v in cartesian:\n",
    "        params = dict(zip(key, v))\n",
    "        yield params\n",
    "\n",
    "params_grid = {'a': [1, 2], 'b': [True, False]}\n",
    "params = _get_params_grid(params_grid)\n",
    "for p in params:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# self._fit(X, y, groups, ParameterGrid(self.param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KFolds:\n",
    "    \"\"\"\n",
    "    K-Folds cross-validation\n",
    "    Provides train/test indices to split data in train/test sets. Split\n",
    "    dataset into k consecutive folds; Each fold is then used once as \n",
    "    a validation while the k - 1 remaining folds form the training set\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int\n",
    "        number of folds. Must be at least 2\n",
    "    \n",
    "    shuffle : boolean\n",
    "        whether to shuffle the data before splitting into batches\n",
    "    \n",
    "    seed : int\n",
    "        When shuffle = True, pseudo-random number generator state used for\n",
    "        shuffling; this ensures reproducibility\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits, shuffle, seed):\n",
    "        self.seed = seed\n",
    "        self.shuffle = shuffle\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "    def split(self, X):\n",
    "        \"\"\"pass in the data to create train/test split for k fold\"\"\"\n",
    "        # shuffle modifies indices inplace\n",
    "        indices = np.arange(X.shape[0])\n",
    "        if self.shuffle:\n",
    "            rstate = np.random.RandomState(self.seed)\n",
    "            rstate.shuffle(indices)\n",
    "\n",
    "        for test_mask in self._iter_test_masks(X, indices):\n",
    "            train_index = indices[np.logical_not(test_mask)]\n",
    "            test_index = indices[test_mask]\n",
    "            yield train_index, test_index\n",
    "        \n",
    "    def _iter_test_masks(self, X, indices):\n",
    "        \"\"\"\n",
    "        create the mask for the test set, then the indices that\n",
    "        are not in the test set belongs in the training set\n",
    "        \"\"\"\n",
    "        # indicate the number of samples in each fold, and also\n",
    "        # make sure the ones that are not evenly splitted also\n",
    "        # gets assigned to a fold (e.g. if we do 2 fold on a\n",
    "        # dataset that has 5 samples, then 1 will be left out,\n",
    "        # and has to be assigned to one of the other fold)\n",
    "        n_samples = X.shape[0]\n",
    "        fold_sizes = (n_samples // self.n_splits) * np.ones(self.n_splits, dtype = np.int)\n",
    "        fold_sizes[:n_samples % self.n_splits] += 1\n",
    "\n",
    "        current = 0\n",
    "        for fold_size in fold_sizes:\n",
    "            start, stop = current, current + fold_size\n",
    "            test_indices = indices[start:stop]\n",
    "            test_mask = np.zeros(n_samples, dtype = np.bool)\n",
    "            test_mask[test_indices] = True\n",
    "            yield test_mask\n",
    "            current = stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [3 4] TEST: [0 1 2]\n",
      "TRAIN: [0 1 2] TEST: [3 4]\n",
      "\n",
      "confirm results with scikit-learn\n",
      "TRAIN: [3 4] TEST: [0 1 2]\n",
      "TRAIN: [0 1 2] TEST: [3 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# create some sample data\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "kf = KFolds(n_splits = 2, shuffle = False, seed = 4312)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "print('\\nconfirm results with scikit-learn')\n",
    "kf = KFold(n_splits = 2, random_state = 4312)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 0.56 seconds for 6 candidates parameter settings.\n",
      "Best score obtained: 0.9733333333333334\n",
      "Parameters:\n",
      "\tmin_samples_split: 7\n",
      "\tmax_depth: 3\n",
      "\tcriterion: gini\n",
      "GridSearchCV took 0.98 seconds for 12 candidate parameter settings.\n",
      "Best score obtained: 0.9666666666666667\n",
      "Parameters:\n",
      "\tmin_samples_split: 1\n",
      "\tmax_depth: 3\n",
      "\tcriterion: gini\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from scipy.stats import randint\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# load the data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# build a classifier\n",
    "clf = RandomForestClassifier(n_estimators = 20)\n",
    "\n",
    "def report(results):\n",
    "    \"\"\"report best scores and corresponding parameters\"\"\"\n",
    "    print( 'Best score obtained: {0}'.format(results.best_score_) )\n",
    "    print('Parameters:')\n",
    "    for param, value in results.best_params_.items():\n",
    "        print( '\\t{}: {}'.format(param, value) )\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'max_depth': [3, None],\n",
    "              'min_samples_split': randint(1, 11),\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 6\n",
    "random_search = RandomizedSearchCV(clf, param_distributions = param_dist,\n",
    "                                   n_iter = n_iter_search)\n",
    "start = time()\n",
    "random_search.fit(X, y)\n",
    "print('RandomizedSearchCV took %.2f seconds for %d candidates'\n",
    "      ' parameter settings.' % ((time() - start), n_iter_search))\n",
    "report(random_search)\n",
    "\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {'max_depth': [3, None],\n",
    "              'min_samples_split': [1, 3, 10],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid = param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "print('GridSearchCV took %.2f seconds for %d candidate parameter settings.'\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "report(grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load the data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from copy import deepcopy\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "n_jobs = -1\n",
    "verbose = True\n",
    "pre_dispatch = '2*n_jobs'\n",
    "clf = RandomForestClassifier()\n",
    "estimator = deepcopy(clf)\n",
    "fit_params = None # additional parameters pass to fit\n",
    "\n",
    "kf = KFolds(n_splits = 3, shuffle = True, seed = 4312)\n",
    "cv_iter = kf.split(X)\n",
    "parameter_iterable = list(_get_params_grid(param_grid))\n",
    "scorer = accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _fit_and_score(estimator, X, y, scorer, train_index, test_index,\n",
    "                   parameters, fit_params):\n",
    "    \n",
    "    # create the train/test split\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # fit the model\n",
    "    fit_params = fit_params if fit_params is not None else {}\n",
    "    estimator.set_params(**parameters)\n",
    "    estimator.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "    # obtain the train/test score\n",
    "    y_pred_train = estimator.predict(X_train)\n",
    "    y_pred_test = estimator.predict(X_test)\n",
    "    train_score = scorer(y_train, y_pred_train)\n",
    "    test_score = scorer(y_test, y_pred_test)\n",
    "    output = [train_score, test_score, parameters]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.95999999999999996,\n",
       "  0.97999999999999998,\n",
       "  {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 1}],\n",
       " [0.95999999999999996,\n",
       "  0.97999999999999998,\n",
       "  {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 3}],\n",
       " [0.95999999999999996,\n",
       "  0.97999999999999998,\n",
       "  {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 10}],\n",
       " [0.97999999999999998,\n",
       "  0.97999999999999998,\n",
       "  {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 1}],\n",
       " [0.97999999999999998,\n",
       "  0.97999999999999998,\n",
       "  {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 3}],\n",
       " [0.96999999999999997,\n",
       "  0.97999999999999998,\n",
       "  {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10}],\n",
       " [0.95999999999999996,\n",
       "  0.97999999999999998,\n",
       "  {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 1}],\n",
       " [0.95999999999999996,\n",
       "  0.97999999999999998,\n",
       "  {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 3}],\n",
       " [0.94999999999999996,\n",
       "  1.0,\n",
       "  {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 10}],\n",
       " [1.0,\n",
       "  1.0,\n",
       "  {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 1}],\n",
       " [0.98999999999999999,\n",
       "  1.0,\n",
       "  {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 3}],\n",
       " [0.95999999999999996,\n",
       "  1.0,\n",
       "  {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10}],\n",
       " [0.98999999999999999,\n",
       "  0.93999999999999995,\n",
       "  {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 1}],\n",
       " [0.98999999999999999,\n",
       "  0.93999999999999995,\n",
       "  {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 3}],\n",
       " [0.97999999999999998,\n",
       "  0.93999999999999995,\n",
       "  {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 10}],\n",
       " [0.98999999999999999,\n",
       "  0.95999999999999996,\n",
       "  {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 1}],\n",
       " [1.0,\n",
       "  0.93999999999999995,\n",
       "  {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 3}],\n",
       " [0.98999999999999999,\n",
       "  0.93999999999999995,\n",
       "  {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10}],\n",
       " [0.98999999999999999,\n",
       "  0.93999999999999995,\n",
       "  {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 1}],\n",
       " [0.97999999999999998,\n",
       "  0.93999999999999995,\n",
       "  {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 3}],\n",
       " [0.98999999999999999,\n",
       "  0.93999999999999995,\n",
       "  {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 10}],\n",
       " [1.0,\n",
       "  0.93999999999999995,\n",
       "  {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 1}],\n",
       " [1.0,\n",
       "  0.93999999999999995,\n",
       "  {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 3}],\n",
       " [0.98999999999999999,\n",
       "  0.93999999999999995,\n",
       "  {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10}],\n",
       " [0.96999999999999997,\n",
       "  0.92000000000000004,\n",
       "  {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 1}],\n",
       " [0.97999999999999998,\n",
       "  0.92000000000000004,\n",
       "  {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 3}],\n",
       " [0.96999999999999997,\n",
       "  0.92000000000000004,\n",
       "  {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 10}],\n",
       " [1.0,\n",
       "  0.90000000000000002,\n",
       "  {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 1}],\n",
       " [0.98999999999999999,\n",
       "  0.92000000000000004,\n",
       "  {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 3}],\n",
       " [0.96999999999999997,\n",
       "  0.93999999999999995,\n",
       "  {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10}],\n",
       " [0.97999999999999998,\n",
       "  0.93999999999999995,\n",
       "  {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 1}],\n",
       " [0.97999999999999998,\n",
       "  0.92000000000000004,\n",
       "  {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 3}],\n",
       " [0.98999999999999999,\n",
       "  0.92000000000000004,\n",
       "  {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 10}],\n",
       " [1.0,\n",
       "  0.90000000000000002,\n",
       "  {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 1}],\n",
       " [1.0,\n",
       "  0.90000000000000002,\n",
       "  {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 3}],\n",
       " [0.97999999999999998,\n",
       "  0.90000000000000002,\n",
       "  {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10}]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel = Parallel(n_jobs = n_jobs, verbose = verbose, pre_dispatch = pre_dispatch)\n",
    "output = parallel(delayed(_fit_and_score)(estimator, X, y, scorer,\n",
    "                                          train_index, test_index, \n",
    "                                          parameters, fit_params)\n",
    "                  for train_index, test_index in cv_iter\n",
    "                  for parameters in parameter_iterable)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://zacharyst.com/2016/03/31/parallelize-a-multifunction-argument-in-python/\n",
    "- https://pythonhosted.org/joblib/parallel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from joblib import Parallel, delayed\n",
    "Parallel(n_jobs = 2, verbose = 1)( delayed(sqrt)(i ** 2) for i in range(10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [Github: scikit-learn's KFold](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/model_selection/_split.py#L347)\n",
    "- [Github: scikit-learn's GridSearch](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/model_selection/_search.py#L685)\n",
    "- [Scikit-learn Documentation: Comparing randomized search and grid search for hyperparameter estimation](http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html#sphx-glr-auto-examples-model-selection-randomized-search-py)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
