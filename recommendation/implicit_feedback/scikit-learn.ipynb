{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn\n",
    "\n",
    "## Grid Search\n",
    "\n",
    "Learning how to implement grid search from [Source code: scikit-learn's model selection](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_search.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted parameters, values:  [('a', [1, 2]), ('b', [True, False])]\n",
      "\n",
      "parameters:  ('a', 'b')\n",
      "values ([1, 2], [True, False])\n",
      "\n",
      "grid search parameters\n",
      "{'a': 1, 'b': True}\n",
      "{'a': 1, 'b': False}\n",
      "{'a': 2, 'b': True}\n",
      "{'a': 2, 'b': False}\n"
     ]
    }
   ],
   "source": [
    "# Grid search\n",
    "from itertools import product\n",
    "from collections import Mapping\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "params_grid = {'a': [1, 2], 'b': [True, False]}\n",
    "\n",
    "# ensures that it also supports list of dictionary,\n",
    "# Mapping ensures a object has keys, values, items, etc. methods\n",
    "# which matches a dictionary\n",
    "# https://docs.python.org/3/library/collections.abc.html\n",
    "if isinstance(params_grid, Mapping):\n",
    "    params_grid = [params_grid]\n",
    "    \n",
    "for p in params_grid:\n",
    "    # for reproducibility, always sort the keys of a dictionary\n",
    "    # this will become a list of paired tuples\n",
    "    items = sorted(p.items())\n",
    "    print('sorted parameters, values: ', items)\n",
    "    print()\n",
    "    \n",
    "    # unpack the list of tuples into two lists tuples, so what's originally \n",
    "    # a list of items [('a', [1, 2]), ('b', [True, False])], becomes\n",
    "    # two lists ('a', 'b'), ([1, 2], [True, False]), with all the keys being the parameter\n",
    "    # and the value being the list of possible values that the parameter can take\n",
    "    # http://stackoverflow.com/questions/7558908/unpacking-a-list-tuple-of-pairs-into-two-lists-tuples\n",
    "    key, value = zip(*items)\n",
    "    print('parameters: ', key)\n",
    "    print('values', value)\n",
    "    print()\n",
    "    \n",
    "    # unpack the list of values to compute the cartesian product\n",
    "    # [(1, True), (1, False), (2, True), (2, False)], and zip it\n",
    "    # back to the original key\n",
    "    print('grid search parameters')\n",
    "    cartesian = product(*value)\n",
    "    for v in cartesian:\n",
    "        params = dict(zip(key, v))\n",
    "        print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 1, 'b': True},\n",
       " {'a': 1, 'b': False},\n",
       " {'a': 2, 'b': True},\n",
       " {'a': 2, 'b': False}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm with scikit-learn's output\n",
    "list( ParameterGrid(params_grid) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': True}\n",
      "{'a': 1, 'b': False}\n",
      "{'a': 2, 'b': True}\n",
      "{'a': 2, 'b': False}\n"
     ]
    }
   ],
   "source": [
    "# making our function\n",
    "def _get_params_grid(params_grid):\n",
    "    \"\"\"\n",
    "    create cartesian product of parameters (grid search),\n",
    "    this will be a generator that will allow looping through\n",
    "    all possible parameter combination, note if we want to\n",
    "    expand this to cross validation we'll have to turn it to a list\n",
    "    \"\"\"\n",
    "    # for reproducibility, always sort the keys of a dictionary\n",
    "    items = sorted(params_grid.items())\n",
    "    \n",
    "    # unpack parameter and the range of values\n",
    "    # into separate list; then unpack the range \n",
    "    # of values to compute the cartesian product\n",
    "    # and zip it back to the original key\n",
    "    key, value = zip(*items)\n",
    "    cartesian = product(*value)\n",
    "    for v in cartesian:\n",
    "        params = dict(zip(key, v))\n",
    "        yield params\n",
    "\n",
    "params_grid = {'a': [1, 2], 'b': [True, False]}\n",
    "params = _get_params_grid(params_grid)\n",
    "for p in params:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# self._fit(X, y, groups, ParameterGrid(self.param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KFolds:\n",
    "    \"\"\"\n",
    "    K-Folds cross-validation\n",
    "    Provides train/test indices to split data in train/test sets. Split\n",
    "    dataset into k consecutive folds; Each fold is then used once as \n",
    "    a validation while the k - 1 remaining folds form the training set\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int\n",
    "        number of folds. Must be at least 2\n",
    "    \n",
    "    shuffle : boolean, default True\n",
    "        whether to shuffle the data before splitting into batches\n",
    "    \n",
    "    seed : int, default 4321\n",
    "        When shuffle = True, pseudo-random number generator state used for\n",
    "        shuffling; this ensures reproducibility\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits, shuffle = True, seed = 4321):\n",
    "        self.seed = seed\n",
    "        self.shuffle = shuffle\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "    def split(self, X):\n",
    "        \"\"\"pass in the data to create train/test split for k fold\"\"\"\n",
    "        # shuffle modifies indices inplace\n",
    "        indices = np.arange(X.shape[0])\n",
    "        if self.shuffle:\n",
    "            rstate = np.random.RandomState(self.seed)\n",
    "            rstate.shuffle(indices)\n",
    "\n",
    "        for test_mask in self._iter_test_masks(X, indices):\n",
    "            train_index = indices[np.logical_not(test_mask)]\n",
    "            test_index = indices[test_mask]\n",
    "            yield train_index, test_index\n",
    "        \n",
    "    def _iter_test_masks(self, X, indices):\n",
    "        \"\"\"\n",
    "        create the mask for the test set, then the indices that\n",
    "        are not in the test set belongs in the training set\n",
    "        \"\"\"\n",
    "        # indicate the number of samples in each fold, and also\n",
    "        # make sure the ones that are not evenly splitted also\n",
    "        # gets assigned to a fold (e.g. if we do 2 fold on a\n",
    "        # dataset that has 5 samples, then 1 will be left out,\n",
    "        # and has to be assigned to one of the other fold)\n",
    "        n_samples = X.shape[0]\n",
    "        fold_sizes = (n_samples // self.n_splits) * np.ones(self.n_splits, dtype = np.int)\n",
    "        fold_sizes[:n_samples % self.n_splits] += 1\n",
    "\n",
    "        current = 0\n",
    "        for fold_size in fold_sizes:\n",
    "            start, stop = current, current + fold_size\n",
    "            test_indices = indices[start:stop]\n",
    "            test_mask = np.zeros(n_samples, dtype = np.bool)\n",
    "            test_mask[test_indices] = True\n",
    "            yield test_mask\n",
    "            current = stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [3 4] TEST: [0 1 2]\n",
      "TRAIN: [0 1 2] TEST: [3 4]\n",
      "\n",
      "confirm results with scikit-learn\n",
      "TRAIN: [3 4] TEST: [0 1 2]\n",
      "TRAIN: [0 1 2] TEST: [3 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# create some sample data\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "kf = KFolds(n_splits = 2, shuffle = False, seed = 4312)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "print('\\nconfirm results with scikit-learn')\n",
    "kf = KFold(n_splits = 2, random_state = 4312)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 0.52 seconds for 6 candidates parameter settings.\n",
      "Best score obtained: 0.9733333333333334\n",
      "Parameters:\n",
      "\tcriterion: entropy\n",
      "\tmax_depth: None\n",
      "\tmin_samples_split: 5\n",
      "GridSearchCV took 1.12 seconds for 12 candidate parameter settings.\n",
      "Best score obtained: 0.9666666666666667\n",
      "Parameters:\n",
      "\tcriterion: entropy\n",
      "\tmax_depth: 3\n",
      "\tmin_samples_split: 1\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from scipy.stats import randint\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# load the data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# build a classifier\n",
    "clf = RandomForestClassifier(n_estimators = 20)\n",
    "\n",
    "def report(results):\n",
    "    \"\"\"report best scores and corresponding parameters\"\"\"\n",
    "    print( 'Best score obtained: {0}'.format(results.best_score_) )\n",
    "    print('Parameters:')\n",
    "    for param, value in results.best_params_.items():\n",
    "        print( '\\t{}: {}'.format(param, value) )\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'max_depth': [3, None],\n",
    "              'min_samples_split': randint(1, 11),\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 6\n",
    "random_search = RandomizedSearchCV(clf, param_distributions = param_dist,\n",
    "                                   n_iter = n_iter_search)\n",
    "start = time()\n",
    "random_search.fit(X, y)\n",
    "print('RandomizedSearchCV took %.2f seconds for %d candidates'\n",
    "      ' parameter settings.' % ((time() - start), n_iter_search))\n",
    "report(random_search)\n",
    "\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {'max_depth': [3, None],\n",
    "              'min_samples_split': [1, 3, 10],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid = param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "print('GridSearchCV took %.2f seconds for %d candidate parameter settings.'\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "report(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://zacharyst.com/2016/03/31/parallelize-a-multifunction-argument-in-python/\n",
    "- https://pythonhosted.org/joblib/parallel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from joblib import Parallel, delayed\n",
    "Parallel(n_jobs = 2, verbose = 1)( delayed(sqrt)(i ** 2) for i in range(10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _fit_and_score(estimator, X, y, scorer, \n",
    "                   train_index, test_index,\n",
    "                   parameters, fit_params):\n",
    "    \n",
    "    # create the train/test split\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # fit the model\n",
    "    estimator.set_params(**parameters)\n",
    "    estimator.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "    # obtain the train/test score\n",
    "    y_pred_train = estimator.predict(X_train)\n",
    "    y_pred_test  = estimator.predict(X_test)\n",
    "    train_score = scorer(y_train, y_pred_train)\n",
    "    test_score  = scorer(y_test, y_pred_test)\n",
    "    output = [train_score, test_score]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GridSearchCV:\n",
    "    \n",
    "    def __init__(self, estimator, scorer, cv, param_grid,\n",
    "                 fit_params = None, verbose = True, n_jobs = -1, \n",
    "                 pre_dispatch = '2*n_jobs', refit = True):\n",
    "        self.cv = cv\n",
    "        self.refit = refit\n",
    "        self.n_jobs = n_jobs\n",
    "        self.scorer = scorer\n",
    "        self.verbose = verbose\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.fit_params = fit_params\n",
    "        self.pre_dispatch = pre_dispatch     \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # object used as a cross-validation generator\n",
    "        # is passed without any modification\n",
    "        if isinstance(self.cv, int):\n",
    "            cv = KFolds(n_splits = self.cv)\n",
    "        else:\n",
    "            cv = self.cv\n",
    "        \n",
    "        # obtain the train/test set index, the parameters\n",
    "        # and perform cross validation\n",
    "        cv_iter = cv.split(X)\n",
    "        params_iterable = list(_get_params_grid(self.param_grid))\n",
    "        fit_params = self.fit_params if self.fit_params is not None else {}\n",
    "        \n",
    "        parallel = Parallel(n_jobs = self.n_jobs, verbose = self.verbose, \n",
    "                            pre_dispatch = self.pre_dispatch)\n",
    "        output = parallel(delayed(_fit_and_score)(deepcopy(self.estimator), \n",
    "                                                  X, y, self.scorer,\n",
    "                                                  train_index, test_index, \n",
    "                                                  parameters, fit_params)\n",
    "                          for train_index, test_index in cv_iter\n",
    "                          for parameters in params_iterable)\n",
    "\n",
    "        # unpack training/testing scores\n",
    "        n_splits = cv.n_splits\n",
    "        n_candidates = len(params_iterable)\n",
    "        train_score, test_score = zip(*output)\n",
    "        train_score = np.array(train_score, dtype = np.float64).reshape(n_splits, n_candidates)\n",
    "        test_score = np.array(test_score, dtype = np.float64).reshape(n_splits, n_candidates)\n",
    "        \n",
    "        # obtain the best score and parameter using the \n",
    "        # best mean test scores across all folds, where\n",
    "        # best here means the higher the better\n",
    "        mean_test_score = np.mean(test_score, axis = 0)\n",
    "        best_index = np.argmax(mean_test_score)\n",
    "        self.best_score = mean_test_score[best_index]\n",
    "        self.best_param = params_iterable[best_index]\n",
    "\n",
    "        # list the mean, std train and test score\n",
    "        # for each parameters combination;\n",
    "        # not sure if 'params', the column with the\n",
    "        # values in the dictionary format is useful or not\n",
    "        mean_train_score = np.mean(train_score, axis = 0)\n",
    "        std_test_score = np.std(test_score, axis = 0)\n",
    "        std_train_score = np.std(train_score, axis = 0)\n",
    "        cv_results = {\n",
    "            'mean_train_score': mean_train_score,\n",
    "            'std_train_score': std_train_score,\n",
    "            'mean_test_score': mean_test_score,\n",
    "            'std_test_score': std_test_score\n",
    "        }\n",
    "\n",
    "        # ensure the columns appear in this order (train score, test score, parameters)\n",
    "        # and order by the best test score\n",
    "        cols = ['mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']\n",
    "        cv_results = pd.DataFrame(cv_results, columns = cols)\n",
    "        df_params  = pd.DataFrame(params_iterable)\n",
    "        cv_results = pd.concat([cv_results, df_params], axis = 1)\n",
    "        cv_results['params'] = params_iterable\n",
    "        cv_results = cv_results.sort_values(['mean_test_score', 'std_test_score'], ascending = False)\n",
    "        cv_results = cv_results.reset_index(drop = True)\n",
    "        self.cv_results = cv_results\n",
    "        \n",
    "        # refit on the entire dataset after performing cross validation\n",
    "        if self.refit:\n",
    "            best_estimator = deepcopy(self.estimator)\n",
    "            best_estimator.set_params(**self.best_param)\n",
    "            best_estimator.fit(X, y, **fit_params)\n",
    "            self.best_estimator = best_estimator\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"call predict on the estimator with the best found parameter\"\"\"\n",
    "        if not self.refit:\n",
    "            raise ValueError('Only available if refit=True')\n",
    "        \n",
    "        return self.best_estimator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.966667</td>\n",
       "      <td>4.714045e-03</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>gini</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 3, 'min_sam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.973333</td>\n",
       "      <td>4.714045e-03</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>gini</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 3, 'min_sam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.970000</td>\n",
       "      <td>8.164966e-03</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.018856</td>\n",
       "      <td>gini</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 3, 'min_sam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.983333</td>\n",
       "      <td>4.714045e-03</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.018856</td>\n",
       "      <td>gini</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'min_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.973333</td>\n",
       "      <td>9.428090e-03</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>entropy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': None, 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.970000</td>\n",
       "      <td>8.164966e-03</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.032660</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3, 'min_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.993333</td>\n",
       "      <td>4.714045e-03</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.028284</td>\n",
       "      <td>gini</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'min_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.024944</td>\n",
       "      <td>gini</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'min_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.973333</td>\n",
       "      <td>4.714045e-03</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3, 'min_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.970000</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3, 'min_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.993333</td>\n",
       "      <td>4.714045e-03</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>entropy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': None, 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.018856</td>\n",
       "      <td>entropy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': None, 'm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_train_score  std_train_score  mean_test_score  std_test_score  \\\n",
       "0           0.966667     4.714045e-03         0.953333        0.009428   \n",
       "1           0.973333     4.714045e-03         0.953333        0.009428   \n",
       "2           0.970000     8.164966e-03         0.946667        0.018856   \n",
       "3           0.983333     4.714045e-03         0.946667        0.018856   \n",
       "4           0.973333     9.428090e-03         0.946667        0.009428   \n",
       "5           0.970000     8.164966e-03         0.940000        0.032660   \n",
       "6           0.993333     4.714045e-03         0.940000        0.028284   \n",
       "7           1.000000     0.000000e+00         0.933333        0.024944   \n",
       "8           0.973333     4.714045e-03         0.933333        0.009428   \n",
       "9           0.970000     1.110223e-16         0.933333        0.009428   \n",
       "10          0.993333     4.714045e-03         0.933333        0.009428   \n",
       "11          1.000000     0.000000e+00         0.926667        0.018856   \n",
       "\n",
       "   criterion  max_depth  min_samples_split  \\\n",
       "0       gini        3.0                  1   \n",
       "1       gini        3.0                  3   \n",
       "2       gini        3.0                 10   \n",
       "3       gini        NaN                 10   \n",
       "4    entropy        NaN                 10   \n",
       "5    entropy        3.0                 10   \n",
       "6       gini        NaN                  3   \n",
       "7       gini        NaN                  1   \n",
       "8    entropy        3.0                  1   \n",
       "9    entropy        3.0                  3   \n",
       "10   entropy        NaN                  1   \n",
       "11   entropy        NaN                  3   \n",
       "\n",
       "                                               params  \n",
       "0   {'criterion': 'gini', 'max_depth': 3, 'min_sam...  \n",
       "1   {'criterion': 'gini', 'max_depth': 3, 'min_sam...  \n",
       "2   {'criterion': 'gini', 'max_depth': 3, 'min_sam...  \n",
       "3   {'criterion': 'gini', 'max_depth': None, 'min_...  \n",
       "4   {'criterion': 'entropy', 'max_depth': None, 'm...  \n",
       "5   {'criterion': 'entropy', 'max_depth': 3, 'min_...  \n",
       "6   {'criterion': 'gini', 'max_depth': None, 'min_...  \n",
       "7   {'criterion': 'gini', 'max_depth': None, 'min_...  \n",
       "8   {'criterion': 'entropy', 'max_depth': 3, 'min_...  \n",
       "9   {'criterion': 'entropy', 'max_depth': 3, 'min_...  \n",
       "10  {'criterion': 'entropy', 'max_depth': None, 'm...  \n",
       "11  {'criterion': 'entropy', 'max_depth': None, 'm...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n_jobs = -1\n",
    "#verbose = True\n",
    "#pre_dispatch = '2*n_jobs'\n",
    "\n",
    "# dictionary of\n",
    "# additional parameters pass to fit\n",
    "# or just None\n",
    "#fit_params = None \n",
    "\n",
    "#n_splits = 3\n",
    "#kf = KFolds(n_splits = n_splits, shuffle = True, seed = 4312)\n",
    "#cv_iter = kf.split(X)\n",
    "#params_iterable = list(_get_params_grid(param_grid))\n",
    "\n",
    "# load the data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# options\n",
    "cv = 3\n",
    "scorer = accuracy_score\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# fit grid search\n",
    "grid_search = GridSearchCV(estimator = clf, scorer = scorer, cv = cv, param_grid = param_grid)\n",
    "grid_search.fit(X, y)\n",
    "grid_search.cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [Github: scikit-learn's KFold](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/model_selection/_split.py#L347)\n",
    "- [Github: scikit-learn's GridSearch](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/model_selection/_search.py#L685)\n",
    "- [Scikit-learn Documentation: Comparing randomized search and grid search for hyperparameter estimation](http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html#sphx-glr-auto-examples-model-selection-randomized-search-py)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
