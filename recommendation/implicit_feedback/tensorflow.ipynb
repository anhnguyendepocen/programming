{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension: \n",
      " (100000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "file_dir = 'ml-100k'\n",
    "file_path = os.path.join(file_dir, 'u.data')\n",
    "if not os.path.isdir(file_dir):\n",
    "    call(['curl', '-O', 'http://files.grouplens.org/datasets/movielens/' + file_dir + '.zip'])\n",
    "    call(['unzip', file_dir + '.zip'])\n",
    "\n",
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "data = pd.read_csv(file_path, sep = '\\t', names = names)\n",
    "print('data dimension: \\n', data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "items_col = 'item_id'\n",
    "users_col = 'user_id'\n",
    "ratings_col = 'rating'\n",
    "threshold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data[data[ratings_col] >= threshold]\n",
    "data[ratings_col] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<943x1574 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 82520 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binary based on threshold (filter out the zero later \n",
    "# or else the length won't match)\n",
    "data[items_col] = data[items_col].astype('category')\n",
    "data[users_col] = data[users_col].astype('category')\n",
    "data[ratings_col] = data[ratings_col].astype('category')\n",
    "\n",
    "ratings = csr_matrix(( data[ratings_col].astype(np.int32),\n",
    "                       (data[users_col].cat.codes, data[items_col].cat.codes) ))\n",
    "ratings.eliminate_zeros()\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "def create_train_test(ratings, test_size = 0.2, seed = 1234):\n",
    "    \"\"\"\n",
    "    split the user-item interactions matrix into train and test set\n",
    "    by removing some of the interactions from every user and pretend\n",
    "    that we never seen them\n",
    "    TODO: parallelize the computation\n",
    "    Parameters\n",
    "    ----------\n",
    "    ratings : scipy sparse csr_matrix\n",
    "        The user-item interactions matrix\n",
    "    test_size : float between 0.0 and 1.0, default 0.2\n",
    "        Proportion of the user-item interactions for each user\n",
    "        in the dataset to move to the test set; e.g. if set to 0.2\n",
    "        and a user has 10 interactions, then 2 will be moved to the\n",
    "        test set\n",
    "    seed : int, default 1234\n",
    "        Seed for reproducible random splitting the \n",
    "        data into train/test set\n",
    "    Returns\n",
    "    -------\n",
    "    train : scipy sparse csr_matrix\n",
    "        Training set\n",
    "    test : scipy sparse csr_matrix\n",
    "        Test set\n",
    "    \"\"\"\n",
    "    assert test_size < 1.0 and test_size > 0.0\n",
    "\n",
    "    # Dictionary Of Keys based sparse matrix is more efficient\n",
    "    # for constructing sparse matrices incrementally compared with csr_matrix\n",
    "    train = ratings.copy().todok()\n",
    "    test = dok_matrix(train.shape, dtype = np.int32)\n",
    "    \n",
    "    # for all the users assign randomly chosen interactions\n",
    "    # to the test and assign those interactions to zero in the training;\n",
    "    # when computing the interactions to go into the test set, \n",
    "    # remember to round up the numbers (e.g. a user has 4 ratings, if the\n",
    "    # test_size is 0.2, then 0.8 ratings will go to test, thus we need to\n",
    "    # round up to ensure the test set gets at least 1 rating)\n",
    "    rstate = np.random.RandomState(seed)\n",
    "    for u in range(ratings.shape[0]):\n",
    "        split_index = ratings[u].indices\n",
    "        n_splits = ceil(test_size * split_index.shape[0])\n",
    "        test_index = rstate.choice(split_index, size = n_splits, replace = False)\n",
    "        test[u, test_index] = ratings[u, test_index]\n",
    "        train[u, test_index] = 0\n",
    "    \n",
    "    train, test = train.tocsr(), test.tocsr()\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<943x1574 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 65641 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = create_train_test(ratings, test_size = 0.2, seed = 1234)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TensorflowBPR:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.01, n_factors = 15, n_iters = 10, \n",
    "                 n_batches = 1000, reg = 0.01, seed = 1234):\n",
    "        self.reg = reg\n",
    "        self.seed = seed\n",
    "        self.n_iters = n_iters\n",
    "        self.n_batches = n_batches\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def fit(self, interactions):\n",
    "        \n",
    "        indptr = interactions.indptr\n",
    "        indices = interactions.indices\n",
    "        n_users, n_items = interactions.shape\n",
    "        batch_iters = interactions.nnz // self.n_batches\n",
    "        \n",
    "        self.history = []\n",
    "        self._build_graph(n_users, n_items)\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "\n",
    "            for i in trange(self.n_iters):\n",
    "                iteration_cost = 0.0\n",
    "                for j in range(batch_iters):\n",
    "                    sampled = self._create_boostrapped_samples(n_users, n_items, indices, indptr)\n",
    "                    sampled_users, sampled_pos_items, sampled_neg_items = sampled\n",
    "                    feed_dict = {self._slice_u: sampled_users, \n",
    "                                 self._slice_i: sampled_pos_items, \n",
    "                                 self._slice_j: sampled_neg_items}\n",
    "                    _, cost = sess.run([self._train_step, self._total_cost], feed_dict)\n",
    "                    iteration_cost += cost / self.n_batches\n",
    "\n",
    "                iteration_cost /= batch_iters\n",
    "                self.history.append(iteration_cost)\n",
    "\n",
    "            self.user_factors = sess.run(self.user_factors)\n",
    "            self.item_factors = sess.run(self.item_factors)\n",
    "            self.item_bias = sess.run(self.item_bias)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def _create_boostrapped_samples(self, n_users, n_items, indices, indptr):\n",
    "        sampled_pos_items = np.zeros(self.n_batches, dtype = np.int32)\n",
    "        sampled_neg_items = np.zeros(self.n_batches, dtype = np.int32)\n",
    "        sampled_users = np.random.choice(n_users, size = self.n_batches).astype(np.int32)\n",
    "\n",
    "        for idx, user in enumerate(sampled_users):\n",
    "            pos_items = indices[ indptr[user]:indptr[user + 1] ]\n",
    "            pos_item = np.random.choice(pos_items)\n",
    "            neg_item = np.random.choice(n_items)\n",
    "            while neg_item in pos_items:\n",
    "                neg_item = np.random.choice(n_items)\n",
    "\n",
    "            sampled_pos_items[idx] = pos_item\n",
    "            sampled_neg_items[idx] = neg_item\n",
    "\n",
    "        return sampled_users, sampled_pos_items, sampled_neg_items       \n",
    "    \n",
    "    def _build_graph(self, n_users, n_items):\n",
    "        self.user_factors = tf.Variable(tf.truncated_normal((n_users, self.n_factors), seed = self.seed), \n",
    "                                        name = 'user_factors')\n",
    "        self.item_factors = tf.Variable(tf.truncated_normal((n_items, self.n_factors), seed = self.seed), \n",
    "                                        name = 'item_factors')\n",
    "        self.item_bias = tf.Variable(tf.zeros(n_items), name = 'item_bias')\n",
    "\n",
    "        # use tf.gather() to select a non-contiguous slice from the tensor\n",
    "        # http://stackoverflow.com/questions/35146444/tensorflow-python-accessing-individual-elements-in-a-tensor\n",
    "        self._slice_u = tf.placeholder(tf.int32, self.n_batches)\n",
    "        self._slice_i = tf.placeholder(tf.int32, self.n_batches)\n",
    "        self._slice_j = tf.placeholder(tf.int32, self.n_batches)\n",
    "        user_u = tf.gather(self.user_factors, self._slice_u)\n",
    "        item_i = tf.gather(self.item_factors, self._slice_i)\n",
    "        item_j = tf.gather(self.item_factors, self._slice_j)\n",
    "        bias_i = tf.gather(self.item_bias, self._slice_i)\n",
    "        bias_j = tf.gather(self.item_bias, self._slice_j)\n",
    "\n",
    "        # compute difference between the two item, item i and item j\n",
    "        x_ui = tf.diag_part( tf.matmul(user_u, tf.transpose(item_i)) )\n",
    "        x_uj = tf.diag_part( tf.matmul(user_u, tf.transpose(item_j)) )\n",
    "        x_uij = bias_i - bias_j + x_ui - x_uj\n",
    "\n",
    "        # minimize the cost\n",
    "        cost_u = self.reg * tf.reduce_sum(user_u ** 2)\n",
    "        cost_i = self.reg * tf.reduce_sum(item_i ** 2)\n",
    "        cost_j = self.reg * tf.reduce_sum(item_j ** 2)\n",
    "        cost_uij = tf.reduce_sum( tf.log(tf.nn.sigmoid(x_uij)) )\n",
    "        self._total_cost = cost_u + cost_i + cost_j - cost_uij\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self._train_step = optimizer.minimize(self._total_cost)\n",
    "        return self\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Obtain the predicted ratings for every users and items\n",
    "        by doing a dot product of the learnt user and item vectors.\n",
    "        The result will be cached to avoid re-computing \n",
    "        it every time we call predict, thus there will\n",
    "        only be an overhead the first time we call it.\n",
    "        Note, ideally you probably don't need to compute \n",
    "        this as it returns a dense matrix and may take\n",
    "        up huge amounts of memory for large datasets\n",
    "        \"\"\"\n",
    "        if not self._predicted:\n",
    "            self._get_prediction()\n",
    "            self._predicted = True\n",
    "\n",
    "        return self._pred\n",
    "\n",
    "    def _get_prediction(self):\n",
    "        \"\"\"Predicted ratings (dot product of user and item vectors)\"\"\"\n",
    "        self._pred = self.user_factors.dot(self.item_factors.T) + self.item_bias\n",
    "        return self\n",
    "\n",
    "    def _predict_user(self, user):\n",
    "        \"\"\"\n",
    "        returns the predicted ratings for the specified user,\n",
    "        this is mainly used in computing evaluation metric,\n",
    "        where we avoid computing the whole predicted rating matrix\n",
    "        TODO : do we even need this in the class?\n",
    "        \"\"\"\n",
    "        user_pred = self.user_factors[user].dot(self.item_factors.T) + self.item_bias\n",
    "        return user_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:28<00:00,  2.84s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.TensorflowBPR at 0x1221b2198>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpr_params = {\n",
    "    'n_factors': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_iters': 10,\n",
    "    'reg': 0.01,\n",
    "    'n_batches': 2000 \n",
    "}\n",
    "\n",
    "tf_bpr = TensorflowBPR(**bpr_params)\n",
    "tf_bpr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auc_score(model, ratings):\n",
    "    auc = 0.0\n",
    "    n_users, n_items = ratings.shape\n",
    "    for user, row in enumerate(ratings):\n",
    "        y_pred = model._predict_user(user)\n",
    "        y_true = np.zeros(n_items, dtype = np.int32)\n",
    "        y_true[row.indices] = 1\n",
    "        auc += roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    auc /= n_users\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.944510167019\n",
      "0.896118426907\n"
     ]
    }
   ],
   "source": [
    "print(auc_score(tf_bpr, train))\n",
    "print(auc_score(tf_bpr, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_factors = 10\n",
    "n_users, n_items = train.shape\n",
    "reg = 0.25\n",
    " = 0.05\n",
    "\n",
    "n_iters = 10\n",
    "n_batches = 2000 # should be smaller than train.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_boostrapped_samples():\n",
    "    indices = train.indices\n",
    "    indptr = train.indptr\n",
    "\n",
    "    sampled_pos_items = np.zeros(n_batches, dtype = np.int32)\n",
    "    sampled_neg_items = np.zeros(n_batches, dtype = np.int32)\n",
    "    sampled_users = np.random.choice(n_users, size = n_batches).astype(np.int32)\n",
    "\n",
    "    for idx, user in enumerate(sampled_users):\n",
    "        pos_items = indices[ indptr[user]:indptr[user + 1] ]\n",
    "        pos_item = np.random.choice(pos_items)\n",
    "        neg_item = np.random.choice(n_items)\n",
    "        while neg_item in pos_items:\n",
    "            neg_item = np.random.choice(n_items)\n",
    "\n",
    "        sampled_pos_items[idx] = pos_item\n",
    "        sampled_neg_items[idx] = neg_item\n",
    "        \n",
    "    return sampled_users, sampled_pos_items, sampled_neg_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sess = tf.InteractiveSession()\n",
    "\n",
    "#tf.diag_part(tf.truncated_normal((2, 2)) ** 2).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "learning_rate = 0.05\n",
    "n_iters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def generate_computational_graph():\n",
    "user_factors = tf.Variable(tf.truncated_normal((n_users, n_factors), seed = seed), name = 'user_factors')\n",
    "item_factors = tf.Variable(tf.truncated_normal((n_items, n_factors), seed = seed), name = 'item_factors')\n",
    "item_bias = tf.Variable(tf.zeros(n_items), name = 'item_bias')\n",
    "\n",
    "# use tf.gather() to select a non-contiguous slice from the tensor\n",
    "# http://stackoverflow.com/questions/35146444/tensorflow-python-accessing-individual-elements-in-a-tensor\n",
    "slice_u = tf.placeholder(tf.int32, n_batches)\n",
    "slice_i = tf.placeholder(tf.int32, n_batches)\n",
    "slice_j = tf.placeholder(tf.int32, n_batches)\n",
    "user_u  = tf.gather(user_factors, slice_u)\n",
    "item_i  = tf.gather(item_factors, slice_i)\n",
    "item_j  = tf.gather(item_factors, slice_j)\n",
    "bias_i  = tf.gather(item_bias, slice_i)\n",
    "bias_j  = tf.gather(item_bias, slice_j)\n",
    "\n",
    "#  \n",
    "x_ui = tf.diag_part( tf.matmul(user_u, tf.transpose(item_i)) )\n",
    "x_uj = tf.diag_part( tf.matmul(user_u, tf.transpose(item_j)) )\n",
    "x_uij = bias_i - bias_j + x_ui - x_uj\n",
    "\n",
    "# minimize the cost\n",
    "cost_u = reg * tf.reduce_sum(user_u ** 2)\n",
    "cost_i = reg * tf.reduce_sum(item_i ** 2)\n",
    "cost_j = reg * tf.reduce_sum(item_j ** 2)\n",
    "cost_uij = tf.reduce_sum( tf.log(tf.nn.sigmoid(x_uij)) )\n",
    "total_cost = cost_u + cost_i + cost_j - cost_uij                \n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_iters = train.nnz // n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in trange(n_iters):\n",
    "        iteration_cost = 0.0\n",
    "        for j in range(batch_iters):\n",
    "            sampled_users, sampled_pos_items, sampled_neg_items = generate_boostrapped_samples()\n",
    "            feed_dict = {slice_u: sampled_users, slice_i: sampled_pos_items, slice_j: sampled_neg_items}\n",
    "            _, cost = sess.run([train_step, total_cost], feed_dict)\n",
    "            iteration_cost += cost / n_batches\n",
    "        \n",
    "        iteration_cost /= batch_iters\n",
    "        history.append(iteration_cost)\n",
    "    \n",
    "    U = sess.run(user_factors)\n",
    "    I = sess.run(item_factors)\n",
    "    B = sess.run(item_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U.dot(I.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_user(user):\n",
    "    user_pred = U[user].dot(I.T) + B\n",
    "    return user_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auc_score(ratings):\n",
    "    auc = 0.0\n",
    "    n_users, n_items = ratings.shape\n",
    "    for user, row in enumerate(ratings):\n",
    "        y_pred = predict_user(user)\n",
    "        y_true = np.zeros(n_items, dtype = np.int32)\n",
    "        y_true[row.indices] = 1\n",
    "        auc += roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    auc /= n_users\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(auc_score(train))\n",
    "print(auc_score(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImplicitMF:\n",
    "    \"\"\"\n",
    "    Alternating Least Squares for implicit feedback\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_iters : int\n",
    "        number of iterations to train the algorithm\n",
    "\n",
    "    n_factors : int\n",
    "        number/dimension of user and item latent factors\n",
    "\n",
    "    alpha : int\n",
    "        scaling factor that indicates the level of confidence in preference\n",
    "\n",
    "    reg : int\n",
    "        regularization term for the user and item latent factors\n",
    "\n",
    "    seed : int\n",
    "        seed for the randomly initialized user, item latent factors\n",
    "\n",
    "    Reference\n",
    "    ---------\n",
    "    Collaborative Filtering for Implicit Feedback Datasets\n",
    "    http://yifanhu.net/PUB/cf.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, n_iters, n_factors, alpha, reg, seed):\n",
    "        self.reg = reg\n",
    "        self.seed = seed\n",
    "        self.alpha = alpha\n",
    "        self.n_iters = n_iters\n",
    "        self.n_factors = n_factors\n",
    "    \n",
    "    def fit(self, train):\n",
    "        \"\"\"train: csr_matrix that holds the training data\"\"\"\n",
    "        \n",
    "        # the original confidence vectors should include a + 1,\n",
    "        # but this direct addition is not allowed when using sparse matrix,\n",
    "        # thus we'll have to deal with this later in the computation\n",
    "        Cui = self.alpha * train\n",
    "        Ciu = Cui.T.tocsr()\n",
    "        self.n_users, self.n_items = Cui.shape\n",
    "        \n",
    "        # initialize user latent vectors X and item latent vectors Y\n",
    "        # randomly with a specified set seed\n",
    "        rstate = np.random.RandomState(self.seed)\n",
    "        self.X = rstate.normal( size = (self.n_users, self.n_factors) )\n",
    "        self.Y = rstate.normal( size = (self.n_items, self.n_factors) )      \n",
    "        for _ in trange(self.n_iters, desc = 'training progress'):\n",
    "            self._als_step(Cui, self.X, self.Y)\n",
    "            self._als_step(Ciu, self.Y, self.X)  \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _als_step(self, Cui, X, Y):\n",
    "        \"\"\"\n",
    "        when solving the user latent vectors,\n",
    "        the item vectors will be fixed and vice versa\n",
    "        \"\"\"\n",
    "        # the variable name follows the notation when holding\n",
    "        # the item vector Y constant and solving for user vector X\n",
    "        \n",
    "        # YtY is a d * d matrix that is computed\n",
    "        # independently of each user\n",
    "        YtY = Y.T.dot(Y)\n",
    "        data = Cui.data\n",
    "        indptr, indices = Cui.indptr, Cui.indices\n",
    "\n",
    "        # for every user build up A and b then solve for Ax = b,\n",
    "        # this for loop is the bottleneck and can be easily parallized\n",
    "        # as each users' computation is independent of one another\n",
    "        for u in range(self.n_users):\n",
    "            # initialize a new A and b for every user\n",
    "            b = np.zeros(self.n_factors)\n",
    "            A = YtY + self.reg * np.eye(self.n_factors)\n",
    "            \n",
    "            for index in range( indptr[u], indptr[u + 1] ):         \n",
    "                # indices[index] stores non-zero positions for a given row\n",
    "                # data[index] stores corresponding confidence,\n",
    "                # we also add 1 to the confidence, since we did not \n",
    "                # do it in the beginning, when we were to give every \n",
    "                # user-item pair and minimal confidence\n",
    "                i = indices[index]\n",
    "                confidence = data[index] + 1\n",
    "\n",
    "                # for b, Y^T C^u p_u\n",
    "                # there should be a times 1 for the preference \n",
    "                # Pui = 1\n",
    "                # b += confidence * Y[i] * Pui\n",
    "                # but the times 1 can be dropped\n",
    "                b += confidence * Y[i]\n",
    "                \n",
    "                # for A, Y^T (C^u - I) Y\n",
    "                A += (confidence - 1) * np.outer(Y[i], Y[i])\n",
    "\n",
    "            X[u] = np.linalg.solve(A, b)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"predict ratings for every user and item\"\"\"\n",
    "        pred = self.X.dot(self.Y.T)\n",
    "        return pred\n",
    "    \n",
    "    def predict_user(self, user):\n",
    "        user_pred = self.X[user].dot(self.Y.T)\n",
    "        return user_pred   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "als = ImplicitMF(n_iters = 20, n_factors = 20, alpha = 15, reg = 0.01, seed = 1234)\n",
    "als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auc_score(model, ratings):\n",
    "    auc = 0.0\n",
    "    n_users, n_items = ratings.shape\n",
    "    for user, row in enumerate(ratings):\n",
    "        y_pred = model._predict_user(user)\n",
    "        y_true = np.zeros(n_items, dtype = np.int32)\n",
    "        y_true[row.indices] = 1\n",
    "        auc += roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    auc /= n_users\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(auc_score2(als, train))\n",
    "print(auc_score2(als, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from lightfm.evaluation import auc_score\n",
    "\n",
    "model = LightFM(learning_rate=0.05, loss='bpr')\n",
    "model.fit(train, epochs=50)\n",
    "\n",
    "train_precision = precision_at_k(model, train, k=10).mean()\n",
    "test_precision = precision_at_k(model, test, k=10).mean()\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test).mean()\n",
    "\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hi = [1,2,2,4,5]\n",
    "found = np.searchsorted(hi, 3)\n",
    "# don't walk off the end of the array\n",
    "end = len(hi) - 1\n",
    "if found > end:\n",
    "    found = end\n",
    "# hi[found] == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_neg_items(pos_items, item):\n",
    "    position = np.searchsorted(pos_items, item)\n",
    "    # don't walk off the end of the array\n",
    "    end = pos_items.shape[0] - 1\n",
    "    if position > end:\n",
    "        position = end\n",
    "    \n",
    "    return pos_items[position] != item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC specifies the probability that, when we draw two examples at random, their predicted pairwise n_factorsing is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# user = 0\n",
    "indices = train.indices\n",
    "indptr = train.indptr\n",
    "\n",
    "for user in range(n_users):\n",
    "    n = 0\n",
    "    user_auc = 0.0\n",
    "    predictions = predict_user(user)\n",
    "\n",
    "    \n",
    "    # sort the positive items so we can use binary search\n",
    "    # to determine the negative items later\n",
    "    pos_items = indices[ indptr[user]:indptr[user + 1] ]\n",
    "    pos_items = np.sort(pos_items)\n",
    "\n",
    "    for pos_item in pos_items:\n",
    "        for neg_item in range(n_items):\n",
    "            if is_neg_items(pos_items, neg_item):\n",
    "                n += 1\n",
    "                if predictions[pos_item] > predictions[neg_item]:\n",
    "                    user_auc += 1\n",
    "                    \n",
    "    user_auc /= n\n",
    "    auc += user_auc\n",
    "                \n",
    "auc / n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
